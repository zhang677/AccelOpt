{
  "name": "2417ae9431fb45cba3bbade8aeb0610e",
  "definition": "fused_add_rmsnorm_h7168",
  "author": "AccelOpt",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "H100"
    ],
    "entry_point": "main.py::run",
    "dependencies": []
  },
  "sources": [
    {
      "path": "main.py",
      "content": "import torch\nimport triton\nimport triton.language as tl\n\n# --------------------------------------------------------------------\n# Kernel: one‑pass fused add + RMSNorm + weight scaling\n# --------------------------------------------------------------------\n@triton.autotune(\n    configs=[\n        # The only sensible config for the fixed hidden size 7168 is to\n        # cover the whole row in one iteration (BLOCK_SIZE_N ≥ HIDDEN_SIZE).\n        triton.Config({'BLOCK_SIZE_N': 8192, 'num_warps': 16, 'num_stages': 2}),\n        # Keeping the other configs would lead to incorrect results because\n        # they would not process the whole row.  The autotuner will therefore\n        # always pick the 8192‑element configuration.\n    ],\n    key=['HIDDEN_SIZE'],\n)\n@triton.jit\ndef _fused_add_rmsnorm_h7168_kernel(\n    hidden_states_ptr,        # const bfloat16 *\n    residual_ptr,             # const bfloat16 *\n    weight_ptr,               # const bfloat16 *\n    output_ptr,               # bfloat16 *\n    stride_hs_b,              # int64\n    stride_res_b,             # int64\n    stride_out_b,             # int64\n    HIDDEN_SIZE: tl.constexpr,   # = 7168 (compile‑time)\n    EPS: tl.constexpr,            # = 1e‑6  (compile‑time)\n    BLOCK_SIZE_N: tl.constexpr,   # must be ≥ HIDDEN_SIZE\n):\n    \"\"\"\n    One‑pass kernel:\n      1) Load the whole row of `hidden_states` and `residual` (masked).\n      2) Compute x = hidden_states + residual.\n      3) Reduce x² to obtain the variance.\n      4) Compute inv_rms = rsqrt(variance + EPS).\n      5) Load the corresponding slice of `weight`.\n      6) Write out (x * inv_rms * weight) as bfloat16.\n    All arithmetic is performed in float32, preserving the baseline precision.\n    \"\"\"\n    pid = tl.program_id(axis=0)                     # one program per batch element\n\n    # Pointers to the current row\n    row_hs = hidden_states_ptr + pid * stride_hs_b\n    row_res = residual_ptr      + pid * stride_res_b\n    row_out = output_ptr        + pid * stride_out_b\n\n    # ----------------------------------------------------------------\n    # 1) Load the whole row (masked – the extra elements are zeroed)\n    # ----------------------------------------------------------------\n    offs = tl.arange(0, BLOCK_SIZE_N)\n    mask = offs < HIDDEN_SIZE\n\n    hs = tl.load(row_hs + offs, mask=mask, other=0.0).to(tl.float32)\n    rs = tl.load(row_res + offs, mask=mask, other=0.0).to(tl.float32)\n\n    # ----------------------------------------------------------------\n    # 2) Fused add\n    # ----------------------------------------------------------------\n    x = hs + rs                     # shape: [BLOCK_SIZE_N] (float32)\n\n    # ----------------------------------------------------------------\n    # 3) RMS computation (block‑wide reduction)\n    # ----------------------------------------------------------------\n    variance = tl.sum(x * x, axis=0) / HIDDEN_SIZE\n    inv_rms  = tl.rsqrt(variance + EPS)\n\n    # ----------------------------------------------------------------\n    # 4) Apply weight and store\n    # ----------------------------------------------------------------\n    w = tl.load(weight_ptr + offs, mask=mask, other=0.0).to(tl.float32)\n    out = x * inv_rms * w               # still float32\n\n    tl.store(row_out + offs, out.to(tl.bfloat16), mask=mask)\n\n\n# --------------------------------------------------------------------\n# Python wrapper – validates inputs, launches the kernel and returns result\n# --------------------------------------------------------------------\ndef run(hidden_states: torch.Tensor,\n        residual: torch.Tensor,\n        weight: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Fused add + RMSNorm (hidden size = 7168) using the optimized Triton kernel.\n    The function keeps the exact numerical behavior of the reference\n    implementation (float32 intermediate arithmetic, bfloat16 I/O).\n\n    Args:\n        hidden_states: Tensor[batch, 7168], dtype=torch.bfloat16\n        residual:      Tensor[batch, 7168], dtype=torch.bfloat16\n        weight:        Tensor[7168],       dtype=torch.bfloat16\n\n    Returns:\n        Tensor[batch, 7168], dtype=torch.bfloat16\n    \"\"\"\n    # --------------------------------------------------------------\n    # 1️⃣ Input validation & device handling\n    # --------------------------------------------------------------\n    if not torch.cuda.is_available():\n        raise RuntimeError(\"Triton kernels require a CUDA‑enabled GPU.\")\n\n    # Make sure all tensors are on the same CUDA device (move if needed)\n    device = hidden_states.device\n    if device.type != \"cuda\":\n        device = torch.device(\"cuda\")\n    hidden_states = hidden_states.to(device)\n    residual = residual.to(device)\n    weight = weight.to(device)\n\n    if hidden_states.dtype != torch.bfloat16 or \\\n       residual.dtype      != torch.bfloat16 or \\\n       weight.dtype        != torch.bfloat16:\n        raise TypeError(\"All inputs must be torch.bfloat16 tensors.\")\n\n    batch_size, hidden_size = hidden_states.shape\n    if hidden_size != 7168:\n        raise ValueError(f\"hidden_size must be 7168, got {hidden_size}\")\n    if residual.shape != hidden_states.shape:\n        raise ValueError(\"`residual` must have the same shape as `hidden_states`.\")\n    if weight.shape != (hidden_size,):\n        raise ValueError(f\"`weight` must be of shape ({hidden_size},).\")\n\n    # --------------------------------------------------------------\n    # 2️⃣ Output allocation\n    # --------------------------------------------------------------\n    output = torch.empty_like(hidden_states)\n\n    # --------------------------------------------------------------\n    # 3️⃣ Grid configuration (one program per batch element)\n    # --------------------------------------------------------------\n    grid = (batch_size,)\n\n    # --------------------------------------------------------------\n    # 4️⃣ Kernel launch\n    # --------------------------------------------------------------\n    _fused_add_rmsnorm_h7168_kernel[grid](\n        hidden_states,\n        residual,\n        weight,\n        output,\n        hidden_states.stride(0),   # stride between rows (batch dimension)\n        residual.stride(0),\n        output.stride(0),\n        HIDDEN_SIZE=hidden_size,\n        EPS=1e-6,\n        # BLOCK_SIZE_N is a compile‑time constant taken from the autotune config\n    )\n\n    return output"
    }
  ],
  "description": "gemini-2.5-pro_triton_05pwmx_plan_1_1"
}