{
  "name": "66c9d248cd3d40e6a9c658da8dccadac",
  "definition": "mla_paged_decode_h16_ckv512_kpe64_ps1",
  "author": "AccelOpt",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "H100"
    ],
    "entry_point": "main.py::run",
    "dependencies": []
  },
  "sources": [
    {
      "path": "main.py",
      "content": "# --------------------------------------------------------------\n# mla_paged_decode_h16_ckv512_kpe64_ps1 – bf16 Tensor‑Core version\n# --------------------------------------------------------------\n#   * One Triton program per batch element (grid = B)\n#   * 16 heads are processed together (H_TILE = 16)\n#   * KV vectors are loaded once per token block and reused\n#   * bf16 data stays in bf16 up to the large matrix‑vector multiplies\n#   * dot‑products are executed on Tensor‑Cores (out_dtype=fp32)\n# --------------------------------------------------------------\n\nimport math\nimport torch\nimport triton\nimport triton.language as tl\n\n\n# ------------------------------------------------------------------\n# 1️⃣  Autotuned kernel – fused heads, bf16 Tensor‑Core arithmetic\n# ------------------------------------------------------------------\n@triton.autotune(\n    configs=[\n        # legacy (single‑head) configs – keep them as fallback\n        triton.Config({'BLOCK_TOK':  64, 'H_TILE': 1}, num_warps=4,  num_stages=3),\n        triton.Config({'BLOCK_TOK': 128, 'H_TILE': 1}, num_warps=8,  num_stages=4),\n        # full‑head fusion configs (H_TILE = 16)\n        triton.Config({'BLOCK_TOK':  64, 'H_TILE': 16}, num_warps=8,  num_stages=4),\n        triton.Config({'BLOCK_TOK': 128, 'H_TILE': 16}, num_warps=16, num_stages=5),\n        triton.Config({'BLOCK_TOK': 256, 'H_TILE': 16}, num_warps=16, num_stages=5),\n        triton.Config({'BLOCK_TOK': 512, 'H_TILE': 16}, num_warps=32, num_stages=5),\n    ],\n    key=['B', 'D_CKV', 'D_KPE']   # H is a compile‑time constant (16), H_TILE lives in the config\n)\n@triton.jit\ndef _paged_decode_kernel_fused_head(\n    QN,                     # (B, 16, 512)      bf16\n    QP,                     # (B, 16, 64)       bf16\n    KC,                     # (P, 512)          bf16\n    KP,                     # (P, 64)           bf16\n    KV_INDICES,             # (N)               int32\n    KV_INDPTR,              # (B+1)             int32\n    SM_SCALE,               # scalar            fp32\n    OUT,                    # (B, 16, 512)      bf16\n    LSE,                    # (B, 16)           fp32\n    #\n    B: tl.constexpr,       # batch size\n    D_CKV: tl.constexpr,   # 512\n    D_KPE: tl.constexpr,   # 64\n    BLOCK_TOK: tl.constexpr,\n    H_TILE: tl.constexpr,  # number of heads processed together (1 or 16)\n):\n    \"\"\"\n    One Triton program processes **H_TILE** heads of a single batch element.\n    The KV block is loaded once and reused for all heads, dramatically\n    reducing global‑memory traffic.\n    All heavy arithmetic (QN·KCᵀ, QP·KPᵀ) stays in bf16 and runs on Tensor‑Cores.\n    \"\"\"\n    pid = tl.program_id(0)                # one program per batch element\n    b = pid                                 # batch index\n\n    H = 16                                   # total heads (constant)\n    mask_h = tl.arange(0, H_TILE) < H       # mask for possibly‑partial tile (fallback case)\n\n    # --------------------------------------------------------------\n    # 2️⃣  Offsets\n    # --------------------------------------------------------------\n    offs_ckv = tl.arange(0, D_CKV)          # (512,)\n    offs_kpe = tl.arange(0, D_KPE)          # (64,)\n    offs_tok = tl.arange(0, BLOCK_TOK)      # (BLOCK_TOK,)\n\n    # --------------------------------------------------------------\n    # 3️⃣  Load queries for the whole tile (keep bf16)\n    # --------------------------------------------------------------\n    qn_ptr = QN + (b * H + tl.arange(0, H_TILE)[:, None]) * D_CKV + offs_ckv[None, :]\n    qp_ptr = QP + (b * H + tl.arange(0, H_TILE)[:, None]) * D_KPE + offs_kpe[None, :]\n\n    qn = tl.load(qn_ptr,\n                 mask=mask_h[:, None],\n                 other=0)                 # (H_TILE, D_CKV) bf16\n    qp = tl.load(qp_ptr,\n                 mask=mask_h[:, None],\n                 other=0)                 # (H_TILE, D_KPE) bf16\n\n    # --------------------------------------------------------------\n    # 4️⃣  KV range for this batch element\n    # --------------------------------------------------------------\n    kv_beg = tl.load(KV_INDPTR + b)          # first token index\n    kv_end = tl.load(KV_INDPTR + b + 1)      # exclusive\n    kv_len = kv_end - kv_beg\n\n    # --------------------------------------------------------------\n    # 5️⃣  Edge case – no KV tokens for this batch element\n    # --------------------------------------------------------------\n    if kv_len <= 0:\n        out_ptr = OUT + (b * H) * D_CKV + tl.arange(0, H_TILE)[:, None] * D_CKV + offs_ckv[None, :]\n        lse_ptr = LSE + b * H + tl.arange(0, H_TILE)\n\n        tl.store(out_ptr,\n                 tl.zeros([H_TILE, D_CKV], dtype=tl.bfloat16),\n                 mask=mask_h[:, None])\n        tl.store(lse_ptr,\n                 tl.full([H_TILE], -float(\"inf\"), dtype=tl.float32),\n                 mask=mask_h)\n        return\n\n    # --------------------------------------------------------------\n    # 6️⃣  Per‑tile accumulators (one per head)\n    # --------------------------------------------------------------\n    s_sum = tl.zeros([H_TILE], dtype=tl.float32)                # Σ exp(logits) per head\n    w_sum = tl.zeros([H_TILE, D_CKV], dtype=tl.float32)         # Σ exp(logits) * KC per head\n\n    # --------------------------------------------------------------\n    # 7️⃣  Token loop – process tokens in blocks of BLOCK_TOK\n    # --------------------------------------------------------------\n    tok_start = tl.zeros([], dtype=tl.int32)\n\n    while tok_start < kv_len:\n        remaining = kv_len - tok_start\n        cur_block = tl.where(remaining < BLOCK_TOK, remaining, BLOCK_TOK)\n        mask_t = offs_tok < cur_block                               # (BLOCK_TOK,)\n\n        # ---- gather token indices ---------------------------------\n        idx_ptr = KV_INDICES + kv_beg + tok_start + offs_tok\n        tok_idx = tl.load(idx_ptr, mask=mask_t, other=0)            # (BLOCK_TOK,)\n\n        # ---- shared KV loads (once per block, used by all heads) --\n        kc_ptr = KC + tok_idx[:, None] * D_CKV + offs_ckv[None, :]   # (T, 512)\n        kp_ptr = KP + tok_idx[:, None] * D_KPE + offs_kpe[None, :]   # (T, 64)\n\n        kc_blk = tl.load(kc_ptr,\n                         mask=mask_t[:, None],\n                         other=0)                                 # (T, 512) bf16\n        kp_blk = tl.load(kp_ptr,\n                         mask=mask_t[:, None],\n                         other=0)                                 # (T, 64)  bf16\n\n        # ---- batched dot‑products for all heads in the tile ------\n        # Tensor‑Core bf16·bf16 → fp32 result\n        l_ckv = tl.dot(qn, tl.trans(kc_blk), out_dtype=tl.float32) # (H_TILE, T)\n        l_kpe = tl.dot(qp, tl.trans(kp_blk), out_dtype=tl.float32) # (H_TILE, T)\n\n        logits = (l_ckv + l_kpe) * SM_SCALE                        # (H_TILE, T)\n\n        exp_logits = tl.exp(logits)\n        # mask out the padded part of the block\n        exp_logits = tl.where(mask_t[None, :], exp_logits, 0.0)     # (H_TILE, T)\n\n        # ---- accumulate ------------------------------------------------\n        s_sum += tl.sum(exp_logits, axis=1)                        # (H_TILE,)\n\n        # For the weighted sum we need the KV values in fp32\n        kc_blk_fp32 = kc_blk.to(tl.float32)\n\n        # (H_TILE, D_CKV) = (H_TILE, T) @ (T, D_CKV)\n        w_sum += tl.dot(exp_logits, kc_blk_fp32)                    # (H_TILE, D_CKV)\n\n        # ---- advance ---------------------------------------------------\n        tok_start += BLOCK_TOK\n\n    # ------------------------------------------------------------------\n    # 8️⃣  Final reduction → output vector and 2‑base LSE\n    # ------------------------------------------------------------------\n    inv_ln2 = 1.4426950408889634          # 1 / ln(2)\n    out_vec = w_sum / s_sum[:, None]      # (H_TILE, D_CKV)\n    lse_val = tl.log(s_sum) * inv_ln2     # (H_TILE,)\n\n    # ------------------------------------------------------------------\n    # 9️⃣  Store results (only the valid heads of the tile)\n    # ------------------------------------------------------------------\n    out_ptr = OUT + (b * H) * D_CKV + tl.arange(0, H_TILE)[:, None] * D_CKV + offs_ckv[None, :]\n    lse_ptr = LSE + b * H + tl.arange(0, H_TILE)\n\n    tl.store(out_ptr, out_vec.to(tl.bfloat16), mask=mask_h[:, None])\n    tl.store(lse_ptr, lse_val, mask=mask_h)\n\n\n# --------------------------------------------------------------\n# Python wrapper – same public API as the reference implementation\n# --------------------------------------------------------------\ndef run(\n    q_nope: torch.Tensor,\n    q_pe: torch.Tensor,\n    ckv_cache: torch.Tensor,\n    kpe_cache: torch.Tensor,\n    kv_indptr: torch.Tensor,\n    kv_indices: torch.Tensor,\n    sm_scale: float,\n) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Optimized paged‑decode kernel with full‑head fusion and bf16 Tensor‑Core dot products.\n    Returns\n    -------\n    output : torch.Tensor  (B, 16, 512)  bf16\n    lse    : torch.Tensor  (B, 16)       fp32\n    \"\"\"\n    if not torch.cuda.is_available():\n        raise RuntimeError(\"CUDA device is required to run Triton kernels.\")\n\n    # -------------------- validation -------------------- #\n    assert q_nope.dtype == torch.bfloat16 and q_pe.dtype == torch.bfloat16\n    B, H, D_CKV = q_nope.shape\n    assert H == 16 and D_CKV == 512\n    assert q_pe.shape == (B, 16, 64)\n    assert ckv_cache.shape[1] == 1 and kpe_cache.shape[1] == 1   # page_size = 1\n    assert kv_indptr.shape[0] == B + 1\n    assert kv_indices.shape[0] == kv_indptr[-1].item()\n\n    # ---------------- device handling ----------------- #\n    orig_device = q_nope.device\n    cuda_dev = torch.cuda.current_device()\n\n    def _to_cuda(t: torch.Tensor) -> torch.Tensor:\n        return t.to(device=cuda_dev, non_blocking=True) if not t.is_cuda else t\n\n    q_nope_d = _to_cuda(q_nope)                                 # (B,16,512)\n    q_pe_d   = _to_cuda(q_pe)                                   # (B,16,64)\n    kc_d     = _to_cuda(ckv_cache.squeeze(1))                   # (P,512) bf16\n    kp_d     = _to_cuda(kpe_cache.squeeze(1))                   # (P,64)  bf16\n    indptr_d = _to_cuda(kv_indptr)                             # (B+1,)  int32\n    indices_d= _to_cuda(kv_indices)                            # (N,)    int32\n\n    # ---------------- output buffers ------------------ #\n    out_d = torch.empty((B, H, D_CKV), dtype=torch.bfloat16, device=cuda_dev)\n    lse_d = torch.empty((B, H),        dtype=torch.float32, device=cuda_dev)\n\n    # ---------------- kernel launch ------------------- #\n    grid = (B,)   # one program per batch element\n\n    _paged_decode_kernel_fused_head[grid](\n        q_nope_d,\n        q_pe_d,\n        kc_d,\n        kp_d,\n        indices_d,\n        indptr_d,\n        float(sm_scale),\n        out_d,\n        lse_d,\n        # compile‑time constants\n        B=B,\n        D_CKV=512,\n        D_KPE=64,\n        # BLOCK_TOK and H_TILE are supplied by the autotuner\n    )\n\n    # ---------------- move results back ---------------- #\n    if orig_device.type == \"cpu\":\n        out_d = out_d.cpu()\n        lse_d = lse_d.cpu()\n    elif orig_device != out_d.device:\n        out_d = out_d.to(orig_device)\n        lse_d = lse_d.to(orig_device)\n\n    return out_d, lse_d"
    }
  ],
  "description": "4695964e19774e24936efe2740e28005_plan_1_0"
}