{
  "name": "0f6af81f89be4179b2df2b8d55ed2f0d",
  "definition": "moe_fp8_block_scale_ds_routing_topk8_ng8_kg4_e32_h7168_i2048",
  "author": "AccelOpt",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "H100"
    ],
    "entry_point": "main.py::run",
    "dependencies": []
  },
  "sources": [
    {
      "path": "main.py",
      "content": "# -------------------------------------------------------------\n# 1. Imports\n# -------------------------------------------------------------\nimport math\nfrom typing import Any, Dict, List\n\nimport torch\nimport triton\nimport triton.language as tl\n\n# -------------------------------------------------------------\n# 2. Helper – block‑wise de‑quantisation (fallback when float‑8 not\n#    available in the current Triton build)\n# -------------------------------------------------------------\ndef _fallback_dequant_fp8_block128(x: torch.Tensor, scale: torch.Tensor) -> torch.Tensor:\n    \"\"\"Fallback de‑quantisation (FP8 → FP32) using block‑wise scales.\"\"\"\n    BLOCK = 128\n    x_fp32 = x.to(torch.float32)                                 # [T, H]\n    s_T = scale.permute(1, 0).contiguous()                       # [T, H/128]\n    s_exp = s_T.unsqueeze(-1).repeat(1, 1, BLOCK)                # [T, H/128, 128]\n    s_exp = s_exp.reshape(x_fp32.shape)                         # [T, H]\n    return x_fp32 * s_exp\n\n\n# -------------------------------------------------------------\n# 3. Triton fused kernel – token‑tiling version\n# -------------------------------------------------------------\n@triton.jit\ndef _moe_fp8_fused_token_kernel(\n    # -----------------------------------------------------------------\n    # pointers\n    # -----------------------------------------------------------------\n    hidden_ptr,          # [T, H]               fp8_e4m3fn\n    hidden_sc_ptr,       # [H/128, T]           fp32   (block scales, transposed)\n    w13_ptr,             # [E_local, 2*I, H]    fp8\n    w13_sc_ptr,          # [E_local, (2I)/128, H/128] fp32\n    w2_ptr,              # [E_local, H, I]      fp8\n    w2_sc_ptr,           # [E_local, H/128, I/128]      fp32\n    topk_expert_ptr,     # [T, TOP_K]           int32\n    topk_weight_ptr,     # [T, TOP_K]           fp32\n    out_ptr,             # [T, H]               fp32 (accumulator)\n    # -----------------------------------------------------------------\n    # compile‑time constants\n    # -----------------------------------------------------------------\n    T:                 tl.constexpr,\n    H:                 tl.constexpr,\n    I:                 tl.constexpr,\n    E_LOCAL:           tl.constexpr,\n    TOP_K:             tl.constexpr,\n    BLOCK_H:           tl.constexpr = 128,\n    BLOCK_I:           tl.constexpr = 128,\n    TOKENS_PER_BLOCK: tl.constexpr = 2,            # <-- tiled token dimension\n):\n    \"\"\"\n    One program processes **TOKENS_PER_BLOCK** consecutive tokens.\n    All expert‑weight tiles are loaded once per token‑tile and reused for\n    every token inside the tile.\n    \"\"\"\n\n    pid = tl.program_id(0)                     # tile id\n    token_base = pid * TOKENS_PER_BLOCK\n    # token indices for this tile\n    token_off = token_base + tl.arange(0, TOKENS_PER_BLOCK)\n    mask_token = token_off < T                  # [TOKENS_PER_BLOCK]\n\n    # -----------------------------------------------------------------\n    # 0) per‑token accumulators for the final output (registered)\n    # -----------------------------------------------------------------\n    out_acc = tl.zeros([TOKENS_PER_BLOCK, H], dtype=tl.float32)\n\n    # -----------------------------------------------------------------\n    # 1) loop over the TOP_K experts selected for each token\n    # -----------------------------------------------------------------\n    for slot in range(TOP_K):\n        # ----- load the expert id and the routing weight for every token\n        #      in the tile (scalar per token)\n        linear_idx = token_off * TOP_K + slot               # [TOKENS_PER_BLOCK]\n        expert = tl.load(topk_expert_ptr + linear_idx,\n                        mask=mask_token, other=-1)         # int32\n        weight = tl.load(topk_weight_ptr + linear_idx,\n                        mask=mask_token, other=0.0)         # fp32\n\n        # skip the whole slot if no token in the tile has a valid expert\n        valid_slot = (expert >= 0) & (weight != 0.0) & mask_token\n        if tl.any(valid_slot) == 0:\n            continue\n\n        # -----------------------------------------------------------------\n        # 2) GEMM‑1 : hidden (TOKENS_PER_BLOCK × H) × W13ᵀ (2I × H)\n        # -----------------------------------------------------------------\n        # accumulator for the 2*I intermediate\n        g1 = tl.zeros([TOKENS_PER_BLOCK, 2 * I], dtype=tl.float32)\n\n        # number of hidden blocks\n        num_h_blocks = H // BLOCK_H\n\n        for hb in range(num_h_blocks):\n            # ----- hidden slice for every token in the tile -----\n            h_off   = hb * BLOCK_H + tl.arange(0, BLOCK_H)\n            h_ptr   = hidden_ptr + token_off[:, None] * H + h_off[None, :]\n            h_fp8   = tl.load(h_ptr, mask=mask_token[:, None], other=tl.float8e4m3fn(0))\n            # block‑scale (layout: [H/128, T] – transposed)\n            sc_off  = hb * T + token_off\n            sc      = tl.load(hidden_sc_ptr + sc_off,\n                              mask=mask_token, other=1.0)          # fp32\n            h_fp32  = h_fp8.to(tl.float32) * sc[:, None]           # [TOKENS_PER_BLOCK, BLOCK_H]\n\n            # ----- W13 tile (expert‑specific) -----\n            # We load the tile **once per expert** (the same for every token in the tile)\n            # but we have to respect that different tokens may point to different experts.\n            # Therefore we materialise a small per‑token weight tile.\n            w13_tile = tl.zeros([TOKENS_PER_BLOCK, 2 * I, BLOCK_H], dtype=tl.float32)\n\n            for t in range(TOKENS_PER_BLOCK):\n                if not mask_token[t] or expert[t] < 0:\n                    continue\n                # base offset for this expert\n                w13_base = expert[t] * (2 * I) * H\n                i_off   = tl.arange(0, 2 * I)\n                w13_off = w13_base + i_off[:, None] * H + (hb * BLOCK_H + tl.arange(0, BLOCK_H))[None, :]\n                w13_fp8 = tl.load(w13_ptr + w13_off,\n                                  mask=True, other=tl.float8e4m3fn(0))\n                # per‑block scales for W13\n                blk_i   = i_off // BLOCK_H\n                sc_off  = expert[t] * ((2 * I) // BLOCK_H) * (H // BLOCK_H) \\\n                          + blk_i * (H // BLOCK_H) + hb\n                w13_sc  = tl.load(w13_sc_ptr + sc_off, mask=True, other=1.0)   # [2I/128]\n                w13_sc_tile = tl.broadcast_to(w13_sc[:, None], [2 * I, BLOCK_H])\n                w13_fp32 = w13_fp8.to(tl.float32) * w13_sc_tile\n                w13_tile[t, :, :] = w13_fp32\n\n            # ----- batched dot product (hidden slice × W13ᵀ) -----\n            # g1[t] += hidden[t] @ W13ᵀ\n            for t in range(TOKENS_PER_BLOCK):\n                if not mask_token[t] or expert[t] < 0:\n                    continue\n                part = tl.dot(h_fp32[t, None, :], w13_tile[t, :, :])   # (1, 2I)\n                g1[t, :] += part[0]\n\n        # -----------------------------------------------------------------\n        # 3) SwiGLU (still per‑token)\n        # -----------------------------------------------------------------\n        x1 = g1[:, 0:I]                     # [TOKENS_PER_BLOCK, I]\n        x2 = g1[:, I:2 * I]                 # [TOKENS_PER_BLOCK, I]\n        silu = x2 / (1.0 + tl.exp(-x2))\n        glu = silu * x1                     # [TOKENS_PER_BLOCK, I]\n\n        # -----------------------------------------------------------------\n        # 4) GEMM‑2 : glu (TOKENS_PER_BLOCK × I) × W2ᵀ (H × I)\n        # -----------------------------------------------------------------\n        o = tl.zeros([TOKENS_PER_BLOCK, H], dtype=tl.float32)\n\n        for hb in range(num_h_blocks):\n            # ----- W2 tile (expert‑specific) -----\n            w2_tile = tl.zeros([TOKENS_PER_BLOCK, BLOCK_H, I], dtype=tl.float32)\n\n            for t in range(TOKENS_PER_BLOCK):\n                if not mask_token[t] or expert[t] < 0:\n                    continue\n                w2_base = expert[t] * H * I\n                h_off   = hb * BLOCK_H + tl.arange(0, BLOCK_H)\n                i_off   = tl.arange(0, I)\n                w2_off  = w2_base + h_off[:, None] + i_off[None, :] * H\n                w2_fp8  = tl.load(w2_ptr + w2_off,\n                                  mask=True, other=tl.float8e4m3fn(0))\n                # per‑block scales for W2\n                blk_i   = i_off // BLOCK_I\n                sc_off  = expert[t] * (H // BLOCK_H) * (I // BLOCK_I) \\\n                          + hb * (I // BLOCK_I) + blk_i\n                w2_sc   = tl.load(w2_sc_ptr + sc_off, mask=True, other=1.0)   # [I/128]\n                w2_sc_tile = tl.broadcast_to(w2_sc[:, None], [I, BLOCK_H])\n                w2_fp32 = w2_fp8.to(tl.float32) * w2_sc_tile      # (I, BLOCK_H)\n                w2_tile[t, :, :] = w2_fp32\n\n            # ----- batched dot (glu × W2ᵀ) -----\n            for t in range(TOKENS_PER_BLOCK):\n                if not mask_token[t] or expert[t] < 0:\n                    continue\n                part2 = tl.dot(glu[t, None, :], w2_tile[t, :, :])   # (1, BLOCK_H)\n                o[t, hb * BLOCK_H: hb * BLOCK_H + BLOCK_H] = part2[0]\n\n        # -----------------------------------------------------------------\n        # 5) Weighted accumulation into the per‑token output accumulator\n        # -----------------------------------------------------------------\n        for t in range(TOKENS_PER_BLOCK):\n            if not mask_token[t] or expert[t] < 0:\n                continue\n            out_acc[t, :] += o[t, :] * weight[t]\n\n    # -----------------------------------------------------------------\n    # 6) Write the accumulated result back to global memory\n    # -----------------------------------------------------------------\n    for t in range(TOKENS_PER_BLOCK):\n        if not mask_token[t]:\n            continue\n        out_off = (token_base + t) * H + tl.arange(0, H)\n        tl.store(out_ptr + out_off, out_acc[t, :], mask=True)\n\n\n# -------------------------------------------------------------\n# 4. Triton autotune configuration (includes token‑tiling)\n# -------------------------------------------------------------\n_moe_fp8_fused_token = triton.autotune(\n    configs=[\n        # 2 tokens per block – low shared‑mem pressure, high occupancy\n        triton.Config({'BLOCK_H': 128, 'BLOCK_I': 128, 'TOKENS_PER_BLOCK': 2},\n                      num_warps=4, num_stages=2, num_ctas=2),\n        # 4 tokens per block – may fit on newer GPUs (e.g. H100)\n        triton.Config({'BLOCK_H': 128, 'BLOCK_I': 128, 'TOKENS_PER_BLOCK': 4},\n                      num_warps=8, num_stages=3, num_ctas=2),\n    ],\n    key=['T', 'H']\n)(_moe_fp8_fused_token_kernel)\n\n\n# -------------------------------------------------------------\n# 5. Public API – optimised run()\n# -------------------------------------------------------------\ndef run(\n    routing_logits:        torch.Tensor,\n    routing_bias:          torch.Tensor,\n    hidden_states:         torch.Tensor,\n    hidden_states_scale:   torch.Tensor,\n    gemm1_weights:         torch.Tensor,\n    gemm1_weights_scale:   torch.Tensor,\n    gemm2_weights:         torch.Tensor,\n    gemm2_weights_scale:   torch.Tensor,\n    local_expert_offset:   int,\n    routed_scaling_factor: float,\n    *args: Any,\n    **kwargs: Dict[str, Any],\n) -> torch.Tensor:\n    \"\"\"\n    Optimised DeepSeek‑V3 MoE forward pass.\n    * FP8 block‑scale de‑quantisation, GEMM1 → SwiGLU → GEMM2 are fused in a\n      tiled‑token Triton kernel.\n    * Routing (group‑wise top‑k) matches the reference implementation\n      bit‑for‑bit.\n    * Returns the MoE output in ``bfloat16`` on the original device of\n      ``hidden_states``.\n    * Falls back to the pure‑PyTorch implementation when the current Triton\n      build lacks ``float8`` support.\n    \"\"\"\n    # -----------------------------------------------------------------\n    # 0) sanity & device handling\n    # -----------------------------------------------------------------\n    if not torch.cuda.is_available():\n        raise RuntimeError(\"MoE kernel requires a CUDA device\")\n\n    def _to_cuda(t: torch.Tensor) -> torch.Tensor:\n        return t.cuda() if not t.is_cuda else t\n\n    tensors_in: List[torch.Tensor] = [\n        routing_logits, routing_bias, hidden_states, hidden_states_scale,\n        gemm1_weights, gemm1_weights_scale,\n        gemm2_weights, gemm2_weights_scale,\n    ]\n    orig_devices = [t.device for t in tensors_in]\n    (\n        routing_logits, routing_bias, hidden_states, hidden_states_scale,\n        gemm1_weights, gemm1_weights_scale,\n        gemm2_weights, gemm2_weights_scale,\n    ) = map(_to_cuda, tensors_in)\n\n    device = hidden_states.device\n    T, H = hidden_states.shape          # T = seq_len, H = hidden_size (7168)\n    I = 2048\n    TOP_K = 8\n    N_GROUP = 8\n    TOPK_GROUP = 4\n    E_GLOBAL = 256\n    E_LOCAL = gemm1_weights.shape[0]   # should be 32\n\n    # -----------------------------------------------------------------\n    # 1) Routing – identical to reference\n    # -----------------------------------------------------------------\n    logits = routing_logits.to(torch.float32)               # [T, 256]\n    bias   = routing_bias.to(torch.float32).reshape(-1)     # [256]\n\n    s = torch.sigmoid(logits)                              # [T, 256]\n    s_wbias = s + bias                                     # broadcast\n\n    group_size = E_GLOBAL // N_GROUP                        # 32\n    s_grouped = s_wbias.view(T, N_GROUP, group_size)       # [T,8,32]\n\n    top2_vals, _ = torch.topk(s_grouped, k=2, dim=2, largest=True, sorted=False)\n    group_scores = top2_vals.sum(dim=2)                     # [T,8]\n\n    _, group_idx = torch.topk(group_scores, k=TOPK_GROUP, dim=1,\n                              largest=True, sorted=False)   # [T,4]\n    group_mask = torch.zeros_like(group_scores)\n    group_mask.scatter_(1, group_idx, 1.0)\n\n    score_mask = group_mask.unsqueeze(2).expand(T, N_GROUP, group_size)\\\n                                    .reshape(T, E_GLOBAL)   # [T,256]\n\n    neg_inf = torch.finfo(torch.float32).min\n    scores_kept = s_wbias.masked_fill(score_mask == 0, neg_inf)\n    _, topk_idx = torch.topk(scores_kept, k=TOP_K, dim=1,\n                             largest=True, sorted=False)      # [T,8] int64\n\n    # routing weights (use s without bias)\n    M = torch.zeros_like(s)\n    M.scatter_(1, topk_idx, 1.0)\n    weights = s * M\n    weights = (weights / (weights.sum(dim=1, keepdim=True) + 1e-20)) \\\n                * routed_scaling_factor                       # [T,256]\n\n    # -----------------------------------------------------------------\n    # 2) Kernel launch – FP8 path vs fallback\n    # -----------------------------------------------------------------\n    if hasattr(tl, \"float8e4m3fn\"):\n        # ----- pack routing tensors for the kernel -----\n        topk_expert = topk_idx.to(torch.int32).contiguous()          # [T, TOP_K]\n        # per‑token‑expert weight = weight for the selected expert\n        topk_weight = weights.gather(1, topk_idx).contiguous()      # [T, TOP_K]\n\n        # ----- output accumulator (fp32) -----\n        out = torch.zeros((T, H), dtype=torch.float32, device=device)\n\n        # ----- launch fused kernel (grid = one tile per program) -----\n        tiles = math.ceil(T / 2)          # default token‑tile = 2 (autotune may pick 4)\n        grid = (tiles,)\n\n        _moe_fp8_fused_token[grid](\n            hidden_states,                     # fp8\n            hidden_states_scale,               # fp32 block scales\n            gemm1_weights,                     # fp8\n            gemm1_weights_scale,               # fp32 scales\n            gemm2_weights,                     # fp8\n            gemm2_weights_scale,               # fp32 scales\n            topk_expert,                       # int32\n            topk_weight,                       # fp32\n            out,                               # fp32 accumulator\n            T=T,\n            H=H,\n            I=I,\n            E_LOCAL=E_LOCAL,\n            TOP_K=TOP_K,\n            # compile‑time constants are left to autotune\n            num_warps=8,          # overridden by autotune\n            num_stages=3,\n        )\n    else:\n        # ----- pure‑PyTorch fallback (identical to reference) -----\n        A = _fallback_dequant_fp8_block128(hidden_states, hidden_states_scale)\n\n        S13 = gemm1_weights_scale.to(torch.float32)\n        S13 = torch.repeat_interleave(S13, 128, dim=1)\n        S13 = torch.repeat_interleave(S13, 128, dim=2)\n        W13 = gemm1_weights.to(torch.float32) * S13\n\n        S2 = gemm2_weights_scale.to(torch.float32)\n        S2 = torch.repeat_interleave(S2, 128, dim=1)\n        S2 = torch.repeat_interleave(S2, 128, dim=2)\n        W2 = gemm2_weights.to(torch.float32) * S2\n\n        out = torch.zeros((T, H), dtype=torch.float32, device=device)\n        local_start = int(local_expert_offset)\n\n        for le in range(E_LOCAL):\n            ge = local_start + le\n            if ge < 0 or ge >= E_GLOBAL:\n                continue\n            sel_mask = (topk_idx == ge).any(dim=1)\n            if not sel_mask.any():\n                continue\n            token_idx = torch.nonzero(sel_mask, as_tuple=False).squeeze(1)\n            A_e = A.index_select(0, token_idx)          # [Tk, H]\n            W13e = W13[le]                              # [4096, H]\n            W2e = W2[le]                                # [H, 2048]\n\n            G1 = A_e @ W13e.t()                         # [Tk, 4096]\n            X1, X2 = G1[:, :I], G1[:, I:]\n            silu = X2 / (1.0 + torch.exp(-X2))\n            C = silu * X1                               # [Tk, 2048]\n            O = C @ W2e.t()                             # [Tk, H]\n\n            w_tok = weights.index_select(0, token_idx)[:, ge]  # [Tk]\n            out.index_add_(0, token_idx, O * w_tok.unsqueeze(1))\n\n    # -----------------------------------------------------------------\n    # 3) Cast to bfloat16 and restore original device\n    # -----------------------------------------------------------------\n    result = out.to(torch.bfloat16)\n    if result.device != orig_devices[2]:      # hidden_states original device\n        result = result.to(orig_devices[2])\n    return result\n\n\n# -------------------------------------------------------------\n# Export\n# -------------------------------------------------------------\n__all__ = [\"run\"]"
    }
  ],
  "description": "c88e46a937744312ba5c6c8f41495926_plan_0_1"
}