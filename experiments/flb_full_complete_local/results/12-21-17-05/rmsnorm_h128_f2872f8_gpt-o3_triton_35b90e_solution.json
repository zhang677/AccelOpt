{
  "name": "e6522f9119574218bd90ee1dc7fce713",
  "definition": "rmsnorm_h128",
  "author": "AccelOpt",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "H100"
    ],
    "entry_point": "main.py::run",
    "dependencies": []
  },
  "sources": [
    {
      "path": "main.py",
      "content": "# =============================================================================\n# rmsnorm_h128 – Triton implementation (batch‑tiling, weight‑reuse,\n#                triple‑buffered row pre‑fetch, pipelined)\n# =============================================================================\nimport torch\nimport triton\nimport triton.language as tl\n\n# --------------------------------------------------------------------------- #\n# Constants (must stay BF16 as in the reference implementation)\n# --------------------------------------------------------------------------- #\nEPS: float = 1e-6          # numerical‑stability term\nHIDDEN_SIZE: int = 128     # hidden dimension (fixed)\n\n# --------------------------------------------------------------------------- #\n# Triton kernel – autotuned over ROWS_PER_BLOCK, num_warps and num_stages\n# --------------------------------------------------------------------------- #\n@triton.autotune(\n    configs=[\n        # ---- original configurations (kept) -------------------------------- #\n        triton.Config({\"BLOCK\": HIDDEN_SIZE, \"ROWS_PER_BLOCK\": 1, \"num_warps\": 4}, num_stages=2),\n        triton.Config({\"BLOCK\": HIDDEN_SIZE, \"ROWS_PER_BLOCK\": 1, \"num_warps\": 4}, num_stages=3),\n        triton.Config({\"BLOCK\": HIDDEN_SIZE, \"ROWS_PER_BLOCK\": 1, \"num_warps\": 4}, num_stages=4),\n        triton.Config({\"BLOCK\": HIDDEN_SIZE, \"ROWS_PER_BLOCK\": 1, \"num_warps\": 4}, num_stages=5),\n\n        triton.Config({\"BLOCK\": HIDDEN_SIZE, \"ROWS_PER_BLOCK\": 2, \"num_warps\": 4}, num_stages=2),\n        triton.Config({\"BLOCK\": HIDDEN_SIZE, \"ROWS_PER_BLOCK\": 2, \"num_warps\": 4}, num_stages=3),\n        triton.Config({\"BLOCK\": HIDDEN_SIZE, \"ROWS_PER_BLOCK\": 2, \"num_warps\": 4}, num_stages=4),\n        triton.Config({\"BLOCK\": HIDDEN_SIZE, \"ROWS_PER_BLOCK\": 2, \"num_warps\": 4}, num_stages=5),\n\n        triton.Config({\"BLOCK\": HIDDEN_SIZE, \"ROWS_PER_BLOCK\": 4, \"num_warps\": 8}, num_stages=2),\n        triton.Config({\"BLOCK\": HIDDEN_SIZE, \"ROWS_PER_BLOCK\": 4, \"num_warps\": 8}, num_stages=3),\n        triton.Config({\"BLOCK\": HIDDEN_SIZE, \"ROWS_PER_BLOCK\": 4, \"num_warps\": 8}, num_stages=4),\n        triton.Config({\"BLOCK\": HIDDEN_SIZE, \"ROWS_PER_BLOCK\": 4, \"num_warps\": 8}, num_stages=5),\n\n        triton.Config({\"BLOCK\": HIDDEN_SIZE, \"ROWS_PER_BLOCK\": 8, \"num_warps\": 8}, num_stages=2),\n        triton.Config({\"BLOCK\": HIDDEN_SIZE, \"ROWS_PER_BLOCK\": 8, \"num_warps\": 8}, num_stages=3),\n        triton.Config({\"BLOCK\": HIDDEN_SIZE, \"ROWS_PER_BLOCK\": 8, \"num_warps\": 8}, num_stages=4),\n        triton.Config({\"BLOCK\": HIDDEN_SIZE, \"ROWS_PER_BLOCK\": 8, \"num_warps\": 8}, num_stages=5),\n\n        # ---- new, larger ROWS_PER_BLOCK values ------------------------------ #\n        triton.Config({\"BLOCK\": HIDDEN_SIZE, \"ROWS_PER_BLOCK\": 12, \"num_warps\": 8},  num_stages=3),\n        triton.Config({\"BLOCK\": HIDDEN_SIZE, \"ROWS_PER_BLOCK\": 16, \"num_warps\": 8},  num_stages=3),\n        triton.Config({\"BLOCK\": HIDDEN_SIZE, \"ROWS_PER_BLOCK\": 24, \"num_warps\": 8},  num_stages=4),\n        triton.Config({\"BLOCK\": HIDDEN_SIZE, \"ROWS_PER_BLOCK\": 32, \"num_warps\": 16}, num_stages=4),\n        triton.Config({\"BLOCK\": HIDDEN_SIZE, \"ROWS_PER_BLOCK\": 64, \"num_warps\": 16}, num_stages=5),\n    ],\n    key=[\"hidden\"],   # hidden size is the only runtime‑varying dimension\n)\n@triton.jit\ndef _rmsnorm_kernel(\n    x_ptr,          # [batch, hidden]    (bf16)\n    w_ptr,          # [hidden]           (bf16)\n    o_ptr,          # [batch, hidden]    (bf16)\n    stride_x,       # stride between rows in x\n    stride_o,       # stride between rows in o\n    batch,          # total number of rows (runtime int)\n    eps: tl.constexpr,        # epsilon for numerical stability\n    hidden: tl.constexpr,     # hidden size (128)\n    ROWS_PER_BLOCK: tl.constexpr,\n    BLOCK: tl.constexpr,\n):\n    \"\"\"\n    Triple‑buffered RMSNorm.\n    Each program processes ``ROWS_PER_BLOCK`` consecutive rows.\n    Three register buffers keep rows i, i+1 and i+2 in flight,\n    so that the load of row i+3 can be started while row i is still being reduced.\n    \"\"\"\n    pid = tl.program_id(axis=0)               # tile index\n    base_row = pid * ROWS_PER_BLOCK            # first row handled by this program\n\n    # ------------------------------------------------------------------- #\n    # Offsets & mask for hidden dimension (BLOCK == hidden)\n    # ------------------------------------------------------------------- #\n    offs = tl.arange(0, BLOCK)\n    hidden_mask = offs < hidden\n\n    # ------------------------------------------------------------------- #\n    # Load weight once (bf16 → fp32)\n    # ------------------------------------------------------------------- #\n    w_bf16 = tl.load(w_ptr + offs, mask=hidden_mask, other=0.0)\n    w_f32 = w_bf16.to(tl.float32)\n\n    # ------------------------------------------------------------------- #\n    # Triple‑buffer pre‑load: rows base, base+1, base+2\n    # ------------------------------------------------------------------- #\n    r0 = base_row + 0\n    r1 = base_row + 1\n    r2 = base_row + 2\n\n    m0 = hidden_mask & (r0 < batch)\n    m1 = hidden_mask & (r1 < batch)\n    m2 = hidden_mask & (r2 < batch)\n\n    buf0 = tl.load(x_ptr + r0 * stride_x + offs, mask=m0, other=0.0).to(tl.float32)\n    buf1 = tl.load(x_ptr + r1 * stride_x + offs, mask=m1, other=0.0).to(tl.float32)\n    buf2 = tl.load(x_ptr + r2 * stride_x + offs, mask=m2, other=0.0).to(tl.float32)\n\n    # ------------------------------------------------------------------- #\n    # Main loop – each iteration consumes the oldest buffer and pre‑fetches\n    # the row that will be needed three steps later (i+3)\n    # ------------------------------------------------------------------- #\n    for i in range(ROWS_PER_BLOCK):\n        cur_row = base_row + i\n        cur_mask = hidden_mask & (cur_row < batch)\n\n        # ---- select the correct buffer -------------------------------------------------\n        if i % 3 == 0:\n            cur_buf = buf0\n        elif i % 3 == 1:\n            cur_buf = buf1\n        else:               # i % 3 == 2\n            cur_buf = buf2\n\n        # ---- RMS‑Norm computation ------------------------------------------------------\n        rsq   = cur_buf * cur_buf\n        mean  = tl.sum(rsq) / hidden\n        inv_r = tl.rsqrt(mean + eps)\n        y_f32 = (cur_buf * inv_r) * w_f32\n        y_bf16 = y_f32.to(tl.bfloat16)\n\n        # ---- store --------------------------------------------------------------------\n        o_ptr_cur = o_ptr + cur_row * stride_o + offs\n        tl.store(o_ptr_cur, y_bf16, mask=cur_mask)\n\n        # ---- pre‑fetch row (cur_row + 3) into the buffer we have just used -------------\n        next_row = cur_row + 3\n        next_mask = hidden_mask & (next_row < batch)\n\n        if i % 3 == 0:\n            # we just used buf0 → refill it\n            buf0 = tl.load(x_ptr + next_row * stride_x + offs,\n                           mask=next_mask, other=0.0).to(tl.float32)\n        elif i % 3 == 1:\n            buf1 = tl.load(x_ptr + next_row * stride_x + offs,\n                           mask=next_mask, other=0.0).to(tl.float32)\n        else:\n            buf2 = tl.load(x_ptr + next_row * stride_x + offs,\n                           mask=next_mask, other=0.0).to(tl.float32)\n\n\n# --------------------------------------------------------------------------- #\n# Python wrapper – identical API to the reference implementation\n# --------------------------------------------------------------------------- #\ndef run(*args, **kwargs):\n    \"\"\"\n    Triton‑accelerated RMSNorm for hidden_size = 128 (bf16) with\n    batch‑tiling, weight‑reuse and a triple‑buffered software pipeline.\n\n    Parameters (positional or keyword):\n        hidden_states : torch.Tensor[batch, 128] (bfloat16)\n        weight        : torch.Tensor[128]      (bfloat16)\n\n    Returns:\n        torch.Tensor[batch, 128] (bfloat16) – on the same device as ``hidden_states``.\n    \"\"\"\n    # --------------------------------------------------------------- #\n    # Argument handling\n    # --------------------------------------------------------------- #\n    if len(args) + len(kwargs) < 2:\n        raise TypeError(\n            \"run() missing required arguments: 'hidden_states' and 'weight'\"\n        )\n    hidden_states = kwargs.pop(\"hidden_states\") if \"hidden_states\" in kwargs else args[0]\n    weight = kwargs.pop(\"weight\") if \"weight\" in kwargs else (\n        args[1] if len(args) > 1 else None\n    )\n    if weight is None:\n        raise TypeError(\"run() missing required argument: 'weight'\")\n    if kwargs:\n        raise TypeError(f\"run() got unexpected keyword arguments {list(kwargs.keys())}\")\n\n    # --------------------------------------------------------------- #\n    # Shape / dtype checks\n    # --------------------------------------------------------------- #\n    if hidden_states.ndim != 2:\n        raise ValueError(\"hidden_states must be a 2‑D tensor [batch, hidden]\")\n    batch, hidden = hidden_states.shape\n    if hidden != HIDDEN_SIZE:\n        raise ValueError(f\"hidden dimension must be {HIDDEN_SIZE}\")\n    if weight.shape != (HIDDEN_SIZE,):\n        raise ValueError(f\"weight shape must be ({HIDDEN_SIZE},)\")\n    if hidden_states.dtype != torch.bfloat16 or weight.dtype != torch.bfloat16:\n        raise ValueError(\"both inputs must be of dtype torch.bfloat16\")\n\n    # --------------------------------------------------------------- #\n    # Fallback to PyTorch when CUDA is unavailable\n    # --------------------------------------------------------------- #\n    if not torch.cuda.is_available():\n        x = hidden_states.to(torch.float32)\n        inv_rms = torch.rsqrt(x.pow(2).mean(dim=-1, keepdim=True) + EPS)\n        y = (x * inv_rms) * weight.to(torch.float32)\n        return y.to(hidden_states.dtype)\n\n    # --------------------------------------------------------------- #\n    # Prepare contiguous GPU tensors and allocate output\n    # --------------------------------------------------------------- #\n    device = hidden_states.device\n    x_gpu = hidden_states if hidden_states.is_cuda else hidden_states.to(device)\n    w_gpu = weight if weight.is_cuda else weight.to(device)\n\n    x_gpu = x_gpu.contiguous()\n    w_gpu = w_gpu.contiguous()\n    out_gpu = torch.empty_like(x_gpu)\n\n    # --------------------------------------------------------------- #\n    # Kernel launch – grid = ceil(batch / ROWS_PER_BLOCK)\n    # --------------------------------------------------------------- #\n    grid = lambda META: ((batch + META[\"ROWS_PER_BLOCK\"] - 1) // META[\"ROWS_PER_BLOCK\"],)\n    _rmsnorm_kernel[grid](\n        x_gpu,\n        w_gpu,\n        out_gpu,\n        x_gpu.stride(0),\n        out_gpu.stride(0),\n        batch,\n        EPS,\n        HIDDEN_SIZE,\n        # ROWS_PER_BLOCK and BLOCK are injected automatically from the chosen config\n    )\n\n    # --------------------------------------------------------------- #\n    # Return tensor on the original device\n    # --------------------------------------------------------------- #\n    return out_gpu if device.type == \"cuda\" else out_gpu.cpu()\n\n\n# --------------------------------------------------------------------------- #\n# Public API\n# --------------------------------------------------------------------------- #\n__all__ = [\"run\"]"
    }
  ],
  "description": "9dcbd60b2b944e948687b2089fe98b6a_plan_1_0"
}