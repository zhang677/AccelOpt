{
  "name": "e1b62bdfc932440ba1575d295bd8e01d",
  "definition": "gqa_paged_prefill_causal_h32_kv8_d128_ps1",
  "author": "AccelOpt",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "H100"
    ],
    "entry_point": "main.py::run",
    "dependencies": []
  },
  "sources": [
    {
      "path": "main.py",
      "content": "# ==============================================================\n# gqa_paged_prefill_causal_h32_kv8_d128_ps1 – optimized version\n# ==============================================================\n\nimport math\nimport inspect\nimport torch\nimport triton\nimport triton.language as tl\n\n\n# ----------------------------------------------------------------------\n# 1️⃣  Fused‑heads Triton kernel (V‑load → dot product)\n# ----------------------------------------------------------------------\n@triton.autotune(\n    configs=[\n        # baseline configs\n        triton.Config({'BLOCK_N': 64},  num_warps=4, num_stages=2),\n        triton.Config({'BLOCK_N': 128}, num_warps=4, num_stages=2),\n        triton.Config({'BLOCK_N': 256}, num_warps=4, num_stages=2),\n        triton.Config({'BLOCK_N': 64},  num_warps=8, num_stages=2),\n        triton.Config({'BLOCK_N': 128}, num_warps=8, num_stages=2),\n        triton.Config({'BLOCK_N': 256}, num_warps=8, num_stages=2),\n\n        # more aggressive configs (larger BLOCK_N, more warps)\n        triton.Config({'BLOCK_N': 512}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_N': 512}, num_warps=8, num_stages=4),\n        triton.Config({'BLOCK_N': 1024}, num_warps=8, num_stages=4),   # new option\n    ],\n    key=['total_q'],                # autotuning key (total_q known at launch time)\n)\n@triton.jit\ndef _gqa_paged_prefill_causal_kernel_fused(\n    # ------------------------------------------------------------------\n    # 0️⃣  Pointers\n    # ------------------------------------------------------------------\n    Q, K_cache, V_cache,\n    QO_indptr, KV_indptr, KV_indices,\n    Q_seq_idx_map, sm_scale,\n    Output, LSE,\n    # ------------------------------------------------------------------\n    # 1️⃣  Strides\n    # ------------------------------------------------------------------\n    stride_q_total, stride_q_head, stride_q_dim,\n    stride_k_page, stride_k_ps, stride_k_head, stride_k_dim,\n    stride_v_page, stride_v_ps, stride_v_head, stride_v_dim,\n    # ------------------------------------------------------------------\n    # 2️⃣  Meta‑data\n    # ------------------------------------------------------------------\n    total_q,\n    num_qo_heads,\n    num_kv_heads,\n    # ------------------------------------------------------------------\n    # 3️⃣  Compile‑time constants\n    # ------------------------------------------------------------------\n    GQA_RATIO: tl.constexpr,          # = num_qo_heads // num_kv_heads\n    HEAD_DIM: tl.constexpr,           # 128\n    BLOCK_N: tl.constexpr,\n    PAGE_SIZE: tl.constexpr,          # 1 (hard‑coded)\n):\n    \"\"\"\n    One program = one (global query token, KV‑head) pair.\n    Grid = (total_q, num_kv_heads)\n    Inside the program we loop over the GQA_RATIO query heads that share the same KV‑head.\n    \"\"\"\n\n    # -------------------------------------------------\n    # 0️⃣ Identify token & KV‑head\n    # -------------------------------------------------\n    global_q_idx = tl.program_id(0)        # [0, total_q)\n    kv_head_idx   = tl.program_id(1)       # [0, num_kv_heads)\n\n    # -------------------------------------------------\n    # 1️⃣ Locate the owning sequence (pre‑computed map)\n    # -------------------------------------------------\n    seq_idx = tl.load(Q_seq_idx_map + global_q_idx)          # int32\n    q_start = tl.load(QO_indptr + seq_idx)                  # int32\n    q_end   = tl.load(QO_indptr + seq_idx + 1)              # int32\n    kv_start = tl.load(KV_indptr + seq_idx)                # int32\n    kv_end   = tl.load(KV_indptr + seq_idx + 1)            # int32\n\n    # -------------------------------------------------\n    # 2️⃣ Causal limits\n    # -------------------------------------------------\n    num_q_tokens = q_end - q_start\n    num_kv_tokens = kv_end - kv_start\n    q_idx_local = global_q_idx - q_start                # position inside the sequence\n    delta = num_kv_tokens - num_q_tokens\n    max_kv_idx_for_q = q_idx_local + delta + 1          # inclusive count of KV entries this query can attend to\n\n    # ------------------------------------------------------------------\n    # 3️⃣ Edge case: no keys to attend to → store zero / -inf and exit\n    # ------------------------------------------------------------------\n    if max_kv_idx_for_q <= 0:\n        head_base = kv_head_idx * GQA_RATIO\n        offs_h = tl.arange(0, GQA_RATIO)\n        qo_head_idxs = head_base + offs_h                     # [GQA_RATIO]\n\n        offs_d = tl.arange(0, HEAD_DIM)\n        out_ptr = Output + global_q_idx * stride_q_total \\\n                         + qo_head_idxs[:, None] * stride_q_head \\\n                         + offs_d[None, :]\n        tl.store(out_ptr, tl.zeros([GQA_RATIO, HEAD_DIM], dtype=tl.bfloat16))\n        lse_ptr = LSE + global_q_idx * num_qo_heads + qo_head_idxs\n        tl.store(lse_ptr, -float(\"inf\"))\n        return\n\n    # -------------------------------------------------\n    # 4️⃣ Load Q for ALL heads that share this KV‑head\n    # -------------------------------------------------\n    head_base = kv_head_idx * GQA_RATIO\n    offs_h = tl.arange(0, GQA_RATIO)                 # [0 .. GQA_RATIO-1]\n    qo_head_idxs = head_base + offs_h                # absolute query‑head indices, shape [GQA_RATIO]\n\n    offs_d = tl.arange(0, HEAD_DIM)\n    q_ptr = Q + global_q_idx * stride_q_total \\\n              + qo_head_idxs[:, None] * stride_q_head \\\n              + offs_d[None, :]\n    q = tl.load(q_ptr, mask=True).to(tl.float32)    # [GQA_RATIO, HEAD_DIM]\n\n    # -------------------------------------------------\n    # 5️⃣ Allocate per‑head accumulators\n    # -------------------------------------------------\n    acc = tl.zeros([GQA_RATIO, HEAD_DIM], dtype=tl.float32)   # weighted sum of V\n    m_i = tl.full([GQA_RATIO], -float(\"inf\"), dtype=tl.float32)   # running max per head\n    l_i = tl.zeros([GQA_RATIO], dtype=tl.float32)                # running exp‑sum per head\n\n    # -------------------------------------------------\n    # 6️⃣ Main KV‑loop (block‑wise)\n    # -------------------------------------------------\n    for n_offset in range(0, max_kv_idx_for_q, BLOCK_N):\n        # ---- offsets & validity mask for the block ----\n        offs_n = n_offset + tl.arange(0, BLOCK_N)                # [BLOCK_N]\n        kv_idx = kv_start + offs_n                               # absolute KV indices\n        valid = offs_n < max_kv_idx_for_q                         # respects causal limit\n        in_range = kv_idx < kv_end                               # safety (should be true)\n\n        # ---- page ids (gather) ----\n        page_ids = tl.load(KV_indices + kv_idx,\n                           mask=valid & in_range,\n                           other=0)                            # [BLOCK_N]\n\n        # ---- load K block (shared for all heads) ----\n        k_ptr = K_cache + (page_ids[:, None] * stride_k_page +\n                           kv_head_idx * stride_k_head +\n                           offs_d[None, :])\n        k = tl.load(k_ptr,\n                    mask=valid[:, None] & in_range[:, None],\n                    other=0.0).to(tl.float32)                 # [BLOCK_N, HEAD_DIM]\n\n        # ---- compute dot‑products Q·Kᵀ for every head (new, fused version) ----\n        # q : [GQA_RATIO, HEAD_DIM] , k : [BLOCK_N, HEAD_DIM] → need Kᵀ : [HEAD_DIM, BLOCK_N]\n        s = tl.dot(q, tl.trans(k))                               # [GQA_RATIO, BLOCK_N]\n\n        # ---- scale and mask ----\n        s = s * sm_scale\n        s = tl.where(valid, s, -float(\"inf\"))\n\n        # ---- online soft‑max per head ----\n        m_new = tl.maximum(m_i, tl.max(s, axis=1))                # [GQA_RATIO]\n        p = tl.exp(s - m_new[:, None])                           # [GQA_RATIO, BLOCK_N]\n        l_new = tl.exp(m_i - m_new) * l_i + tl.sum(p, axis=1)    # [GQA_RATIO]\n\n        # ---- rescale previous accumulator before adding new contribution ----\n        acc = acc * tl.exp(m_i - m_new)[:, None]\n\n        # ---- load V block (contiguous in BLOCK_N dimension) ----\n        v_ptr = V_cache + (page_ids[:, None] * stride_v_page +\n                           kv_head_idx * stride_v_head +\n                           offs_d[None, :])\n        v = tl.load(v_ptr,\n                    mask=valid[:, None] & in_range[:, None],\n                    other=0.0).to(tl.float32)                 # [BLOCK_N, HEAD_DIM]\n\n        # ---- accumulate weighted V via a batched matrix‑multiply ----\n        # p: [GQA_RATIO, BLOCK_N] , v: [BLOCK_N, HEAD_DIM]\n        # result: [GQA_RATIO, HEAD_DIM]\n        acc_update = tl.dot(p, v)                                # fp32\n        acc = acc + acc_update\n\n        # ---- commit new running statistics for next iteration ----\n        m_i = m_new\n        l_i = l_new\n\n    # -------------------------------------------------\n    # 7️⃣ Finalise per‑head results\n    # -------------------------------------------------\n    o = acc / l_i[:, None]                 # [GQA_RATIO, HEAD_DIM]  (fp32)\n    lse = m_i + tl.log(l_i)                # natural‑log\n    lse = lse * 1.4426950408889634         # convert to log₂\n\n    # -------------------------------------------------\n    # 8️⃣ Store results\n    # -------------------------------------------------\n    out_ptr = Output + global_q_idx * stride_q_total \\\n                       + qo_head_idxs[:, None] * stride_q_head \\\n                       + offs_d[None, :]\n    tl.store(out_ptr, o.to(tl.bfloat16))\n\n    lse_ptr = LSE + global_q_idx * num_qo_heads + qo_head_idxs\n    tl.store(lse_ptr, lse)\n\n\n# ----------------------------------------------------------------------\n# 2️⃣  Host‑side wrapper (API unchanged)\n# ----------------------------------------------------------------------\ndef gqa_paged_prefill_causal_h32_kv8_d128_ps1(\n    q: torch.Tensor,\n    k_cache: torch.Tensor,\n    v_cache: torch.Tensor,\n    qo_indptr: torch.Tensor,\n    kv_indptr: torch.Tensor,\n    kv_indices: torch.Tensor,\n    sm_scale: float | None = None,\n) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Prepare strides, sequence‑map and launch the fused‑heads Triton kernel.\n    \"\"\"\n    # ---- validation -------------------------------------------------\n    assert q.dim() == 3, \"q must be a 3‑D tensor\"\n    assert k_cache.dim() == 4, \"k_cache must be a 4‑D tensor\"\n    assert v_cache.dim() == 4, \"v_cache must be a 4‑D tensor\"\n    assert q.dtype == torch.bfloat16\n    assert k_cache.dtype == torch.bfloat16\n    assert v_cache.dtype == torch.bfloat16\n    assert qo_indptr.dtype == torch.int32\n    assert kv_indptr.dtype == torch.int32\n    assert kv_indices.dtype == torch.int32\n\n    total_q, num_qo_heads, head_dim = q.shape\n    num_pages, page_size, num_kv_heads, _ = k_cache.shape\n\n    # ---- constants -------------------------------------------------\n    assert num_qo_heads == 32\n    assert num_kv_heads == 8\n    assert head_dim == 128\n    assert page_size == 1\n\n    if sm_scale is None:\n        sm_scale = 1.0 / math.sqrt(head_dim)\n\n    # ---- allocate outputs -------------------------------------------\n    output = torch.empty_like(q)                                 # bf16\n    lse = torch.empty((total_q, num_qo_heads), dtype=torch.float32, device=q.device)\n\n    # ---- pre‑compute map: global query idx → sequence idx ----------\n    batch_size = qo_indptr.numel() - 1\n    q_seq_len = qo_indptr[1:] - qo_indptr[:-1]                    # lengths per sequence\n    q_seq_idx_map = torch.arange(batch_size, device=q.device, dtype=torch.int32) \\\n                        .repeat_interleave(q_seq_len)\n\n    # ---- launch grid ------------------------------------------------\n    grid = (total_q, num_kv_heads)   # one program per (query token, KV‑head)\n\n    _gqa_paged_prefill_causal_kernel_fused[grid](\n        # tensors\n        q, k_cache, v_cache,\n        qo_indptr, kv_indptr, kv_indices,\n        q_seq_idx_map, sm_scale,\n        output, lse,\n        # strides\n        q.stride(0), q.stride(1), q.stride(2),\n        k_cache.stride(0), k_cache.stride(1), k_cache.stride(2), k_cache.stride(3),\n        v_cache.stride(0), v_cache.stride(1), v_cache.stride(2), v_cache.stride(3),\n        # meta‑data\n        total_q,\n        num_qo_heads,\n        num_kv_heads,\n        # compile‑time constants\n        GQA_RATIO=num_qo_heads // num_kv_heads,   # 4\n        HEAD_DIM=head_dim,\n        PAGE_SIZE=page_size,\n    )\n    return output, lse\n\n\n# ----------------------------------------------------------------------\n# 3️⃣  Public entry point `run` – mirrors the reference implementation\n# ----------------------------------------------------------------------\ndef run(*args, **kwargs):\n    \"\"\"\n    Validates arguments, moves everything to CUDA, calls the Triton\n    implementation and returns results on the original device of ``q``.\n    \"\"\"\n    # -------------------------------------------------\n    # 1️⃣ Bind arguments to the core signature\n    # -------------------------------------------------\n    sig = inspect.signature(gqa_paged_prefill_causal_h32_kv8_d128_ps1)\n    try:\n        bound = sig.bind(*args, **kwargs)\n        bound.apply_defaults()\n    except TypeError as e:\n        raise TypeError(f\"Argument binding error: {e}\") from e\n\n    # -------------------------------------------------\n    # 2️⃣ Extract tensors & optional scalar\n    # -------------------------------------------------\n    q = bound.arguments[\"q\"]\n    k_cache = bound.arguments[\"k_cache\"]\n    v_cache = bound.arguments[\"v_cache\"]\n    qo_indptr = bound.arguments[\"qo_indptr\"]\n    kv_indptr = bound.arguments[\"kv_indptr\"]\n    kv_indices = bound.arguments[\"kv_indices\"]\n    sm_scale = bound.arguments.get(\"sm_scale\", None)\n\n    # -------------------------------------------------\n    # 3️⃣ Ensure a CUDA device is available\n    # -------------------------------------------------\n    if not torch.cuda.is_available():\n        raise RuntimeError(\"Triton kernels require a CUDA device.\")\n\n    # -------------------------------------------------\n    # 4️⃣ Move everything to the (first) CUDA tensor's device\n    # -------------------------------------------------\n    target_device = next(\n        (t.device for t in (q, k_cache, v_cache, qo_indptr, kv_indptr, kv_indices)\n         if isinstance(t, torch.Tensor) and t.is_cuda),\n        torch.device(\"cuda\"),\n    )\n    q = q.to(target_device)\n    k_cache = k_cache.to(target_device)\n    v_cache = v_cache.to(target_device)\n    qo_indptr = qo_indptr.to(target_device)\n    kv_indptr = kv_indptr.to(target_device)\n    kv_indices = kv_indices.to(target_device)\n\n    # -------------------------------------------------\n    # 5️⃣ Call the optimized Triton implementation\n    # -------------------------------------------------\n    output, lse = gqa_paged_prefill_causal_h32_kv8_d128_ps1(\n        q, k_cache, v_cache, qo_indptr, kv_indptr, kv_indices, sm_scale\n    )\n\n    # -------------------------------------------------\n    # 6️⃣ Return results on the original device of ``q``\n    # -------------------------------------------------\n    orig_device = bound.arguments[\"q\"].device\n    return output.to(orig_device), lse.to(orig_device)"
    }
  ],
  "description": "e79ef19352724293bb9c220c8fd0bcdb_plan_1_1"
}