{
  "name": "0ef390ece99346faa62f4db7af2df445",
  "definition": "mla_paged_prefill_causal_h16_ckv512_kpe64_ps1",
  "author": "AccelOpt",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "H100"
    ],
    "entry_point": "main.py::run",
    "dependencies": []
  },
  "sources": [
    {
      "path": "main.py",
      "content": "# -------------------------------------------------------------\n#  Optimized Triton kernel  (multi‑head per block)\n#  – merges HEADS_PER_BLOCK heads inside a single block\n#  – keeps the original precision (bf16 for I/O, fp32 inside)\n# -------------------------------------------------------------\nimport math\nimport torch\nimport triton\nimport triton.language as tl\n\n# ------------------------------------------------------------------\n#  Autotuned launch‑parameter configuration\n# ------------------------------------------------------------------\nHEADS_PER_BLOCK_DEFAULT = 4\n\n@triton.autotune(\n    configs=[\n        # fast configs (2‑stage)\n        triton.Config(\n            {\"BLOCK_N\": 128, \"BLOCK_DC\": 128, \"BLOCK_DK\": 64, \"HEADS_PER_BLOCK\": HEADS_PER_BLOCK_DEFAULT},\n            num_warps=8,\n            num_stages=2,\n        ),\n        triton.Config(\n            {\"BLOCK_N\": 64, \"BLOCK_DC\": 128, \"BLOCK_DK\": 64, \"HEADS_PER_BLOCK\": HEADS_PER_BLOCK_DEFAULT},\n            num_warps=4,\n            num_stages=2,\n        ),\n        # larger KV‑tile with 3‑stage pipeline\n        triton.Config(\n            {\"BLOCK_N\": 256, \"BLOCK_DC\": 128, \"BLOCK_DK\": 64, \"HEADS_PER_BLOCK\": HEADS_PER_BLOCK_DEFAULT},\n            num_warps=8,\n            num_stages=3,\n        ),\n    ],\n    key=[\"total_q\", \"H\", \"D_CKV\", \"D_KPE\"],\n)\n@triton.jit\ndef mla_paged_prefill_causal_h16_ckv512_kpe64_ps1_kernel(\n    q_nope_ptr,          # bf16   [total_q, H, D_CKV]\n    q_pe_ptr,            # bf16   [total_q, H, D_KPE]\n    ckv_ptr,             # bf16   [num_pages, D_CKV]\n    kpe_ptr,             # bf16   [num_pages, D_KPE]\n    qo_indptr_ptr,       # i32    [len_indptr]\n    kv_indptr_ptr,       # i32    [len_indptr]\n    kv_indices_ptr,      # i32    [num_kv_indices]\n    q_to_seq_ptr,        # i32    [total_q]\n    sm_scale,            # f32\n    output_ptr,          # bf16   [total_q, H, D_CKV]\n    lse_ptr,             # f32    [total_q, H]\n    total_q,\n    len_indptr,\n    num_pages,\n    num_kv_indices,\n    H: tl.constexpr,           # =16\n    D_CKV: tl.constexpr,       # =512\n    D_KPE: tl.constexpr,       # =64\n    BLOCK_N: tl.constexpr,     # KV‑tile size (64/128/256)\n    BLOCK_DC: tl.constexpr,    # sub‑tile of CKV (4×128 = 512)\n    BLOCK_DK: tl.constexpr,    # sub‑tile of KPE (covers whole KPE)\n    HEADS_PER_BLOCK: tl.constexpr,\n):\n    # ------------------------------------------------------------------\n    #  Program identifier (single‑dimensional)\n    # ------------------------------------------------------------------\n    pid = tl.program_id(0)                 # linear block id\n\n    # Number of head‑blocks per query\n    h_blocks = (H + HEADS_PER_BLOCK - 1) // HEADS_PER_BLOCK\n\n    # --------------------------------------------------------------\n    #  Decode which query and which head‑block this program works on\n    # --------------------------------------------------------------\n    qid          = pid // h_blocks                      # [0, total_q)\n    head_block   = pid % h_blocks\n    head_start   = head_block * HEADS_PER_BLOCK\n    head_end     = tl.minimum(head_start + HEADS_PER_BLOCK, H)\n\n    # Early‑exit if this block is out of range (guard against padding)\n    if qid >= total_q:\n        return\n\n    # --------------------------------------------------------------\n    #  Sequence (batch) id for this query\n    # --------------------------------------------------------------\n    seq_id = tl.load(q_to_seq_ptr + qid).to(tl.int32)\n\n    # --------------------------------------------------------------\n    #  Query range of the sequence\n    # --------------------------------------------------------------\n    q_start = tl.load(qo_indptr_ptr + seq_id).to(tl.int32)\n    q_end   = tl.load(qo_indptr_ptr + seq_id + 1).to(tl.int32)\n    q_len   = q_end - q_start\n    q_rel   = qid - q_start                 # position of this query inside its seq\n\n    # --------------------------------------------------------------\n    #  KV range of the sequence\n    # --------------------------------------------------------------\n    kv_beg = tl.load(kv_indptr_ptr + seq_id).to(tl.int32)\n    kv_end = tl.load(kv_indptr_ptr + seq_id + 1).to(tl.int32)\n    kv_len = kv_end - kv_beg\n\n    # Early‑exit flags (same for every head in the block)\n    do_work = (kv_len > 0) & (q_len > 0)\n\n    # --------------------------------------------------------------\n    #  Causal‑mask parameters (same for every head)\n    # --------------------------------------------------------------\n    prefix_len    = kv_len - q_len                # tokens already cached\n    query_abs_pos = prefix_len + q_rel            # absolute position of this query\n\n    # ------------------------------------------------------------------\n    #  Streaming softmax (base‑2) helpers\n    # ------------------------------------------------------------------\n    inv_ln2 = 1.4426950408889634          # 1 / ln(2)\n\n    # ------------------------------------------------------------------\n    #  Loop over the heads that belong to this block\n    # ------------------------------------------------------------------\n    for h in range(head_start, head_end):\n        # --------------------------------------------------------------\n        #  Load Q for this head (hoisted – reused for all KV tiles)\n        # --------------------------------------------------------------\n        qn_base = (qid * H + h) * D_CKV        # start of q_nope row\n        qp_base = (qid * H + h) * D_KPE        # start of q_pe   row\n\n        qn_chunk0 = tl.load(q_nope_ptr + qn_base + tl.arange(0, BLOCK_DC),\n                            mask=True, other=0).to(tl.float32)\n        qn_chunk1 = tl.load(q_nope_ptr + qn_base + BLOCK_DC + tl.arange(0, BLOCK_DC),\n                            mask=True, other=0).to(tl.float32)\n        qn_chunk2 = tl.load(q_nope_ptr + qn_base + 2 * BLOCK_DC + tl.arange(0, BLOCK_DC),\n                            mask=True, other=0).to(tl.float32)\n        qn_chunk3 = tl.load(q_nope_ptr + qn_base + 3 * BLOCK_DC + tl.arange(0, BLOCK_DC),\n                            mask=True, other=0).to(tl.float32)\n\n        qp_full = tl.load(q_pe_ptr + qp_base + tl.arange(0, D_KPE),\n                         mask=True, other=0).to(tl.float32)\n\n        # --------------------------------------------------------------\n        #  Output / softmax accumulators for this head\n        # --------------------------------------------------------------\n        O0 = tl.zeros((BLOCK_DC,), dtype=tl.float32)\n        O1 = tl.zeros((BLOCK_DC,), dtype=tl.float32)\n        O2 = tl.zeros((BLOCK_DC,), dtype=tl.float32)\n        O3 = tl.zeros((BLOCK_DC,), dtype=tl.float32)\n\n        m_i2 = -float(\"inf\")          # current max (base‑2)\n        l_i2 = 0.0                     # sum of exp2 shifted by m_i2\n\n        # --------------------------------------------------------------\n        #  Main KV‑tile loop\n        # --------------------------------------------------------------\n        start = tl.zeros([], dtype=tl.int32)\n        while start < kv_len:\n            # ---- Load KV indices for the current tile ---------------------------------\n            offs_n   = start + tl.arange(0, BLOCK_N)                # token offsets inside KV\n            mask_n   = offs_n < kv_len                               # out‑of‑range guard\n            page_idx = tl.load(kv_indices_ptr + kv_beg + offs_n,\n                               mask=mask_n, other=0).to(tl.int32)   # token ids\n\n            # ---- Load the four CKV sub‑tiles (once per tile) -------------------------\n            ckv_tile0 = tl.load(\n                ckv_ptr + page_idx[:, None] * D_CKV + tl.arange(0, BLOCK_DC)[None, :],\n                mask=mask_n[:, None], other=0).to(tl.float32)\n\n            ckv_tile1 = tl.load(\n                ckv_ptr + page_idx[:, None] * D_CKV + (BLOCK_DC + tl.arange(0, BLOCK_DC))[None, :],\n                mask=mask_n[:, None], other=0).to(tl.float32)\n\n            ckv_tile2 = tl.load(\n                ckv_ptr + page_idx[:, None] * D_CKV + (2 * BLOCK_DC + tl.arange(0, BLOCK_DC))[None, :],\n                mask=mask_n[:, None], other=0).to(tl.float32)\n\n            ckv_tile3 = tl.load(\n                ckv_ptr + page_idx[:, None] * D_CKV + (3 * BLOCK_DC + tl.arange(0, BLOCK_DC))[None, :],\n                mask=mask_n[:, None], other=0).to(tl.float32)\n\n            # ---- Compute logits = q_nope·ckv + q_pe·kpe --------------------------------\n            logits_tile = tl.zeros((BLOCK_N,), dtype=tl.float32)\n\n            logits_tile += tl.sum(ckv_tile0 * qn_chunk0[None, :], axis=1)\n            logits_tile += tl.sum(ckv_tile1 * qn_chunk1[None, :], axis=1)\n            logits_tile += tl.sum(ckv_tile2 * qn_chunk2[None, :], axis=1)\n            logits_tile += tl.sum(ckv_tile3 * qn_chunk3[None, :], axis=1)\n\n            kp_tile = tl.load(\n                kpe_ptr + page_idx[:, None] * D_KPE + tl.arange(0, D_KPE)[None, :],\n                mask=mask_n[:, None], other=0).to(tl.float32)\n            logits_tile += tl.sum(kp_tile * qp_full[None, :], axis=1)\n\n            # ---- Scale, causal mask, streaming softmax (base‑2) -----------------------\n            logits_tile = logits_tile * sm_scale\n            z_tile = logits_tile * inv_ln2\n\n            causal_mask = offs_n <= query_abs_pos\n            valid_mask  = mask_n & causal_mask\n            z_tile = tl.where(valid_mask, z_tile, -float(\"inf\"))\n\n            # max for the tile (base‑2)\n            m_tile = tl.max(z_tile, axis=0)\n            m_new  = tl.maximum(m_i2, m_tile)\n\n            alpha = tl.where(m_i2 == -float(\"inf\"), 0.0, tl.exp2(m_i2 - m_new))\n\n            p_tile = tl.exp2(z_tile - m_new)\n            p_tile = tl.where(valid_mask, p_tile, 0.0)\n\n            sum_p = tl.sum(p_tile, axis=0)\n\n            # update running max / sum\n            l_i2 = l_i2 * alpha + sum_p\n            O0 = O0 * alpha\n            O1 = O1 * alpha\n            O2 = O2 * alpha\n            O3 = O3 * alpha\n            m_i2 = m_new\n\n            # ---- Accumulate weighted CKV values ------------------------------------\n            O0 += tl.sum(p_tile[:, None] * ckv_tile0, axis=0)\n            O1 += tl.sum(p_tile[:, None] * ckv_tile1, axis=0)\n            O2 += tl.sum(p_tile[:, None] * ckv_tile2, axis=0)\n            O3 += tl.sum(p_tile[:, None] * ckv_tile3, axis=0)\n\n            # --------------------------------------------------------------\n            #  Advance to the next KV tile\n            # --------------------------------------------------------------\n            start += BLOCK_N\n\n        # ------------------------------------------------------------------\n        #  Write results back (only if there is work for this head)\n        # ------------------------------------------------------------------\n        if do_work:\n            # lse in base‑2 (m_i2 + log2(l_i2))\n            lse_val = m_i2 + tl.log2(l_i2)\n\n            inv_l = 1.0 / l_i2\n            O0 = O0 * inv_l\n            O1 = O1 * inv_l\n            O2 = O2 * inv_l\n            O3 = O3 * inv_l\n\n            out_base = (qid * H + h) * D_CKV\n\n            tl.store(output_ptr + out_base + tl.arange(0, BLOCK_DC),               O0.to(tl.bfloat16), mask=True)\n            tl.store(output_ptr + out_base + BLOCK_DC + tl.arange(0, BLOCK_DC),   O1.to(tl.bfloat16), mask=True)\n            tl.store(output_ptr + out_base + 2*BLOCK_DC + tl.arange(0, BLOCK_DC), O2.to(tl.bfloat16), mask=True)\n            tl.store(output_ptr + out_base + 3*BLOCK_DC + tl.arange(0, BLOCK_DC), O3.to(tl.bfloat16), mask=True)\n\n            tl.store(lse_ptr + (qid * H + h), lse_val)\n\n\n# -------------------------------------------------------------\n#  Host‑side helper & launch wrapper (multi‑head per block)\n# -------------------------------------------------------------\ndef run(\n    q_nope: torch.Tensor,\n    q_pe: torch.Tensor,\n    ckv_cache: torch.Tensor,\n    kpe_cache: torch.Tensor,\n    qo_indptr: torch.Tensor,\n    kv_indptr: torch.Tensor,\n    kv_indices: torch.Tensor,\n    sm_scale: float | None = None,\n    heads_per_block: int = HEADS_PER_BLOCK_DEFAULT,\n) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Execute the fused‑ckv kernel with **head‑fusion**: each CUDA block processes\n    ``heads_per_block`` attention heads for the same query token.\n\n    Parameters\n    ----------\n    q_nope     : torch.Tensor  [total_q, H, D_CKV]  (bf16)\n    q_pe       : torch.Tensor  [total_q, H, D_KPE]  (bf16)\n    ckv_cache  : torch.Tensor  [num_pages, 1, D_CKV] (bf16)\n    kpe_cache  : torch.Tensor  [num_pages, 1, D_KPE] (bf16)\n    qo_indptr  : torch.Tensor  [len_indptr] (int32)\n    kv_indptr  : torch.Tensor  [len_indptr] (int32)\n    kv_indices : torch.Tensor  [num_kv_indices] (int32)\n    sm_scale   : float | None, optional\n                 Softmax scaling factor. If ``None`` the default\n                 ``1 / sqrt(D_CKV)`` (as in the original implementation) is used.\n    heads_per_block : int, optional\n                 Number of heads fused inside a single block. Must divide the\n                 total head dimension (default = 4).\n\n    Returns\n    -------\n    output : torch.Tensor   (bfloat16) shape [total_q, H, D_CKV]\n    lse    : torch.Tensor   (float32) shape [total_q, H]\n    \"\"\"\n    # ------------------------------------------------------------------\n    #  Utility: move to CUDA if input is on CPU\n    # ------------------------------------------------------------------\n    def to_cuda(t):\n        if not t.is_cuda:\n            if not torch.cuda.is_available():\n                raise RuntimeError(\n                    \"CUDA not available – all inputs must be on GPU or CUDA must be installed.\"\n                )\n            return t.cuda()\n        return t\n\n    # ------------------------------------------------------------------\n    #  Type / shape checks (mirrors baseline)\n    # ------------------------------------------------------------------\n    assert q_nope.dtype in (torch.bfloat16, torch.float16, torch.float32)\n    assert q_pe.dtype   in (torch.bfloat16, torch.float16, torch.float32)\n    assert ckv_cache.dtype in (torch.bfloat16, torch.float16, torch.float32)\n    assert kpe_cache.dtype in (torch.bfloat16, torch.float16, torch.float32)\n    assert qo_indptr.dtype == torch.int32\n    assert kv_indptr.dtype == torch.int32\n    assert kv_indices.dtype == torch.int32\n\n    # ------------------------------------------------------------------\n    #  Preserve original device (so we can copy back if caller used CPU)\n    # ------------------------------------------------------------------\n    orig_device = q_nope.device\n\n    # ------------------------------------------------------------------\n    #  Move everything to GPU and enforce contiguous layout + bf16 for\n    #  the tensors that the kernel expects.\n    # ------------------------------------------------------------------\n    q_nope    = to_cuda(q_nope).contiguous().to(torch.bfloat16)\n    q_pe      = to_cuda(q_pe).contiguous().to(torch.bfloat16)\n    ckv_cache = to_cuda(ckv_cache).contiguous().to(torch.bfloat16)\n    kpe_cache = to_cuda(kpe_cache).contiguous().to(torch.bfloat16)\n    qo_indptr = to_cuda(qo_indptr).contiguous()\n    kv_indptr = to_cuda(kv_indptr).contiguous()\n    kv_indices = to_cuda(kv_indices).contiguous()\n\n    # ------------------------------------------------------------------\n    #  Shapes / constants\n    # ------------------------------------------------------------------\n    total_q, H, D_CKV = q_nope.shape\n    D_KPE = q_pe.shape[-1]\n    page_size = ckv_cache.shape[1]\n    len_indptr = qo_indptr.shape[0]\n    batch_size = len_indptr - 1\n    num_pages = ckv_cache.shape[0]\n    num_kv_indices = kv_indices.shape[0]\n\n    assert H == 16, \"num_qo_heads must be 16\"\n    assert D_CKV == 512, \"head_dim_ckv must be 512\"\n    assert D_KPE == 64, \"head_dim_kpe must be 64\"\n    assert page_size == 1, \"page_size must be 1\"\n    assert total_q == int(qo_indptr[-1].item()), \"total_q must equal qo_indptr[-1]\"\n    assert num_kv_indices == int(kv_indptr[-1].item()), \"num_kv_indices must equal kv_indptr[-1]\"\n\n    # ------------------------------------------------------------------\n    #  Squeeze the page dimension (page_size == 1)\n    # ------------------------------------------------------------------\n    ckv_squeezed = ckv_cache.squeeze(1)      # [num_pages, 512]\n    kpe_squeezed = kpe_cache.squeeze(1)      # [num_pages, 64]\n\n    # ------------------------------------------------------------------\n    #  Default softmax scale (same as original reference if not supplied)\n    # ------------------------------------------------------------------\n    if sm_scale is None:\n        sm_scale = 1.0 / math.sqrt(float(D_CKV))\n    sm_scale = float(sm_scale)\n\n    # ------------------------------------------------------------------\n    #  Build a mapping from each query index → sequence id (batch element)\n    #  This is tiny work (O(total_q)) and is done on the host CPU.\n    # ------------------------------------------------------------------\n    q_to_seq = torch.empty((total_q,), dtype=torch.int32, device=q_nope.device)\n    qo_indptr_cpu = qo_indptr.cpu().to(torch.int64)\n    for b in range(batch_size):\n        start = int(qo_indptr_cpu[b].item())\n        end   = int(qo_indptr_cpu[b + 1].item())\n        if end > start:\n            q_to_seq[start:end] = b\n\n    # ------------------------------------------------------------------\n    #  Allocate output buffers\n    # ------------------------------------------------------------------\n    output = torch.zeros(\n        (total_q, H, D_CKV), dtype=torch.bfloat16, device=q_nope.device\n    )\n    lse = torch.full(\n        (total_q, H), -float(\"inf\"), dtype=torch.float32, device=q_nope.device\n    )\n\n    # ------------------------------------------------------------------\n    #  Launch configuration (grid = total_q * ceil(H / heads_per_block))\n    # ------------------------------------------------------------------\n    h_blocks_per_query = (H + heads_per_block - 1) // heads_per_block\n    grid = (total_q * h_blocks_per_query,)\n\n    # ------------------------------------------------------------------\n    #  Call the Triton kernel\n    # ------------------------------------------------------------------\n    mla_paged_prefill_causal_h16_ckv512_kpe64_ps1_kernel[grid](\n        q_nope,\n        q_pe,\n        ckv_squeezed,\n        kpe_squeezed,\n        qo_indptr,\n        kv_indptr,\n        kv_indices,\n        q_to_seq,\n        sm_scale,\n        output,\n        lse,\n        total_q,\n        len_indptr,\n        num_pages,\n        num_kv_indices,\n        H=H,\n        D_CKV=D_CKV,\n        D_KPE=D_KPE,\n        # BLOCK_* and HEADS_PER_BLOCK are injected automatically by @triton.autotune\n    )\n\n    # ------------------------------------------------------------------\n    #  Copy back to original device if caller gave us CPU tensors\n    # ------------------------------------------------------------------\n    if orig_device.type != \"cuda\":\n        output = output.to(orig_device)\n        lse    = lse.to(orig_device)\n\n    return output, lse"
    }
  ],
  "description": "777908358e1349bf96887e38fecb0930_plan_1_0"
}