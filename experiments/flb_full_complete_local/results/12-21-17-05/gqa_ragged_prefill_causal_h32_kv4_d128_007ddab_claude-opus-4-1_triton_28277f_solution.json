{
  "name": "40724a7193cf42eda2d03426dfaf4b50",
  "definition": "gqa_ragged_prefill_causal_h32_kv4_d128",
  "author": "AccelOpt",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "H100"
    ],
    "entry_point": "main.py::run",
    "dependencies": []
  },
  "sources": [
    {
      "path": "main.py",
      "content": "# -------------------------------------------------------------\n#  Triton implementation of ragged GQA causal pre‑fill attention\n#  (BF16 activations, FP32 accumulation, additive‑bias mask)\n# -------------------------------------------------------------\nimport torch\nimport triton\nimport triton.language as tl\nimport math\n\n\n# -------------------------------------------------------------\n#  1️⃣  Autotuned kernel\n# -------------------------------------------------------------\n@triton.autotune(\n    configs=[\n        # ---- Existing safe configs ---------------------------------\n        triton.Config(\n            {\"BLOCK_M\": 16, \"BLOCK_N\": 32, \"num_warps\": 2, \"num_stages\": 2},\n            num_stages=2,\n        ),\n        triton.Config(\n            {\"BLOCK_M\": 32, \"BLOCK_N\": 64, \"num_warps\": 4, \"num_stages\": 2},\n            num_stages=2,\n        ),\n        triton.Config(\n            {\"BLOCK_M\": 64, \"BLOCK_N\": 64, \"num_warps\": 8, \"num_stages\": 2},\n            num_stages=2,\n        ),\n        triton.Config(\n            {\"BLOCK_M\": 32, \"BLOCK_N\": 128, \"num_warps\": 8, \"num_stages\": 2},\n            num_stages=2,\n        ),\n        # ---- Deeper‑pipeline configs ---------------------------------\n        triton.Config(\n            {\"BLOCK_M\": 64, \"BLOCK_N\": 128, \"num_warps\": 16, \"num_stages\": 3},\n            num_stages=3,\n        ),\n        triton.Config(\n            {\"BLOCK_M\": 128, \"BLOCK_N\": 128, \"num_warps\": 16, \"num_stages\": 3},\n            num_stages=3,\n        ),\n        triton.Config(\n            {\"BLOCK_M\": 128, \"BLOCK_N\": 256, \"num_warps\": 16, \"num_stages\": 3},\n            num_stages=3,\n        ),\n        triton.Config(\n            {\"BLOCK_M\": 256, \"BLOCK_N\": 128, \"num_warps\": 16, \"num_stages\": 3},\n            num_stages=3,\n        ),\n        # ---- NEW aggressive configs ---------------------------------\n        triton.Config(\n            {\"BLOCK_M\": 128, \"BLOCK_N\": 256, \"num_warps\": 32, \"num_stages\": 4},\n            num_stages=4,\n        ),\n        triton.Config(\n            {\"BLOCK_M\": 256, \"BLOCK_N\": 256, \"num_warps\": 32, \"num_stages\": 4},\n            num_stages=4,\n        ),\n        triton.Config(\n            {\"BLOCK_M\": 256, \"BLOCK_N\": 512, \"num_warps\": 32, \"num_stages\": 4},\n            num_stages=4,\n        ),\n        triton.Config(\n            {\"BLOCK_M\": 128, \"BLOCK_N\": 512, \"num_warps\": 32, \"num_stages\": 4},\n            num_stages=4,\n        ),\n    ],\n    key=[\"total_q\", \"total_kv\", \"num_qo_heads\", \"num_kv_heads\"],\n)\n@triton.jit\ndef gqa_ragged_prefill_causal_kernel(\n    # -------------------------------------------------\n    # Pointers\n    # -------------------------------------------------\n    q_ptr,\n    k_ptr,\n    v_ptr,\n    output_ptr,\n    lse_ptr,\n    qo_indptr_ptr,\n    kv_indptr_ptr,\n    # -------------------------------------------------\n    # Scalars\n    # -------------------------------------------------\n    sm_scale,\n    total_q,\n    total_kv,\n    # -------------------------------------------------\n    # Strides\n    # -------------------------------------------------\n    stride_q_tok,\n    stride_q_h,\n    stride_q_d,\n    stride_kv_tok,\n    stride_kv_h,\n    stride_kv_d,\n    stride_out_tok,\n    stride_out_h,\n    stride_out_d,\n    stride_lse_tok,\n    stride_lse_h,\n    # -------------------------------------------------\n    # Compile‑time constants (filled by the autotuner)\n    # -------------------------------------------------\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_D: tl.constexpr,\n    NUM_QO_HEADS: tl.constexpr,\n    NUM_KV_HEADS: tl.constexpr,\n    GQA_RATIO: tl.constexpr,\n):\n    \"\"\"\n    Single‑kernel ragged‑batch GQA causal pre‑fill attention.\n    Grid dimensions:\n        program_id(0) – query‑tile (M dimension)\n        program_id(1) – output head index (0 … NUM_QO_HEADS‑1)\n        program_id(2) – batch index\n    The inner KV‑loop loads each KV‑tile **once** and re‑uses it for\n    all Q‑rows of the current tile.\n    \"\"\"\n    # -------------------------------------------------\n    # 0) Identify which batch we are processing\n    # -------------------------------------------------\n    batch_id = tl.program_id(2)\n\n    q_start = tl.load(qo_indptr_ptr + batch_id)\n    q_end = tl.load(qo_indptr_ptr + batch_id + 1)\n    kv_start = tl.load(kv_indptr_ptr + batch_id)\n    kv_end = tl.load(kv_indptr_ptr + batch_id + 1)\n\n    num_q_tokens = q_end - q_start\n    num_kv_tokens = kv_end - kv_start\n\n    # -------------------------------------------------\n    # 1) Program identifiers that stay constant for a block\n    # -------------------------------------------------\n    prog_id = tl.program_id(0)               # tile index in the query dimension\n    head_idx = tl.program_id(1)              # output head index (0 … NUM_QO_HEADS‑1)\n\n    kv_head = head_idx // GQA_RATIO           # which KV‑head this Q‑head uses\n    delta = num_kv_tokens - num_q_tokens      # causal‑mask offset (can be negative)\n\n    # -------------------------------------------------\n    # 2) Grid‑stride loop over the query tiles\n    # -------------------------------------------------\n    stride_q_tiles = tl.num_programs(0)\n    q_tile_start = prog_id * BLOCK_M\n\n    while q_tile_start < num_q_tokens:\n        # ------------------------------\n        # 2️⃣ Load Q tile (BLOCK_M × BLOCK_D) – BF16\n        # ------------------------------\n        q_offs_m = tl.arange(0, BLOCK_M)\n        q_offs_d = tl.arange(0, BLOCK_D)\n\n        q_mask = (q_tile_start + q_offs_m[:, None] < num_q_tokens) & (q_offs_d[None, :] < BLOCK_D)\n\n        q_ptrs = (\n            q_ptr\n            + (q_start + q_tile_start + q_offs_m[:, None]) * stride_q_tok\n            + head_idx * stride_q_h\n            + q_offs_d[None, :] * stride_q_d\n        )\n        q = tl.load(q_ptrs, mask=q_mask, other=0.0)          # BF16 → stays BF16\n\n        # ------------------------------\n        # 3️⃣ Initialise stable‑softmax accumulators for this Q‑tile\n        # ------------------------------\n        m_i = tl.full([BLOCK_M], -float(\"inf\"), dtype=tl.float32)   # max per query\n        l_i = tl.zeros([BLOCK_M], dtype=tl.float32)                # Σexp\n        acc = tl.zeros([BLOCK_M, BLOCK_D], dtype=tl.float32)       # output accumulator\n\n        # -------------------------------------------------\n        # 4️⃣ Loop over KV tiles – each tile is loaded **once**\n        # -------------------------------------------------\n        # One‑pass early‑exit bound: for the current Q‑tile we never need KV\n        # positions beyond max_kv_idx_tile, defined by the causal mask.\n        q_tile_last = tl.minimum(q_tile_start + BLOCK_M - 1, num_q_tokens - 1)\n        max_kv_idx_tile = tl.minimum(q_tile_last + 1 + delta, num_kv_tokens)\n\n        kv_block_start = tl.zeros([], dtype=tl.int32)   # scalar 0\n        while kv_block_start < max_kv_idx_tile:\n            # ----- Load K (BF16) -------------------------------------------------\n            k_offs_n = tl.arange(0, BLOCK_N)\n            k_offs_d = tl.arange(0, BLOCK_D)\n\n            k_mask = (kv_block_start + k_offs_n[:, None] < num_kv_tokens) & (\n                k_offs_d[None, :] < BLOCK_D\n            )\n            k_ptrs = (\n                k_ptr\n                + (kv_start + kv_block_start + k_offs_n[:, None]) * stride_kv_tok\n                + kv_head * stride_kv_h\n                + k_offs_d[None, :] * stride_kv_d\n            )\n            k = tl.load(k_ptrs, mask=k_mask, other=0.0)          # BF16\n\n            # ----- Causal mask as additive bias ---------------------------------\n            q_pos = q_tile_start + q_offs_m                               # absolute q indices\n            max_kv_idx = q_pos + 1 + delta                               # inclusive upper bound per query\n            kv_pos = kv_block_start + k_offs_n\n\n            mask_bias = tl.where(\n                kv_pos[None, :] < max_kv_idx[:, None],\n                tl.zeros([], dtype=tl.float32),\n                tl.full([], -1e9, dtype=tl.float32),\n            )  # [BLOCK_M, BLOCK_N]\n\n            # ----- Q·Kᵀ + bias (BF16×BF16 → FP32) ---------------------------------\n            qk = tl.dot(q, tl.trans(k), out_dtype=tl.float32) * sm_scale + mask_bias  # [BLOCK_M, BLOCK_N]\n\n            # ----- Stable soft‑max update ------------------------------------\n            m_i_new = tl.maximum(m_i, tl.max(qk, axis=1))\n            p = tl.exp(qk - m_i_new[:, None])                # un‑normalised probs (FP32)\n            alpha = tl.exp(m_i - m_i_new)                    # correction factor (FP32)\n            l_i_new = alpha * l_i + tl.sum(p, axis=1)\n\n            # ----- Load V (BF16 → FP32) --------------------------------------\n            v_offs_n = tl.arange(0, BLOCK_N)\n            v_offs_d = tl.arange(0, BLOCK_D)\n\n            v_mask = (kv_block_start + v_offs_n[:, None] < num_kv_tokens) & (\n                v_offs_d[None, :] < BLOCK_D\n            )\n            v_ptrs = (\n                v_ptr\n                + (kv_start + kv_block_start + v_offs_n[:, None]) * stride_kv_tok\n                + kv_head * stride_kv_h\n                + v_offs_d[None, :] * stride_kv_d\n            )\n            v = tl.load(v_ptrs, mask=v_mask, other=0.0)        # BF16\n            v_f32 = v.to(tl.float32)                         # convert once per tile\n\n            # ----- Accumulate ------------------------------------------------\n            acc = acc * alpha[:, None]                       # rescale previous contribution\n            acc += tl.dot(p, v_f32, out_dtype=tl.float32)    # [BLOCK_M, BLOCK_D]\n\n            # ----- Update running max / sum ---------------------------------\n            m_i = m_i_new\n            l_i = l_i_new\n\n            # ----- advance KV block -------------------------------------------------\n            kv_block_start += BLOCK_N\n\n        # -------------------------------------------------\n        # 5️⃣ Normalise and write outputs for this Q‑tile\n        # -------------------------------------------------\n        acc = acc / l_i[:, None]                     # [BLOCK_M, BLOCK_D]\n\n        # ----- Write attention output ---------------------------------------\n        out_offs_m = tl.arange(0, BLOCK_M)\n        out_offs_d = tl.arange(0, BLOCK_D)\n        out_mask = (q_tile_start + out_offs_m[:, None] < num_q_tokens) & (\n            out_offs_d[None, :] < BLOCK_D\n        )\n        out_ptrs = (\n            output_ptr\n            + (q_start + q_tile_start + out_offs_m[:, None]) * stride_out_tok\n            + head_idx * stride_out_h\n            + out_offs_d[None, :] * stride_out_d\n        )\n        tl.store(out_ptrs, acc.to(tl.bfloat16), mask=out_mask)\n\n        # ----- Write LSE (log‑sum‑exp in base‑2) ---------------------------\n        lse_offs = tl.arange(0, BLOCK_M)\n        lse_mask = q_tile_start + lse_offs < num_q_tokens\n        lse_ptrs = (\n            lse_ptr\n            + (q_start + q_tile_start + lse_offs) * stride_lse_tok\n            + head_idx * stride_lse_h\n        )\n        log2_e = 1.4426950408889634      # 1 / ln(2)\n        lse_val = (m_i + tl.log(l_i)) * log2_e\n        tl.store(lse_ptrs, lse_val, mask=lse_mask)\n\n        # -------------------------------------------------\n        # 6️⃣ Advance to the next Q‑tile (grid‑stride)\n        # -------------------------------------------------\n        q_tile_start += stride_q_tiles * BLOCK_M\n\n\n# -------------------------------------------------------------\n# 2️⃣  Host‑side driver – grid size adapts to the autotuned BLOCK_M\n# -------------------------------------------------------------\ndef run(q, k, v, qo_indptr, kv_indptr, sm_scale):\n    \"\"\"\n    Triton implementation of ragged GQA causal pre‑fill attention\n    (single‑kernel, fully autotuned, with additive‑bias causal masking\n    and per‑KV‑tile reuse across Q‑tiles).\n    \"\"\"\n    # ---------------------------------------------------------\n    # 0) Make sure everything lives on CUDA\n    # ---------------------------------------------------------\n    orig_devices = [t.device for t in (q, k, v, qo_indptr, kv_indptr)]\n    needs_cuda = [not t.is_cuda for t in (q, k, v, qo_indptr, kv_indptr)]\n\n    if any(needs_cuda):\n        if needs_cuda[0]:\n            q = q.cuda()\n        if needs_cuda[1]:\n            k = k.cuda()\n        if needs_cuda[2]:\n            v = v.cuda()\n        if needs_cuda[3]:\n            qo_indptr = qo_indptr.cuda()\n        if needs_cuda[4]:\n            kv_indptr = kv_indptr.cuda()\n\n    device = q.device\n\n    # ---------------------------------------------------------\n    # 1) Extract dimensions & sanity‑check\n    # ---------------------------------------------------------\n    total_q, num_qo_heads, head_dim = q.shape\n    total_kv, num_kv_heads, _ = k.shape\n    len_indptr = qo_indptr.shape[0]\n\n    assert num_qo_heads == 32, \"num_qo_heads must be 32\"\n    assert num_kv_heads == 4,  \"num_kv_heads must be 4\"\n    assert head_dim == 128,    \"head_dim must be 128\"\n\n    assert total_q == qo_indptr[-1].item()\n    assert total_kv == kv_indptr[-1].item()\n\n    GQA_RATIO = num_qo_heads // num_kv_heads\n\n    # ---------------------------------------------------------\n    # 2) Allocate outputs\n    # ---------------------------------------------------------\n    output = torch.zeros(\n        (total_q, num_qo_heads, head_dim), dtype=torch.bfloat16, device=device\n    )\n    lse = torch.full(\n        (total_q, num_qo_heads), -float(\"inf\"), dtype=torch.float32, device=device\n    )\n\n    # ---------------------------------------------------------\n    # 3) Grid configuration – meta‑aware (depends on the chosen BLOCK_M)\n    # ---------------------------------------------------------\n    max_q_len = (qo_indptr[1:] - qo_indptr[:-1]).max().item()\n\n    # Lambda receives the concrete meta‑dictionary after autotuning.\n    grid = lambda META: (\n        triton.cdiv(max_q_len, META[\"BLOCK_M\"]),   # dim‑0 : query‑tiles\n        num_qo_heads,                             # dim‑1 : heads\n        len_indptr - 1,                           # dim‑2 : batches\n    )\n\n    # ---------------------------------------------------------\n    # 4) Launch the kernel\n    # ---------------------------------------------------------\n    gqa_ragged_prefill_causal_kernel[grid](\n        # pointers\n        q,\n        k,\n        v,\n        output,\n        lse,\n        qo_indptr,\n        kv_indptr,\n        # scalars\n        sm_scale,\n        total_q,\n        total_kv,\n        # strides\n        q.stride(0),\n        q.stride(1),\n        q.stride(2),\n        k.stride(0),\n        k.stride(1),\n        k.stride(2),\n        output.stride(0),\n        output.stride(1),\n        output.stride(2),\n        lse.stride(0),\n        lse.stride(1),\n        # compile‑time constants (autotuner will fill the rest)\n        BLOCK_D=head_dim,\n        NUM_QO_HEADS=num_qo_heads,\n        NUM_KV_HEADS=num_kv_heads,\n        GQA_RATIO=GQA_RATIO,\n    )\n\n    # ---------------------------------------------------------\n    # 5) Move results back to the original device if needed\n    # ---------------------------------------------------------\n    if not orig_devices[0].type == \"cuda\":   # q was originally on CPU\n        output = output.to(orig_devices[0])\n        lse = lse.to(orig_devices[0])\n\n    return output, lse"
    }
  ],
  "description": "9f065e5cdf5d4d15914cf79afba5329b_plan_2_1"
}