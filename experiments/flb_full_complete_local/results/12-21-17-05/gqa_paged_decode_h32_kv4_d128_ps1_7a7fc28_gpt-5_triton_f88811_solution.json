{
  "name": "54fc7efa3d6045c3a2acbb497d086e73",
  "definition": "gqa_paged_decode_h32_kv4_d128_ps1",
  "author": "AccelOpt",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "H100"
    ],
    "entry_point": "main.py::run",
    "dependencies": []
  },
  "sources": [
    {
      "path": "main.py",
      "content": "# ------------------------------------------------------------\n# Triton kernel – one program per (batch, KV‑head)\n# ------------------------------------------------------------\nimport math\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({\"BLOCK_M\": 64},  num_warps=4,  num_stages=2),\n        triton.Config({\"BLOCK_M\": 64},  num_warps=8,  num_stages=2),\n        triton.Config({\"BLOCK_M\":128},  num_warps=8,  num_stages=2),\n        triton.Config({\"BLOCK_M\":128},  num_warps=8,  num_stages=3),\n        triton.Config({\"BLOCK_M\":256},  num_warps=8,  num_stages=2),\n        triton.Config({\"BLOCK_M\":256},  num_warps=16, num_stages=2),\n    ],\n    # “max_seq_len” is the longest KV‑sequence in the batch;\n    # it is passed from the host as a launch‑time key.\n    key=[\"batch_size\", \"max_seq_len\"],\n)\n@triton.jit\ndef gqa_paged_decode_h32_kv4_d128_ps1_kernel(\n    # --------------------- pointers ---------------------\n    q_ptr,               # *bf16  [B, Hq, D]\n    k_cache_ptr,         # *bf16  [P, 1, Hk, D]\n    v_cache_ptr,         # *bf16  [P, 1, Hk, D]\n    kv_indptr_ptr,       # *i32   [B+1]\n    kv_indices_ptr,      # *i32   [N]\n    sm_scale,            # f32 scalar\n    output_ptr,          # *bf16  [B, Hq, D]\n    lse_ptr,             # *f32   [B, Hq]\n    # --------------------- sizes -----------------------\n    batch_size,          # i32\n    max_seq_len,         # i32  (longest KV length in the batch)\n    # --------------------- strides ----------------------\n    stride_q_b, stride_q_h, stride_q_d,\n    stride_k_p, stride_k_s, stride_k_h, stride_k_d,\n    stride_v_p, stride_v_s, stride_v_h, stride_v_d,\n    stride_o_b, stride_o_h, stride_o_d,\n    stride_lse_b, stride_lse_h,\n    # --------------- compile‑time constants -------------\n    BLOCK_M: tl.constexpr,          # token block size (tuned)\n    D_HEAD: tl.constexpr = 128,    # head dimension (fixed)\n    GQA_RATIO: tl.constexpr = 8,   # Qo heads per KV head (fixed)\n    NUM_KV_HEADS: tl.constexpr = 4 # number of KV heads (fixed)\n):\n    \"\"\"\n    One program processes a *single* (batch, KV‑head) tuple.\n    All `GQA_RATIO` (=8) Qo‑heads that share the KV‑head are handled\n    simultaneously (vectorised over the first dimension).\n    Heavy matmuls stay in BF16 so that Tensor‑Core WMMA is used.\n    \"\"\"\n    pid = tl.program_id(0)\n\n    # ----------------------------------------------------------------\n    # Decode (batch, KV‑head) from the linear program id\n    # ----------------------------------------------------------------\n    b   = pid // NUM_KV_HEADS          # batch index\n    kv_h = pid %  NUM_KV_HEADS          # KV‑head index\n\n    if b >= batch_size:\n        return\n\n    # ----------------------------------------------------------------\n    # KV‑range for this batch element\n    # ----------------------------------------------------------------\n    page_start = tl.load(kv_indptr_ptr + b).to(tl.int32)\n    page_end   = tl.load(kv_indptr_ptr + (b + 1)).to(tl.int32)\n    seq_len = page_end - page_start          # number of tokens for this sequence\n\n    # ----------------------------------------------------------------\n    # Early‑exit for empty sequences (store zeros & -inf LSE for the\n    # current group of Qo‑heads)\n    # ----------------------------------------------------------------\n    neg_inf = tl.full((), -float(\"inf\"), dtype=tl.float32)\n    if seq_len <= 0:\n        d_off = tl.arange(0, D_HEAD)                     # [D]\n        h_off = kv_h * GQA_RATIO + tl.arange(0, GQA_RATIO)  # [G]\n\n        # output = 0, lse = -inf for the heads in this KV‑head\n        out_ptrs = (output_ptr\n                    + b * stride_o_b\n                    + h_off[:, None] * stride_o_h\n                    + d_off[None, :] * stride_o_d)          # (G, D)\n        tl.store(out_ptrs, tl.zeros((GQA_RATIO, D_HEAD), dtype=tl.bfloat16))\n\n        lse_ptrs = (lse_ptr\n                    + b * stride_lse_b\n                    + h_off * stride_lse_h)                 # (G)\n        tl.store(lse_ptrs, tl.full((GQA_RATIO,), neg_inf, dtype=tl.float32))\n        return\n\n    # ----------------------------------------------------------------\n    # Load the Q‑vectors belonging to this KV‑head (shape GQA_RATIO × D)\n    # ----------------------------------------------------------------\n    d_off = tl.arange(0, D_HEAD)                     # [D]\n    h_off = kv_h * GQA_RATIO + tl.arange(0, GQA_RATIO)  # [G]\n    q_ptrs = (q_ptr\n              + b * stride_q_b\n              + h_off[:, None] * stride_q_h\n              + d_off[None, :] * stride_q_d)          # (G, D)\n    q_mat = tl.load(q_ptrs)                          # (G, D)  bf16   ← keep BF16\n\n    # ----------------------------------------------------------------\n    # Streaming‑softmax state per Qo‑head (stays FP32)\n    # ----------------------------------------------------------------\n    m_i = tl.full((GQA_RATIO,), neg_inf, dtype=tl.float32)   # (G)\n    l_i = tl.zeros((GQA_RATIO,), dtype=tl.float32)          # (G)\n    acc = tl.zeros((GQA_RATIO, D_HEAD), dtype=tl.float32)   # (G, D)\n\n    # ----------------------------------------------------------------\n    # Token‑block loop (K/V loaded once per block, reused for all heads)\n    # ----------------------------------------------------------------\n    token_off = tl.arange(0, BLOCK_M)\n    pos = tl.zeros((), dtype=tl.int32)\n\n    while pos < seq_len:\n        cur = pos + token_off                     # candidate token ids\n        mask = cur < seq_len                       # (M) bool\n\n        # --------------------------------------------------------\n        # Load page ids (one per token, because page_size == 1)\n        # --------------------------------------------------------\n        page_ids = tl.load(\n            kv_indices_ptr + page_start + cur,\n            mask=mask,\n            other=0\n        ).to(tl.int32)                           # (M)\n\n        # --------------------------------------------------------\n        # Load K block for the current KV‑head (shared for all heads)\n        # --------------------------------------------------------\n        k_ptrs = (k_cache_ptr\n                  + page_ids[:, None] * stride_k_p\n                  + 0 * stride_k_s\n                  + kv_h * stride_k_h\n                  + d_off[None, :] * stride_k_d)   # (M, D)\n        k_block = tl.load(k_ptrs,\n                         mask=mask[:, None],\n                         other=0)                 # (M, D) bf16\n\n        # --------------------------------------------------------\n        # Compute logits = Q · Kᵀ   (BF16 matmul → Tensor‑Core)\n        # --------------------------------------------------------\n        logits_bf16 = tl.dot(q_mat, tl.trans(k_block)) * sm_scale   # (G, M) bf16\n        logits = logits_bf16.to(tl.float32)                         # cast once for soft‑max\n\n        # mask padding positions\n        logits = tl.where(mask[None, :], logits,\n                          tl.full((), -float(\"inf\"), tl.float32))\n\n        # --------------------------------------------------------\n        # Streaming‑softmax update (still FP32)\n        # --------------------------------------------------------\n        m_curr = tl.max(logits, axis=1)                # (G)\n        m_new  = tl.maximum(m_i, m_curr)               # (G)\n\n        p = tl.exp(logits - m_new[:, None])            # (G, M) FP32\n        l_part = tl.sum(p, axis=1)                     # (G)\n\n        # update normaliser\n        l_i = l_i * tl.exp(m_i - m_new) + l_part\n        # shift accumulator with the same max‑shift\n        acc = acc * tl.exp(m_i - m_new)[:, None]\n\n        # --------------------------------------------------------\n        # Load V block (shared for all heads) – keep BF16\n        # --------------------------------------------------------\n        v_ptrs = (v_cache_ptr\n                  + page_ids[:, None] * stride_v_p\n                  + 0 * stride_v_s\n                  + kv_h * stride_v_h\n                  + d_off[None, :] * stride_v_d)   # (M, D)\n        v_block = tl.load(v_ptrs,\n                         mask=mask[:, None],\n                         other=0)                 # (M, D) bf16\n\n        # --------------------------------------------------------\n        # Weighted sum: p (FP32) * V (BF16)\n        # Cast p to BF16 so tl.dot can use Tensor‑Core\n        # --------------------------------------------------------\n        p_bf16 = p.to(tl.bfloat16)                     # (G, M) bf16\n        weighted_bf16 = tl.dot(p_bf16, v_block)        # (G, D) bf16\n        acc += weighted_bf16.to(tl.float32)            # accumulate in FP32\n\n        # --------------------------------------------------------\n        # Prepare for next block\n        # --------------------------------------------------------\n        m_i = m_new\n        pos += BLOCK_M\n\n    # ------------------------------------------------------------\n    # Finalise each Qo‑head in the group\n    # ------------------------------------------------------------\n    nonempty = l_i > 0.0\n    out = tl.where(\n        nonempty[:, None],\n        acc / l_i[:, None],\n        tl.zeros((GQA_RATIO, D_HEAD), dtype=tl.float32)\n    )\n\n    # Store outputs (shape GQA_RATIO × D) back as BF16\n    out_ptrs = (output_ptr\n                + b * stride_o_b\n                + h_off[:, None] * stride_o_h\n                + d_off[None, :] * stride_o_d)            # (G, D)\n    tl.store(out_ptrs, out.to(tl.bfloat16))\n\n    # 2‑base log‑sum‑exp per Qo‑head\n    inv_ln2 = 1.4426950408889634      # 1 / ln(2)\n    lse_val = tl.where(\n        nonempty,\n        (tl.log(l_i) + m_i) * inv_ln2,\n        tl.full((GQA_RATIO,), neg_inf, dtype=tl.float32)\n    )\n    lse_ptrs = (lse_ptr\n                + b * stride_lse_b\n                + h_off * stride_lse_h)               # (G)\n    tl.store(lse_ptrs, lse_val)\n\n\n# ------------------------------------------------------------\n# Helper – ensure tensor is on the correct CUDA device\n# ------------------------------------------------------------\ndef _ensure_cuda(t: torch.Tensor, device: torch.device) -> torch.Tensor:\n    \"\"\"Move tensor to *device* (or copy if on CPU).\"\"\"\n    if t.device.type == \"cuda\":\n        return t if t.device == device else t.to(device)\n    if not torch.cuda.is_available():\n        raise RuntimeError(\"CUDA not available – Triton kernels need a GPU.\")\n    return t.to(device, non_blocking=True)\n\n\n# ------------------------------------------------------------\n# Optimised run() – launches one program per (batch, KV‑head)\n# ------------------------------------------------------------\ndef run(\n    q: torch.Tensor,\n    k_cache: torch.Tensor,\n    v_cache: torch.Tensor,\n    kv_indptr: torch.Tensor,\n    kv_indices: torch.Tensor,\n    sm_scale: float | torch.Tensor | None = None,\n):\n    \"\"\"\n    Triton‑accelerated GQA paged decode with BF16 Tensor‑Core matmuls.\n    The API and semantics are identical to the reference implementation.\n    \"\"\"\n    # ----------------------------------------------------------------\n    # Fixed specification constants\n    # ----------------------------------------------------------------\n    HEAD_DIM = 128\n    NUM_QO_HEADS = 32\n    NUM_KV_HEADS = 4\n    PAGE_SIZE = 1\n    GQA_RATIO = NUM_QO_HEADS // NUM_KV_HEADS          # = 8\n\n    # ----------------------------------------------------------------\n    # Default softmax scale\n    # ----------------------------------------------------------------\n    if sm_scale is None:\n        sm_scale = 1.0 / math.sqrt(HEAD_DIM)\n\n    # Normalise ``sm_scale`` to a Python float (required for kernel arg)\n    if isinstance(sm_scale, (float, int)):\n        sm_scale_val = float(sm_scale)\n    elif isinstance(sm_scale, torch.Tensor):\n        if sm_scale.numel() != 1:\n            raise ValueError(\"sm_scale must be a scalar.\")\n        sm_scale_val = float(sm_scale.item())\n    else:\n        raise TypeError(\"sm_scale must be float, int or a 0‑dim torch.Tensor\")\n\n    # ----------------------------------------------------------------\n    # Basic sanity checks (mirrors the reference implementation)\n    # ----------------------------------------------------------------\n    if q.ndim != 3:\n        raise ValueError(\"q must be of shape [B, Hq, D]\")\n    batch_size, num_qo_heads, head_dim = q.shape\n    if (num_qo_heads != NUM_QO_HEADS) or (head_dim != HEAD_DIM):\n        raise AssertionError(\"q must have shape [batch, 32, 128]\")\n\n    if k_cache.ndim != 4 or v_cache.ndim != 4:\n        raise ValueError(\"k_cache / v_cache must be 4‑D\")\n    if k_cache.shape != v_cache.shape:\n        raise AssertionError(\"k_cache and v_cache must have identical shapes\")\n    num_pages, page_size, num_kv_heads, head_dim_k = k_cache.shape\n    if (num_kv_heads != NUM_KV_HEADS) or (head_dim_k != HEAD_DIM) or (page_size != PAGE_SIZE):\n        raise AssertionError(\"k_cache/v_cache shape mismatch with spec\")\n\n    if kv_indptr.ndim != 1 or kv_indices.ndim != 1:\n        raise ValueError(\"kv_indptr / kv_indices must be 1‑D\")\n    len_indptr = kv_indptr.shape[0]\n    if len_indptr != batch_size + 1:\n        raise AssertionError(\"len_indptr must be batch_size + 1\")\n    if int(kv_indptr[-1].item()) != kv_indices.shape[0]:\n        raise AssertionError(\"kv_indices length inconsistent with kv_indptr[-1]\")\n\n    # dtype checks\n    if q.dtype != torch.bfloat16:\n        raise TypeError(\"q must be bfloat16\")\n    if k_cache.dtype != torch.bfloat16 or v_cache.dtype != torch.bfloat16:\n        raise TypeError(\"k_cache and v_cache must be bfloat16\")\n    if kv_indptr.dtype != torch.int32 or kv_indices.dtype != torch.int32:\n        raise TypeError(\"kv_indptr / kv_indices must be int32\")\n\n    # ----------------------------------------------------------------\n    # Pick a CUDA device (the first tensor that already lives on GPU wins)\n    # ----------------------------------------------------------------\n    work_device = (\n        q.device if q.is_cuda else\n        k_cache.device if k_cache.is_cuda else\n        v_cache.device if v_cache.is_cuda else\n        kv_indptr.device if kv_indptr.is_cuda else\n        kv_indices.device if kv_indices.is_cuda else\n        torch.device(\"cuda\")\n    )\n\n    # ----------------------------------------------------------------\n    # Move everything to the working device and make them contiguous\n    # ----------------------------------------------------------------\n    q_dev = _ensure_cuda(q.contiguous(), work_device)\n    k_dev = _ensure_cuda(k_cache.contiguous(), work_device)\n    v_dev = _ensure_cuda(v_cache.contiguous(), work_device)\n    indptr_dev = _ensure_cuda(kv_indptr.contiguous(), work_device)\n    indices_dev = _ensure_cuda(kv_indices.contiguous(), work_device)\n\n    # ----------------------------------------------------------------\n    # Allocate output tensors on the working device\n    # ----------------------------------------------------------------\n    output_dev = torch.empty(\n        (batch_size, NUM_QO_HEADS, HEAD_DIM),\n        dtype=torch.bfloat16,\n        device=work_device,\n    )\n    lse_dev = torch.empty(\n        (batch_size, NUM_QO_HEADS),\n        dtype=torch.float32,\n        device=work_device,\n    )\n\n    # ----------------------------------------------------------------\n    # Compute the maximum sequence length – needed as a tuning key\n    # ----------------------------------------------------------------\n    indptr_cpu = indptr_dev.cpu()\n    seq_lens = indptr_cpu[1:] - indptr_cpu[:-1]\n    max_seq_len = int(seq_lens.max().item())\n\n    # ----------------------------------------------------------------\n    # Kernel launch configuration\n    # ----------------------------------------------------------------\n    # One program per (batch, KV‑head)\n    grid = (batch_size * NUM_KV_HEADS,)\n\n    gqa_paged_decode_h32_kv4_d128_ps1_kernel[grid](\n        # pointers\n        q_dev,\n        k_dev,\n        v_dev,\n        indptr_dev,\n        indices_dev,\n        sm_scale_val,\n        output_dev,\n        lse_dev,\n        # sizes / autotune key\n        batch_size,\n        max_seq_len,\n        # strides (passed in the same order as kernel signature)\n        q_dev.stride(0), q_dev.stride(1), q_dev.stride(2),\n        k_dev.stride(0), k_dev.stride(1), k_dev.stride(2), k_dev.stride(3),\n        v_dev.stride(0), v_dev.stride(1), v_dev.stride(2), v_dev.stride(3),\n        output_dev.stride(0), output_dev.stride(1), output_dev.stride(2),\n        lse_dev.stride(0), lse_dev.stride(1),\n        # compile‑time constants\n        D_HEAD=HEAD_DIM,\n        GQA_RATIO=GQA_RATIO,\n        NUM_KV_HEADS=NUM_KV_HEADS,\n    )\n\n    # ----------------------------------------------------------------\n    # Return results on the original device of ``q``\n    # ----------------------------------------------------------------\n    if work_device != q.device:\n        output = output_dev.to(q.device)\n        lse = lse_dev.to(q.device)\n    else:\n        output = output_dev\n        lse = lse_dev\n\n    return output, lse"
    }
  ],
  "description": "6e89b724a91741b3a4d754f4cc29369d_plan_3_1"
}