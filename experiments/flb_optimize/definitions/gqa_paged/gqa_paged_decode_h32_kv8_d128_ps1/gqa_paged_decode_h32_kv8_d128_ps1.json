{
  "name": "gqa_paged_decode_h32_kv8_d128_ps1",
  "op_type": "gqa_paged",
  "axes": {
    "batch_size": {
      "type": "var",
      "description": "Total number of query tokens."
    },
    "num_qo_heads": {
      "type": "const",
      "value": 32,
      "description": null
    },
    "num_kv_heads": {
      "type": "const",
      "value": 8,
      "description": null
    },
    "head_dim": {
      "type": "const",
      "value": 128,
      "description": null
    },
    "num_pages": {
      "type": "var",
      "description": null
    },
    "page_size": {
      "type": "const",
      "value": 1,
      "description": null
    },
    "len_indptr": {
      "type": "var",
      "description": "Length of kv_indptr array."
    },
    "num_kv_indices": {
      "type": "var",
      "description": "Total number of KV page indices."
    }
  },
  "inputs": {
    "q": {
      "shape": [
        "batch_size",
        "num_qo_heads",
        "head_dim"
      ],
      "dtype": "bfloat16",
      "description": null
    },
    "k_cache": {
      "shape": [
        "num_pages",
        "page_size",
        "num_kv_heads",
        "head_dim"
      ],
      "dtype": "bfloat16",
      "description": null
    },
    "v_cache": {
      "shape": [
        "num_pages",
        "page_size",
        "num_kv_heads",
        "head_dim"
      ],
      "dtype": "bfloat16",
      "description": null
    },
    "kv_indptr": {
      "shape": [
        "len_indptr"
      ],
      "dtype": "int32",
      "description": "KV page offsets for each sequence."
    },
    "kv_indices": {
      "shape": [
        "num_kv_indices"
      ],
      "dtype": "int32",
      "description": "Page IDs for KV cache lookups."
    },
    "sm_scale": {
      "shape": null,
      "dtype": "float32",
      "description": "Softmax scale. Default is (1/sqrt(head_dim))."
    }
  },
  "outputs": {
    "output": {
      "shape": [
        "batch_size",
        "num_qo_heads",
        "head_dim"
      ],
      "dtype": "bfloat16",
      "description": null
    },
    "lse": {
      "shape": [
        "batch_size",
        "num_qo_heads"
      ],
      "dtype": "float32",
      "description": "The 2-based log-sum-exp of attention logits."
    }
  },
  "reference": "import torch\nimport math\n\n\n@torch.no_grad()\ndef run(q, k_cache, v_cache, kv_indptr, kv_indices, sm_scale):\n    batch_size, num_qo_heads, head_dim = q.shape\n    _, page_size, num_kv_heads, _ = k_cache.shape\n    len_indptr = kv_indptr.shape[0]\n    num_kv_indices = kv_indices.shape[0]\n\n    # Check constants\n    assert num_qo_heads == 32\n    assert num_kv_heads == 8\n    assert head_dim == 128\n    assert page_size == 1\n\n    # Check constraints\n    assert len_indptr == batch_size + 1\n    assert num_kv_indices == kv_indptr[-1].item()\n\n    device = q.device\n\n    output = torch.zeros(\n        (batch_size, num_qo_heads, head_dim), dtype=torch.bfloat16, device=device\n    )\n    lse = torch.full(\n        (batch_size, num_qo_heads), -float(\"inf\"), dtype=torch.float32, device=device\n    )\n\n    gqa_ratio = num_qo_heads // num_kv_heads\n\n    k_cache_flat = k_cache.squeeze(1).to(\n        torch.float32\n    )  # [num_pages, num_kv_heads, head_dim]\n    v_cache_flat = v_cache.squeeze(1).to(\n        torch.float32\n    )  # [num_pages, num_kv_heads, head_dim]\n\n    for b in range(batch_size):\n        page_start = int(kv_indptr[b].item())\n        page_end = int(kv_indptr[b + 1].item())\n\n        if page_start >= page_end:\n            # No KV cache for this batch element\n            output[b].zero_()\n            continue\n\n        # Pages are the token indices for page_size=1\n        token_indices = kv_indices[page_start:page_end].to(torch.long)\n        # Number of tokens is the number of pages for page_size=1\n        num_tokens = token_indices.shape[0]\n\n        if num_tokens == 0:\n            output[b].zero_()\n            continue\n\n        # Get Q, K, V for this batch\n        k_batch = k_cache_flat[token_indices]  # [num_tokens, num_kv_heads, head_dim]\n        v_batch = v_cache_flat[token_indices]  # [num_tokens, num_kv_heads, head_dim]\n        q_batch = q[b].to(torch.float32)  # [num_qo_heads, head_dim]\n\n        for h in range(num_qo_heads):\n            # Find corresponding KV head for GQA\n            kv_head = h // gqa_ratio\n\n            q_head = q_batch[h]  # [head_dim]\n            k_head = k_batch[:, kv_head]  # [num_tokens, head_dim]\n            v_head = v_batch[:, kv_head]  # [num_tokens, head_dim]\n\n            logits = torch.matmul(q_head, k_head.T)  # [num_tokens]\n            logits_scaled = logits * sm_scale\n\n            # Compute 2-base LSE\n            lse[b, h] = torch.logsumexp(logits_scaled, dim=-1) / math.log(2.0)\n\n            attn = torch.softmax(logits_scaled, dim=-1)  # [num_tokens]\n            out_head = torch.matmul(attn, v_head)  # [head_dim]\n            output[b, h] = out_head.to(torch.bfloat16)\n\n    return output, lse",
  "tags": [
    "stage:decode",
    "status:verified",
    "model:llama-3.1-8b"
  ],
  "description": "Batched Grouped Query Attention decode with a paged KV cache. Captured from Llama-3.1-8B.",
  "constraints": [
    "len_indptr == batch_size + 1",
    "num_kv_indices == kv_indptr[-1].item()"
  ]
}