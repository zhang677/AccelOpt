{
  "name": "gqa_ragged_prefill_causal_h32_kv4_d128",
  "op_type": "gqa_ragged",
  "axes": {
    "num_qo_heads": {
      "type": "const",
      "value": 32,
      "description": null
    },
    "num_kv_heads": {
      "type": "const",
      "value": 4,
      "description": null
    },
    "head_dim": {
      "type": "const",
      "value": 128,
      "description": null
    },
    "len_indptr": {
      "type": "var",
      "description": "Length of indptr arrays. Should be the same for qo_indptr and kv_indptr (batch_size + 1)."
    },
    "total_q": {
      "type": "var",
      "description": "Total number of query tokens."
    },
    "total_kv": {
      "type": "var",
      "description": "Total key-value tokens across all sequences."
    }
  },
  "inputs": {
    "q": {
      "shape": [
        "total_q",
        "num_qo_heads",
        "head_dim"
      ],
      "dtype": "bfloat16",
      "description": null
    },
    "k": {
      "shape": [
        "total_kv",
        "num_kv_heads",
        "head_dim"
      ],
      "dtype": "bfloat16",
      "description": null
    },
    "v": {
      "shape": [
        "total_kv",
        "num_kv_heads",
        "head_dim"
      ],
      "dtype": "bfloat16",
      "description": null
    },
    "qo_indptr": {
      "shape": [
        "len_indptr"
      ],
      "dtype": "int32",
      "description": "Query offsets for each sequence."
    },
    "kv_indptr": {
      "shape": [
        "len_indptr"
      ],
      "dtype": "int32",
      "description": "Key-value offsets for each sequence."
    },
    "sm_scale": {
      "shape": null,
      "dtype": "float32",
      "description": "Softmax scale. Default is (1/sqrt(head_dim))."
    }
  },
  "outputs": {
    "output": {
      "shape": [
        "total_q",
        "num_qo_heads",
        "head_dim"
      ],
      "dtype": "bfloat16",
      "description": "Attention output tensor."
    },
    "lse": {
      "shape": [
        "total_q",
        "num_qo_heads"
      ],
      "dtype": "float32",
      "description": "The 2-based log-sum-exp of attention logits."
    }
  },
  "reference": "import torch\nimport math\n\n\n@torch.no_grad()\ndef run(q, k, v, qo_indptr, kv_indptr, sm_scale):\n    total_q, num_qo_heads, head_dim = q.shape\n    total_kv, num_kv_heads, _ = k.shape\n    len_indptr = qo_indptr.shape[0]\n\n    # Check constants\n    assert num_qo_heads == 32\n    assert num_kv_heads == 4\n    assert head_dim == 128\n\n    # Check constraints\n    assert total_q == qo_indptr[-1].item()\n    assert total_kv == kv_indptr[-1].item()\n\n    device = q.device\n\n    output = torch.zeros(\n        (total_q, num_qo_heads, head_dim), dtype=torch.bfloat16, device=device\n    )\n    lse = torch.full(\n        (total_q, num_qo_heads), -float(\"inf\"), dtype=torch.float32, device=device\n    )\n\n    gqa_ratio = num_qo_heads // num_kv_heads\n\n    q_f32 = q.to(torch.float32)\n    k_f32 = k.to(torch.float32)\n    v_f32 = v.to(torch.float32)\n\n    for b in range(len_indptr - 1):\n        q_start = int(qo_indptr[b].item())\n        q_end = int(qo_indptr[b + 1].item())\n\n        kv_start = int(kv_indptr[b].item())\n        kv_end = int(kv_indptr[b + 1].item())\n\n        if q_start >= q_end or kv_start >= kv_end:\n            # No queries or KV for this batch element\n            continue\n\n        # Get Q, K, V for this batch\n        q_batch = q_f32[q_start:q_end]  # [num_q_tokens, num_qo_heads, head_dim]\n        k_batch = k_f32[kv_start:kv_end]  # [num_kv_tokens, num_kv_heads, head_dim]\n        v_batch = v_f32[kv_start:kv_end]  # [num_kv_tokens, num_kv_heads, head_dim]\n\n        num_q_tokens = q_batch.shape[0]\n        num_kv_tokens = k_batch.shape[0]\n        delta = num_kv_tokens - num_q_tokens\n\n        k_expanded = k_batch.repeat_interleave(gqa_ratio, dim=1)\n        v_expanded = v_batch.repeat_interleave(gqa_ratio, dim=1)\n\n        # Compute attention scores: Q @ K^T\n        logits = torch.einsum('qhd,khd->qhk', q_batch, k_expanded) * sm_scale\n\n        # For position q_idx, can attend to KV positions [0, min(q_idx + 1 + delta, num_kv_tokens))\n        q_positions = torch.arange(num_q_tokens, device=device)  # [num_q_tokens]\n        kv_positions = torch.arange(num_kv_tokens, device=device)  # [num_kv_tokens]\n        \n        # Apply causal mask\n        causal_mask = kv_positions[None, :] < (q_positions[:, None] + 1 + delta)\n        logits = logits.masked_fill(~causal_mask[:, None, :], float('-inf'))\n\n        # Compute 2-base LSE\n        lse_batch = torch.logsumexp(logits, dim=-1) / math.log(2.0)\n        lse[q_start:q_end] = lse_batch\n\n        attn_weights = torch.softmax(logits, dim=-1)  # [num_q_tokens, num_qo_heads, num_kv_tokens]\n        output_batch = torch.einsum('qhk,khd->qhd', attn_weights, v_expanded)\n        output[q_start:q_end] = output_batch.to(torch.bfloat16)\n\n    return output, lse",
  "tags": [
    "stage:prefill",
    "status:verified",
    "model:qwen3-30b-a3b"
  ],
  "description": "Batched Grouped Query Attention prefill with ragged (variable-length) inputs. Causal mask is applied. Captured from Qwen3-30B-A3B during total prefill.",
  "constraints": [
    "total_q == qo_indptr[-1].item()",
    "total_kv == kv_indptr[-1].item()"
  ]
}