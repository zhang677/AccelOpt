{
  "name": "claude-opus-4-1-20250805_triton_faa4d0",
  "definition": "gemm_n2048_k4096",
  "author": "claude-opus-4-1-20250805",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "B200"
    ],
    "entry_point": "main.py::run",
    "dependencies": []
  },
  "sources": [
    {
      "path": "main.py",
      "content": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef gemm_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bn, stride_bk,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    \"\"\"\n    Compute C = A @ B.T where B is stored as [N, K]\n    \"\"\"\n    # Get program ids\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    # Create block pointers\n    rm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    rn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    rk = tl.arange(0, BLOCK_SIZE_K)\n    \n    # Create pointers to A and B\n    A = a_ptr + rm[:, None] * stride_am + rk[None, :] * stride_ak\n    B = b_ptr + rn[:, None] * stride_bn + rk[None, :] * stride_bk\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    \n    # Main loop over K dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load tiles with boundary checks\n        a_mask = (rm[:, None] < M) & ((k + rk[None, :]) < K)\n        b_mask = (rn[:, None] < N) & ((k + rk[None, :]) < K)\n        \n        a = tl.load(A, mask=a_mask, other=0.0)\n        b = tl.load(B, mask=b_mask, other=0.0)\n        \n        # Compute dot product (note: B is already transposed in memory layout)\n        acc += tl.dot(a, tl.trans(b), allow_tf32=True)\n        \n        # Advance pointers\n        A += BLOCK_SIZE_K * stride_ak\n        B += BLOCK_SIZE_K * stride_bk\n    \n    # Convert accumulator to float16\n    c = acc.to(tl.float16)\n    \n    # Store output with boundary checks\n    C = c_ptr + rm[:, None] * stride_cm + rn[None, :] * stride_cn\n    c_mask = (rm[:, None] < M) & (rn[None, :] < N)\n    tl.store(C, c, mask=c_mask)\n\n\ndef gemm_n2048_k4096(A, B):\n    \"\"\"\n    Optimized GEMM for N=2048, K=4096 on B200 GPU\n    A: [M, K] float16\n    B: [N, K] float16\n    Returns: C = A @ B.T [M, N] float16\n    \"\"\"\n    # Get dimensions\n    M, K_a = A.shape\n    N, K_b = B.shape\n    \n    assert K_a == K_b == 4096, f\"K dimension mismatch or not 4096: A.shape={A.shape}, B.shape={B.shape}\"\n    assert N == 2048, f\"N dimension must be 2048, got {N}\"\n    assert A.dtype == torch.float16 and B.dtype == torch.float16, \"Inputs must be float16\"\n    \n    # Allocate output\n    C = torch.empty((M, N), device=A.device, dtype=torch.float16)\n    \n    # Block sizes optimized for B200\n    # B200 has large shared memory and high compute throughput\n    BLOCK_SIZE_M = 128\n    BLOCK_SIZE_N = 128\n    BLOCK_SIZE_K = 64\n    \n    # Calculate grid dimensions\n    grid = lambda META: (\n        triton.cdiv(M, META['BLOCK_SIZE_M']),\n        triton.cdiv(N, META['BLOCK_SIZE_N']),\n        1\n    )\n    \n    # Launch kernel\n    gemm_kernel[grid](\n        A, B, C,\n        M, N, K_a,\n        A.stride(0), A.stride(1),\n        B.stride(0), B.stride(1),\n        C.stride(0), C.stride(1),\n        BLOCK_SIZE_M=BLOCK_SIZE_M,\n        BLOCK_SIZE_N=BLOCK_SIZE_N,\n        BLOCK_SIZE_K=BLOCK_SIZE_K,\n    )\n    \n    return C\n\n\ndef run(*args, **kwargs):\n    \"\"\"\n    Entry point function with complete device management\n    Handles both positional and keyword arguments\n    \"\"\"\n    # Handle both positional and keyword arguments\n    if len(args) == 2:\n        A, B = args\n    elif 'A' in kwargs and 'B' in kwargs:\n        A = kwargs['A']\n        B = kwargs['B']\n    else:\n        raise ValueError(\"Expected either 2 positional arguments (A, B) or keyword arguments 'A' and 'B'\")\n    \n    # Check CUDA availability\n    if not torch.cuda.is_available():\n        raise RuntimeError(\"CUDA is not available. This kernel requires a GPU.\")\n    \n    # Store original devices\n    original_device_A = A.device\n    original_device_B = B.device\n    \n    # Move tensors to GPU if needed\n    if A.device.type == 'cpu':\n        A = A.cuda()\n    elif A.device.type != 'cuda':\n        raise ValueError(f\"Unsupported device type for tensor A: {A.device.type}\")\n    \n    if B.device.type == 'cpu':\n        B = B.cuda()\n    elif B.device.type != 'cuda':\n        raise ValueError(f\"Unsupported device type for tensor B: {B.device.type}\")\n    \n    # Ensure correct dtypes\n    if A.dtype != torch.float16:\n        A = A.to(torch.float16)\n    if B.dtype != torch.float16:\n        B = B.to(torch.float16)\n    \n    # Validate shapes\n    if len(A.shape) != 2 or len(B.shape) != 2:\n        raise ValueError(f\"Expected 2D tensors, got A.shape={A.shape}, B.shape={B.shape}\")\n    \n    if A.shape[1] != 4096:\n        raise ValueError(f\"Expected A to have K=4096, got {A.shape[1]}\")\n    \n    if B.shape[0] != 2048 or B.shape[1] != 4096:\n        raise ValueError(f\"Expected B to have shape [2048, 4096], got {B.shape}\")\n    \n    # Run the kernel\n    C = gemm_n2048_k4096(A, B)\n    \n    # Move result back to original device (prioritize A's device)\n    if original_device_A.type == 'cpu':\n        C = C.cpu()\n    \n    return C"
    }
  ],
  "description": "claude-opus-4-1-20250805 optimized kernel for gemm_n2048_k4096 (round 1)"
}