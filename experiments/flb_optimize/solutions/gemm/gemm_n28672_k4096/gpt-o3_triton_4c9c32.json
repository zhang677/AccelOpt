{
  "name": "gpt-o3_triton_4c9c32",
  "definition": "gemm_n28672_k4096",
  "author": "gpt-o3",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "B200"
    ],
    "entry_point": "main.py::run",
    "dependencies": []
  },
  "sources": [
    {
      "path": "main.py",
      "content": "import math\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef gemm_kernel(\n    A_ptr, B_ptr, C_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bn, stride_bk,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n):\n    pid_m = tl.program_id(0)      # program id for M dimension\n    pid_n = tl.program_id(1)      # program id for N dimension\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)              # [BLOCK_M]\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)              # [BLOCK_N]\n    offs_k = tl.arange(0, BLOCK_K)                                # [BLOCK_K]\n\n    a_ptrs = A_ptr + (offs_m[:, None] * stride_am +\n                      offs_k[None, :] * stride_ak)                # [BLOCK_M, BLOCK_K]\n    b_ptrs = B_ptr + (offs_n[None, :] * stride_bn +\n                      offs_k[:, None] * stride_bk)                # [BLOCK_K, BLOCK_N]\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    num_k_iters = tl.cdiv(K, BLOCK_K)\n    for _ in range(num_k_iters):\n        a = tl.load(\n            a_ptrs,\n            mask=(offs_m[:, None] < M) & (offs_k[None, :] < K),\n            other=0.0\n        )\n        b = tl.load(\n            b_ptrs,\n            mask=(offs_n[None, :] < N) & (offs_k[:, None] < K),\n            other=0.0\n        )\n        acc += tl.dot(a, b)\n\n        offs_k += BLOCK_K\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n\n    c_ptrs = C_ptr + (offs_m[:, None] * stride_cm +\n                      offs_n[None, :] * stride_cn)\n    acc = acc.to(tl.float16)\n    tl.store(\n        c_ptrs,\n        acc,\n        mask=(offs_m[:, None] < M) & (offs_n[None, :] < N)\n    )\n\n\ndef run(A: torch.Tensor, B: torch.Tensor):\n    \"\"\"\n    High-performance GEMM on B200 GPUs.\n    C = A @ B.T\n    Shapes:\n        A: [M, 4096]   (float16)\n        B: [28672, 4096] (float16)\n        C: [M, 28672]  (float16)\n    \"\"\"\n    if A.ndim != 2 or B.ndim != 2:\n        raise ValueError(\"A and B must be 2-D tensors\")\n    if A.shape[1] != 4096 or B.shape[1] != 4096 or B.shape[0] != 28672:\n        raise ValueError(\"Expected shapes: A [M, 4096], B [28672, 4096]\")\n    if A.dtype != torch.float16 or B.dtype != torch.float16:\n        raise TypeError(\"A and B must be float16\")\n\n    if not torch.cuda.is_available():\n        raise RuntimeError(\"CUDA is required to run this Triton kernel\")\n\n    orig_device_A = A.device\n    orig_device_B = B.device\n\n    A_cuda = A.cuda() if not A.is_cuda else A\n    B_cuda = B.cuda() if not B.is_cuda else B\n\n    M = A_cuda.shape[0]\n    N = 28672\n    K = 4096\n\n    C_cuda = torch.empty((M, N), device=A_cuda.device, dtype=torch.float16)\n\n    BLOCK_M = 128\n    BLOCK_N = 256\n    BLOCK_K = 32\n\n    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))\n\n    gemm_kernel[grid](\n        A_cuda, B_cuda, C_cuda,\n        M, N, K,\n        A_cuda.stride(0), A_cuda.stride(1),\n        B_cuda.stride(0), B_cuda.stride(1),\n        C_cuda.stride(0), C_cuda.stride(1),\n        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,\n        num_warps=8,\n        num_stages=4\n    )\n\n    torch.cuda.synchronize()\n\n    if orig_device_A.type == \"cuda\":\n        return C_cuda\n    return C_cuda.cpu()\n\n\n# Allow module import without immediate execution\nif __name__ == \"__main__\":\n    # Simple correctness test\n    M_test = 256\n    A_test = torch.randn((M_test, 4096), dtype=torch.float16)\n    B_test = torch.randn((28672, 4096), dtype=torch.float16)\n    C_ref = (A_test.float() @ B_test.t().float()).half()\n    C_out = run(A_test, B_test)\n    assert torch.allclose(C_ref, C_out, atol=1e-2, rtol=1e-2)\n    print(\"Test passed!\")"
    }
  ],
  "description": "o3 optimized kernel for gemm_n28672_k4096 (round 1)"
}