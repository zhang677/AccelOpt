{
  "name": "gpt-5_triton_793693",
  "definition": "gemm_n6144_k4096",
  "author": "gpt-5-2025-08-07",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "B200"
    ],
    "entry_point": "main.py::run",
    "dependencies": []
  },
  "sources": [
    {
      "path": "main.py",
      "content": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 64},  num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 128}, num_stages=5, num_warps=8),\n        triton.Config({'BLOCK_M': 64,  'BLOCK_N': 256, 'BLOCK_K': 64},  num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 64},  num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_M': 64,  'BLOCK_N': 128, 'BLOCK_K': 64},  num_stages=4, num_warps=4),\n    ],\n    key=['M'],\n)\n@triton.jit\ndef _gemm_n_6144_k_4096_kernel(\n    A_ptr, B_ptr, C_ptr,\n    M, N, K,\n    stride_am, stride_ak,    # A: [M, K]\n    stride_bn, stride_bk,    # B: [N, K]\n    stride_cm, stride_cn,    # C: [M, N]\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    k0 = 0\n    while k0 < K:\n        a_ptrs = A_ptr + (offs_m[:, None] * stride_am + (k0 + offs_k)[None, :] * stride_ak)\n        b_ptrs = B_ptr + (offs_n[None, :] * stride_bn + (k0 + offs_k)[:, None] * stride_bk)\n        a = tl.load(a_ptrs, mask=(offs_m[:, None] < M) & ((k0 + offs_k)[None, :] < K), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_n[None, :] < N) & ((k0 + offs_k)[:, None] < K), other=0.0)\n        acc += tl.dot(a, b)\n        k0 += BLOCK_K\n\n    c = acc.to(tl.float16)\n    c_ptrs = C_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    tl.store(c_ptrs, c, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))\n\n\ndef run(*args, **kwargs):\n    if len(args) == 2 and not kwargs:\n        A, B = args\n    else:\n        A = kwargs.get('A', args[0] if len(args) > 0 else None)\n        B = kwargs.get('B', args[1] if len(args) > 1 else None)\n        if A is None or B is None:\n            raise ValueError(\"run expects tensors A and B as positional or keyword arguments\")\n\n    if not isinstance(A, torch.Tensor) or not isinstance(B, torch.Tensor):\n        raise TypeError(\"A and B must be torch.Tensor\")\n\n    if A.dtype != torch.float16 or B.dtype != torch.float16:\n        raise TypeError(\"A and B must be float16 tensors\")\n\n    if A.ndim != 2 or B.ndim != 2:\n        raise ValueError(\"A and B must be 2D tensors\")\n\n    M, KA = A.shape\n    NB, KB = B.shape\n    if KA != KB:\n        raise ValueError(f\"Incompatible inner dimensions: A is (*, {KA}), B is (*, {KB})\")\n    if NB != 6144:\n        raise ValueError(f\"B must have N=6144 as the first dimension, got {NB}\")\n    if KB != 4096:\n        raise ValueError(f\"B must have K=4096 as the second dimension, got {KB}\")\n\n    cuda_available = torch.cuda.is_available()\n    A_is_cuda = A.is_cuda\n    B_is_cuda = B.is_cuda\n\n    if (A_is_cuda or B_is_cuda) and not cuda_available:\n        raise RuntimeError(\"CUDA tensors provided but CUDA is not available\")\n\n    # Choose device: prefer GPU if available or if any input is on GPU\n    if A_is_cuda:\n        device = A.device\n    elif B_is_cuda:\n        device = B.device\n    else:\n        device = torch.device('cuda') if cuda_available else torch.device('cpu')\n\n    if device.type == 'cpu' and not cuda_available:\n        return torch.matmul(A, B.T)\n\n    # Move to the chosen CUDA device if needed\n    if device.type == 'cuda':\n        dev_index = device.index if device.index is not None else 0\n        A_dev = A.cuda(dev_index, non_blocking=True).contiguous()\n        B_dev = B.cuda(dev_index, non_blocking=True).contiguous()\n    else:\n        A_dev = A.contiguous()\n        B_dev = B.contiguous()\n\n    M = A_dev.shape[0]\n    K = A_dev.shape[1]\n    N = B_dev.shape[0]\n\n    C_dev = torch.empty((M, N), dtype=torch.float16, device=A_dev.device)\n\n    stride_am, stride_ak = A_dev.stride()\n    stride_bn, stride_bk = B_dev.stride()\n    stride_cm, stride_cn = C_dev.stride()\n\n    def grid(meta):\n        return (triton.cdiv(M, meta['BLOCK_M']), triton.cdiv(N, meta['BLOCK_N']))\n\n    _gemm_n_6144_k_4096_kernel[grid](\n        A_dev, B_dev, C_dev,\n        M, N, K,\n        stride_am, stride_ak,\n        stride_bn, stride_bk,\n        stride_cm, stride_cn,\n    )\n\n    # Move result back to A's original device\n    if A.device == C_dev.device:\n        return C_dev\n    else:\n        return C_dev.to(A.device, non_blocking=True)"
    }
  ],
  "description": "gpt-5-2025-08-07 high reasoning effort optimized kernel for gemm_n6144_k4096 (round 2)"
}