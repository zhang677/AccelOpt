{
  "name": "gpt-5_triton_14f133",
  "definition": "gemm_n4096_k4096",
  "author": "gpt-5-2025-08-07",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "B200"
    ],
    "entry_point": "main.py::run",
    "dependencies": []
  },
  "sources": [
    {
      "path": "main.py",
      "content": "import math\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 64}, num_warps=8, num_stages=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 64}, num_warps=8, num_stages=4),\n        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=8, num_stages=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 128}, num_warps=8, num_stages=4),\n    ],\n    key=['M'],\n)\n@triton.jit\ndef _gemm_n4096_k4096_kernel(\n    A_ptr, B_ptr, C_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bn, stride_bk,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n):\n    tl.static_assert(BLOCK_K % 16 == 0, \"BLOCK_K must be a multiple of 16 for tensor cores\")\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    M_mask = offs_m[:, None] < M\n    N_mask = offs_n[None, :] < N\n\n    for k0 in range(0, K, BLOCK_K):\n        offs_k = k0 + tl.arange(0, BLOCK_K)\n\n        # Pointers\n        a_ptrs = A_ptr + (offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak)\n        b_ptrs = B_ptr + (offs_n[:, None] * stride_bn + offs_k[None, :] * stride_bk)\n\n        a = tl.load(a_ptrs, mask=M_mask & (offs_k[None, :] < K), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_n[:, None] < N) & (offs_k[None, :] < K), other=0.0)\n\n        acc += tl.dot(a, tl.trans(b))\n\n    c_ptrs = C_ptr + (offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn)\n    tl.store(c_ptrs, acc.to(tl.float16), mask=M_mask & N_mask)\n\n\ndef run(A, B, **kwargs):\n    \"\"\"\n    Compute C = A @ B.T where:\n    - A: [M, 4096] float16\n    - B: [4096, 4096] float16\n    Returns C: [M, 4096] float16\n    \"\"\"\n    if not isinstance(A, torch.Tensor) or not isinstance(B, torch.Tensor):\n        raise TypeError(\"A and B must be torch.Tensor\")\n\n    if A.ndim != 2 or B.ndim != 2:\n        raise ValueError(f\"Expected 2D tensors, got A.ndim={A.ndim}, B.ndim={B.ndim}\")\n\n    M, K_a = A.shape\n    N_b, K_b = B.shape\n\n    if K_a != 4096 or K_b != 4096 or N_b != 4096:\n        raise ValueError(f\"Invalid shapes: A is {A.shape}, B is {B.shape}; expected A: [M,4096], B: [4096,4096]\")\n\n    # Dtype checks/conversions\n    if A.dtype != torch.float16:\n        A = A.to(torch.float16)\n    if B.dtype != torch.float16:\n        B = B.to(torch.float16)\n\n    # Device management\n    orig_dev_A = A.device\n    orig_dev_B = B.device\n    any_cuda_input = (A.is_cuda or B.is_cuda)\n\n    if not torch.cuda.is_available():\n        if any_cuda_input:\n            raise RuntimeError(\"CUDA is not available but one or more inputs are CUDA tensors.\")\n        # Triton requires CUDA; no CPU fallback provided\n        raise RuntimeError(\"CUDA is required to run this Triton kernel, but no CUDA device is available.\")\n\n    # Choose compute device\n    if A.is_cuda:\n        compute_device = A.device\n    elif B.is_cuda:\n        compute_device = B.device\n    else:\n        compute_device = torch.device(f\"cuda:{torch.cuda.current_device()}\")\n\n    # Move to compute device and make contiguous for optimal access\n    A_dev = A.to(device=compute_device, non_blocking=True).contiguous()\n    B_dev = B.to(device=compute_device, non_blocking=True).contiguous()\n\n    # Allocate output on compute device\n    N = 4096\n    K = 4096\n    C_dev = torch.empty((M, N), dtype=torch.float16, device=compute_device)\n\n    # Kernel launch parameters\n    def grid(meta):\n        return (triton.cdiv(M, meta['BLOCK_M']), triton.cdiv(N, meta['BLOCK_N']))\n\n    # Call kernel\n    _gemm_n4096_k4096_kernel[grid](\n        A_dev, B_dev, C_dev,\n        M, N, K,\n        A_dev.stride(0), A_dev.stride(1),\n        B_dev.stride(0), B_dev.stride(1),\n        C_dev.stride(0), C_dev.stride(1),\n    )\n\n    # Decide output device: preserve original locations; if both were CPU, return CPU; otherwise prefer A's device if CUDA, else B's\n    if orig_dev_A.type == 'cpu' and orig_dev_B.type == 'cpu':\n        out_device = torch.device('cpu')\n    elif orig_dev_A.type == 'cuda':\n        out_device = orig_dev_A\n    elif orig_dev_B.type == 'cuda':\n        out_device = orig_dev_B\n    else:\n        out_device = torch.device('cpu')\n\n    C_out = C_dev if C_dev.device == out_device else C_dev.to(out_device, non_blocking=True)\n    return C_out"
    }
  ],
  "description": "gpt-5-2025-08-07 high reasoning effort optimized kernel for gemm_n4096_k4096 (round 1)"
}