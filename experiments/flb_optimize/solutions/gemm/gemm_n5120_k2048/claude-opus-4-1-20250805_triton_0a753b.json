{
  "name": "claude-opus-4-1-20250805_triton_0a753b",
  "definition": "gemm_n5120_k2048",
  "author": "claude-opus-4-1-20250805",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "B200"
    ],
    "entry_point": "main.py::run",
    "dependencies": []
  },
  "sources": [
    {
      "path": "main.py",
      "content": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef gemm_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bn, stride_bk,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    \"\"\"GEMM kernel optimized for B200 GPU with N=5120, K=2048.\"\"\"\n    # Get program IDs\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    # Compute block offsets\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    # Create masks for boundary conditions\n    mask_m = offs_m < M\n    mask_n = offs_n < N\n    \n    # Initialize pointers to A and B blocks\n    a_ptrs = a_ptr + (offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_n[:, None] * stride_bn + offs_k[None, :] * stride_bk)\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    \n    # Main K-loop\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load A and B blocks with boundary checking\n        mask_k = offs_k < K - k\n        a_mask = mask_m[:, None] & mask_k[None, :]\n        b_mask = mask_n[:, None] & mask_k[None, :]\n        \n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        \n        # Compute matrix multiplication for this block\n        acc += tl.dot(a, tl.trans(b))\n        \n        # Advance pointers\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    \n    # Convert accumulator to fp16 and store result\n    c = acc.to(tl.float16)\n    \n    # Compute output pointer and mask\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm < M)[:, None] & (offs_cn < N)[None, :]\n    \n    tl.store(c_ptrs, c, mask=c_mask)\n\n\ndef run(*args, **kwargs):\n    \"\"\"Entry point function with complete device management.\"\"\"\n    # Handle both args and kwargs\n    if len(args) >= 2:\n        A, B = args[0], args[1]\n    else:\n        A = kwargs.get('A')\n        B = kwargs.get('B')\n    \n    if A is None or B is None:\n        raise ValueError(\"Missing required arguments A and B\")\n    \n    # Store original device information\n    orig_device_A = A.device\n    orig_device_B = B.device\n    \n    # Check if CUDA is available for GPU operations\n    if not torch.cuda.is_available():\n        if A.is_cuda or B.is_cuda:\n            raise RuntimeError(\"CUDA is not available but GPU tensors were provided\")\n    \n    # Move tensors to GPU if needed\n    if torch.cuda.is_available():\n        if not A.is_cuda:\n            A = A.cuda()\n        if not B.is_cuda:\n            B = B.cuda()\n    \n    # Validate input shapes and dtypes\n    assert A.dtype == torch.float16, f\"Expected A to be float16, got {A.dtype}\"\n    assert B.dtype == torch.float16, f\"Expected B to be float16, got {B.dtype}\"\n    assert A.shape[1] == 2048, f\"Expected A.shape[1] == 2048, got {A.shape[1]}\"\n    assert B.shape[0] == 5120, f\"Expected B.shape[0] == 5120, got {B.shape[0]}\"\n    assert B.shape[1] == 2048, f\"Expected B.shape[1] == 2048, got {B.shape[1]}\"\n    \n    M, K = A.shape\n    N = B.shape[0]\n    \n    # Allocate output tensor\n    C = torch.empty((M, N), dtype=torch.float16, device=A.device)\n    \n    # Define block sizes optimized for B200 GPU\n    # B200 has large shared memory and high throughput\n    BLOCK_SIZE_M = 128\n    BLOCK_SIZE_N = 128\n    BLOCK_SIZE_K = 64\n    \n    # Compute grid dimensions\n    grid = lambda META: (\n        triton.cdiv(M, META['BLOCK_SIZE_M']),\n        triton.cdiv(N, META['BLOCK_SIZE_N']),\n    )\n    \n    # Launch kernel\n    gemm_kernel[grid](\n        A, B, C,\n        M, N, K,\n        A.stride(0), A.stride(1),\n        B.stride(0), B.stride(1),\n        C.stride(0), C.stride(1),\n        BLOCK_SIZE_M=BLOCK_SIZE_M,\n        BLOCK_SIZE_N=BLOCK_SIZE_N,\n        BLOCK_SIZE_K=BLOCK_SIZE_K,\n    )\n    \n    # Move result back to original device if needed\n    if orig_device_A.type == 'cpu':\n        C = C.cpu()\n    \n    return C"
    }
  ],
  "description": "claude-opus-4-1-20250805 optimized kernel for gemm_n5120_k2048 (round 1)"
}