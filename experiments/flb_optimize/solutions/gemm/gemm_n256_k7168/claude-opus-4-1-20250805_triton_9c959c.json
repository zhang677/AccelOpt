{
  "name": "claude-opus-4-1-20250805_triton_9c959c",
  "definition": "gemm_n256_k7168",
  "author": "claude-opus-4-1-20250805",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "B200"
    ],
    "entry_point": "main.py::run",
    "dependencies": []
  },
  "sources": [
    {
      "path": "main.py",
      "content": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef gemm_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bn, stride_bk,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n):\n    # Program ID and grid dimensions\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    # Compute block boundaries\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    \n    # Compute pointers to first blocks of A and B\n    a_base = a_ptr + rm[:, None] * stride_am\n    b_base = b_ptr + rn[:, None] * stride_bn\n    \n    # Main loop over K dimension\n    for k in range(0, K, BLOCK_K):\n        rk = k + tl.arange(0, BLOCK_K)\n        \n        # Load A block with masking\n        a_mask = (rm[:, None] < M) & (rk[None, :] < K)\n        a = tl.load(a_base + rk[None, :] * stride_ak, mask=a_mask, other=0.0)\n        \n        # Load B block with masking\n        b_mask = (rn[:, None] < N) & (rk[None, :] < K)\n        b = tl.load(b_base + rk[None, :] * stride_bk, mask=b_mask, other=0.0)\n        \n        # Accumulate dot product\n        acc += tl.dot(a, tl.trans(b), allow_tf32=True)\n    \n    # Write result with masking\n    c_mask = (rm[:, None] < M) & (rn[None, :] < N)\n    c = c_ptr + rm[:, None] * stride_cm + rn[None, :] * stride_cn\n    tl.store(c, acc.to(tl.float16), mask=c_mask)\n\ndef run(*args, **kwargs):\n    \"\"\"Entry point function for GEMM operation.\"\"\"\n    # Handle both positional and keyword arguments\n    if len(args) == 2:\n        A, B = args\n    elif len(args) == 0 and 'A' in kwargs and 'B' in kwargs:\n        A = kwargs['A']\n        B = kwargs['B']\n    else:\n        raise ValueError(\"Expected exactly 2 arguments (A, B)\")\n    \n    # Store original device\n    original_device_A = A.device\n    original_device_B = B.device\n    \n    # Move to GPU if needed\n    if A.device.type == 'cpu':\n        if not torch.cuda.is_available():\n            raise RuntimeError(\"CUDA is not available but CPU tensors were provided\")\n        A = A.cuda()\n    elif A.device.type != 'cuda':\n        raise ValueError(f\"Unsupported device type: {A.device.type}\")\n    \n    if B.device.type == 'cpu':\n        if not torch.cuda.is_available():\n            raise RuntimeError(\"CUDA is not available but CPU tensors were provided\")\n        B = B.cuda()\n    elif B.device.type != 'cuda':\n        raise ValueError(f\"Unsupported device type: {B.device.type}\")\n    \n    # Validate input shapes and dtypes\n    assert A.dtype == torch.float16, f\"Expected A to be float16, got {A.dtype}\"\n    assert B.dtype == torch.float16, f\"Expected B to be float16, got {B.dtype}\"\n    assert A.dim() == 2, f\"Expected A to be 2D, got {A.dim()}D\"\n    assert B.dim() == 2, f\"Expected B to be 2D, got {B.dim()}D\"\n    \n    M, K_A = A.shape\n    N, K_B = B.shape\n    \n    assert K_A == 7168, f\"Expected K dimension of A to be 7168, got {K_A}\"\n    assert K_B == 7168, f\"Expected K dimension of B to be 7168, got {K_B}\"\n    assert N == 256, f\"Expected N dimension of B to be 256, got {N}\"\n    \n    # Allocate output tensor on GPU\n    C = torch.empty((M, N), dtype=torch.float16, device=A.device)\n    \n    # Configure block sizes optimized for B200\n    # B200 has high memory bandwidth and compute capability\n    BLOCK_M = 128\n    BLOCK_N = 128\n    BLOCK_K = 64\n    \n    # Calculate grid dimensions\n    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))\n    \n    # Launch kernel\n    gemm_kernel[grid](\n        a_ptr=A,\n        b_ptr=B,\n        c_ptr=C,\n        M=M,\n        N=N,\n        K=7168,\n        stride_am=A.stride(0),\n        stride_ak=A.stride(1),\n        stride_bn=B.stride(0),\n        stride_bk=B.stride(1),\n        stride_cm=C.stride(0),\n        stride_cn=C.stride(1),\n        BLOCK_M=BLOCK_M,\n        BLOCK_N=BLOCK_N,\n        BLOCK_K=BLOCK_K,\n    )\n    \n    # Move result back to original device if needed\n    if original_device_A.type == 'cpu':\n        C = C.cpu()\n    \n    return C"
    }
  ],
  "description": "claude-opus-4-1-20250805 optimized kernel for gemm_n256_k7168 (round 1)"
}