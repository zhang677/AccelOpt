{
  "name": "gemini-2.5-pro_triton_kmw3sz",
  "definition": "gemm_n128_k2048",
  "author": "gemini-2.5-pro",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "B200"
    ],
    "entry_point": "main.py::run",
    "dependencies": []
  },
  "sources": [
    {
      "path": "main.py",
      "content": "import torch\nimport triton\nimport triton.language as tl\nimport math\n\n# This Triton kernel is a fused GEMM operation C = A @ B.T\n# A is of shape (M, K), B is of shape (N, K) -> C is of shape (M, N)\n# It is optimized for the case where N=128 and K=2048.\n# The M dimension is variable.\n# Optimizations for B200 (Hopper-like architecture):\n# - Use tl.dot() which maps to Tensor Core operations for fp16 inputs.\n# - Accumulator is in fp32 for precision.\n# - Autotuner explores different block sizes and pipeline stages.\n# - BLOCK_SIZE_N is fixed to 128 to compute a full output row-block per thread block.\n# - A 1D grid is used over the M-dimension, which is simple and effective.\n# - Software pipelining is enabled via num_stages to hide memory latency.\n\n@triton.autotune(\n    configs=[\n        # Basic configurations with varying block sizes and stages\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_K': 64, 'num_stages': 4, 'num_warps': 4}),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_K': 32, 'num_stages': 5, 'num_warps': 4}),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_K': 64, 'num_stages': 3, 'num_warps': 4}),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_K': 128, 'num_stages': 2, 'num_warps': 4}),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_K': 32, 'num_stages': 4, 'num_warps': 4}),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_K': 64, 'num_stages': 3, 'num_warps': 8}),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_K': 128, 'num_stages': 2, 'num_warps': 8}),\n        # Configurations with larger M-block for potentially large M inputs\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_K': 32, 'num_stages': 3, 'num_warps': 8}),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_K': 64, 'num_stages': 2, 'num_warps': 8}),\n    ],\n    key=['M'],\n)\n@triton.jit\ndef gemm_kernel(\n    # Pointers to matrices\n    A_ptr, B_ptr, C_ptr,\n    # Matrix dimensions\n    M, N, K,\n    # Strides for matrices\n    stride_am, stride_ak,\n    stride_bn, stride_bk,\n    stride_cm, stride_cn,\n    # Meta-parameters\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    \"\"\"\n    Triton kernel for GEMM: C = A @ B.T\n    A: [M, K]\n    B: [N, K]\n    C: [M, N]\n    \"\"\"\n    # -----------------------------------------------------------\n    # Map program ids to M-dimension\n    # This program will compute a BLOCK_SIZE_M x N tile of C\n    pid = tl.program_id(axis=0)\n\n    # -----------------------------------------------------------\n    # Create offsets for the M, N, and K dimensions.\n    # We will compute a BLOCK_SIZE_M x BLOCK_SIZE_N block of C.\n    offs_m = (pid * BLOCK_SIZE_M) + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # -----------------------------------------------------------\n    # Initialise pointers to the first element of the A and B tiles.\n    # A is [M, K], B is [N, K].\n    # Pointer for A tile: [BLOCK_SIZE_M, BLOCK_SIZE_K]\n    A_ptrs = A_ptr + (offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    \n    # Pointer for B tile: [BLOCK_SIZE_K, BLOCK_SIZE_N]\n    # To compute A @ B.T, we need to effectively transpose the tile of B\n    # during the load. We do this by swapping the roles of N and K offsets\n    # in the pointer calculation.\n    B_ptrs = B_ptr + (offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn)\n\n    # -----------------------------------------------------------\n    # Accumulator for the C tile, initialized to zeros.\n    # Using float32 for higher precision during accumulation.\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # -----------------------------------------------------------\n    # Loop over the K dimension by increments of BLOCK_SIZE_K.\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        # Load the tiles of A and B from global memory.\n        # Masking is applied to handle cases where M or K are not perfect multiples of block sizes.\n        mask_a = (offs_m[:, None] < M) & ((k * BLOCK_SIZE_K + offs_k[None, :]) < K)\n        a = tl.load(A_ptrs, mask=mask_a, other=0.0)\n        \n        # Load a tile of B. Because of the pointer setup, this tile is\n        # effectively transposed, with shape [BLOCK_SIZE_K, BLOCK_SIZE_N].\n        # Masking is only needed for the K dimension.\n        mask_b = (k * BLOCK_SIZE_K + offs_k[:, None]) < K\n        b = tl.load(B_ptrs, mask=mask_b, other=0.0)\n        \n        # Perform matrix multiplication.\n        # a: [BLOCK_SIZE_M, BLOCK_SIZE_K]\n        # b: [BLOCK_SIZE_K, BLOCK_SIZE_N]\n        # The result is [BLOCK_SIZE_M, BLOCK_SIZE_N], which is correct.\n        accumulator += tl.dot(a, b)\n        \n        # Advance pointers to the next K-block.\n        A_ptrs += BLOCK_SIZE_K * stride_ak\n        B_ptrs += BLOCK_SIZE_K * stride_bk\n\n    # -----------------------------------------------------------\n    # Cast accumulator to the output dtype (float16) and store the result.\n    c = accumulator.to(tl.float16)\n    \n    # Create pointers for the C matrix and store the result.\n    offs_cm = (pid * BLOCK_SIZE_M) + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = tl.arange(0, BLOCK_SIZE_N)\n    C_ptrs = C_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    \n    # Mask for writing to C to handle M not being a multiple of BLOCK_SIZE_M.\n    mask_c = (offs_cm[:, None] < M)\n    tl.store(C_ptrs, c, mask=mask_c)\n\n\ndef gemm_n128_k2048(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Wrapper function for the GEMM kernel.\n\n    Args:\n        A (torch.Tensor): A tensor of shape [M, 2048] and dtype float16.\n        B (torch.Tensor): A tensor of shape [128, 2048] and dtype float16.\n\n    Returns:\n        torch.Tensor: The result of A @ B.T, with shape [M, 128] and dtype float16.\n    \"\"\"\n    # --- Input validation ---\n    if not A.is_cuda or not B.is_cuda:\n        raise ValueError(\"Input tensors must be on a CUDA device.\")\n    if A.dtype != torch.float16 or B.dtype != torch.float16:\n        raise ValueError(\"Input tensors must have dtype torch.float16.\")\n    \n    M, K_A = A.shape\n    N, K_B = B.shape\n    \n    if K_A != 2048 or K_B != 2048:\n        raise ValueError(f\"K dimension must be 2048, but got {K_A} for A and {K_B} for B.\")\n    if N != 128:\n        raise ValueError(f\"N dimension for B must be 128, but got {N}.\")\n    \n    # --- Output tensor allocation ---\n    # The output C will have shape [M, N]\n    C = torch.empty((M, N), device=A.device, dtype=torch.float16)\n\n    # --- Grid setup ---\n    # The grid is 1D, with each program instance computing a block of M rows.\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']),)\n    \n    # --- Kernel launch ---\n    gemm_kernel[grid](\n        A, B, C,\n        M, N, 2048,\n        A.stride(0), A.stride(1),\n        B.stride(0), B.stride(1),\n        C.stride(0), C.stride(1),\n        BLOCK_SIZE_N=128,  # N is fixed, so we set BLOCK_SIZE_N to the full dimension.\n    )\n    \n    return C\n\n\ndef run(*args, **kwargs):\n    \"\"\"\n    Public entry point for the GEMM operation.\n    This function handles device management, argument parsing, and kernel execution.\n    It moves data to the GPU if necessary, runs the computation, and moves the\n    result back to the original device.\n\n    Usage:\n        run(A, B)\n        run(A=tensor_a, B=tensor_b)\n    \"\"\"\n    # --- Argument parsing ---\n    if 'A' in kwargs and 'B' in kwargs:\n        A = kwargs['A']\n        B = kwargs['B']\n    elif len(args) == 2:\n        A, B = args\n    else:\n        raise ValueError(\"run() expects two positional arguments (A, B) or two keyword arguments (A=..., B=...).\")\n\n    # --- Device Management ---\n    if not torch.cuda.is_available():\n        raise RuntimeError(\"Triton requires a CUDA-enabled GPU, but CUDA is not available.\")\n\n    original_devices = {\n        'A': A.device,\n        'B': B.device,\n    }\n    \n    # Determine the target CUDA device for computation.\n    # If any input is on CUDA, use that device. Otherwise, use the default CUDA device.\n    target_device = torch.device(\"cuda\")\n    for tensor in [A, B]:\n        if tensor.is_cuda:\n            target_device = tensor.device\n            break\n            \n    # Move all tensors to the target CUDA device.\n    try:\n        A_gpu = A.to(target_device)\n        B_gpu = B.to(target_device)\n    except Exception as e:\n        raise RuntimeError(f\"Failed to move tensors to device {target_device}. Error: {e}\")\n\n    # --- Execute the kernel ---\n    C_gpu = gemm_n128_k2048(A_gpu, B_gpu)\n\n    # --- Restore device of output ---\n    # The output tensor is moved back to the device of the first input tensor 'A'.\n    output_device = original_devices['A']\n    if C_gpu.device != output_device:\n        C = C_gpu.to(output_device)\n    else:\n        C = C_gpu\n        \n    return C"
    }
  ],
  "description": "gemini-2.5-pro optimized kernel for gemm_n128_k2048 (round 2)"
}