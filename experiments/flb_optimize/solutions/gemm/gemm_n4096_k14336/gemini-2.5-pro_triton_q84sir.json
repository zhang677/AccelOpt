{
  "name": "gemini-2.5-pro_triton_q84sir",
  "definition": "gemm_n4096_k14336",
  "author": "gemini-2.5-pro",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "B200"
    ],
    "entry_point": "main.py::run",
    "dependencies": []
  },
  "sources": [
    {
      "path": "main.py",
      "content": "import torch\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef gemm_kernel(\n    A, B, C,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bn, stride_bk,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n):\n    \"\"\"\n    Triton Kernel for GEMM C = A @ B.T optimized for B200.\n    A is (M, K), B is (N, K), C is (M, N).\n    This kernel computes C[m, n] = sum_k(A[m, k] * B[n, k]).\n\n    Tuning and Strategy:\n    - Tiling: The problem is tiled into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N) for the output C.\n      The K dimension is processed in blocks of BLOCK_SIZE_K.\n    - Grouping: Thread blocks are grouped (GROUP_SIZE_M) along the M dimension to improve L2 cache reuse\n      for the B matrix. Blocks within a group are executed by a single SM or cluster, keeping the\n      relevant parts of B in L2 cache. This is achieved by remapping a 1D grid launch.\n    - Pipelining: Software pipelining (num_stages in the launcher) is used to hide the latency of loading\n      data from global memory. While one block of A and B is being processed, the next is being fetched.\n    - Data Types: FP32 is used for accumulation to maintain precision and avoid overflow, before\n      converting the final result back to FP16.\n    - B200 Optimizations: This kernel uses large block sizes (128x128) and a high thread count per block\n      (8 warps = 256 threads) to fully utilize the compute resources of modern architectures like B200.\n      The `tl.dot` instruction maps directly to Tensor Core operations.\n    \"\"\"\n    # -----------------------------------------------------------\n    # Map program ids to tiles\n    # This kernel uses a 1D launch grid and remaps it to a 2D grid with grouping.\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n\n    # Remap to a 2D grid with grouping for L2 cache locality\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n\n    # Within the group, blocks are arranged column-major\n    pid_m = first_pid_m + (pid % group_size)\n    pid_n = (pid % num_pid_in_group) // group_size\n\n    # ----------------------------------------------------------\n    # Create pointers for the first blocks of A and B.\n    # rm/rn will be block-level offsets, offs_m/n will be thread-level offsets\n    offs_m = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M))\n    offs_n = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N))\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Pointers to the start of the first tile for this program instance\n    a_ptrs = A + (offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = B + (offs_n[:, None] * stride_bn + offs_k[None, :] * stride_bk)\n\n    # -----------------------------------------------------------\n    # Accumulator initialization\n    # Use float32 for higher precision and to avoid overflow\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # -----------------------------------------------------------\n    # Main loop over the K dimension\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        # Load the next block of A and B\n        # Masking is applied to handle cases where K is not a multiple of BLOCK_SIZE_K\n        # and for the variable M dimension.\n        a_mask = (offs_m[:, None] < M) & (offs_k[None, :] + k * BLOCK_SIZE_K < K)\n        b_mask = (offs_n[:, None] < N) & (offs_k[None, :] + k * BLOCK_SIZE_K < K)\n\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n\n        # The operation is A @ B.T, which means we need to compute dot(A, B.T)\n        # We load a tile from B of shape [BLOCK_N, BLOCK_K]. Triton's tl.dot requires\n        # the second operand to be of shape [K, N]. The load pattern for `b` is already\n        # effectively transposing it for memory access efficiency.\n        # So we use tl.trans to get B into the right shape for tl.dot.\n        accumulator += tl.dot(a, tl.trans(b))\n\n        # Advance pointers to the next K-block\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    # Cast accumulator to the output dtype\n    c = accumulator.to(tl.float16)\n\n    # -----------------------------------------------------------\n    # Write back the block of the output matrix C\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = C + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n\ndef run(*args, **kwargs):\n    \"\"\"\n    Public entry point for the gemm_n4096_k14336 operation.\n\n    This function serves as a complete wrapper, handling device management,\n    tensor validation, kernel launching, and result handling.\n\n    Args:\n        A (torch.Tensor): The first input tensor of shape [M, 14336] and dtype float16.\n        B (torch.Tensor): The second input tensor of shape [4096, 14336] and dtype float16.\n\n    Returns:\n        torch.Tensor: The output tensor C of shape [M, 4096] and dtype float16,\n                      on the same device as the input tensors.\n    \"\"\"\n    # -----------------------------------------------------------\n    # Argument parsing\n    # -----------------------------------------------------------\n    if 'A' in kwargs and 'B' in kwargs:\n        A = kwargs['A']\n        B = kwargs['B']\n    elif len(args) == 2:\n        A, B = args\n    else:\n        raise ValueError(\"Expected 2 positional arguments (A, B) or keyword arguments 'A' and 'B'.\")\n\n    # -----------------------------------------------------------\n    # Constants and validation\n    # -----------------------------------------------------------\n    N, K = 4096, 14336\n    assert A.shape[1] == K, f\"Input A has invalid K dimension: {A.shape[1]}, expected {K}\"\n    assert B.shape[0] == N, f\"Input B has invalid N dimension: {B.shape[0]}, expected {N}\"\n    assert B.shape[1] == K, f\"Input B has invalid K dimension: {B.shape[1]}, expected {K}\"\n    assert A.dtype == torch.float16, f\"Input A must be float16, but is {A.dtype}\"\n    assert B.dtype == torch.float16, f\"Input B must be float16, but is {B.dtype}\"\n    M = A.shape[0]\n\n    # -----------------------------------------------------------\n    # Device management\n    # -----------------------------------------------------------\n    if not torch.cuda.is_available():\n        raise RuntimeError(\"This Triton kernel requires a CUDA-enabled GPU.\")\n\n    original_device = A.device\n    is_cpu = original_device.type == 'cpu'\n\n    if is_cpu:\n        A_gpu = A.cuda()\n        B_gpu = B.cuda()\n    else:\n        A_gpu = A\n        B_gpu = B\n\n    # -----------------------------------------------------------\n    # Kernel launch\n    # -----------------------------------------------------------\n    # Allocate output tensor\n    C = torch.empty((M, N), device=A_gpu.device, dtype=torch.float16)\n\n    # Grid computation\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n\n    # Kernel call\n    # Using a single, well-tuned configuration for B200.\n    # In a real-world scenario, this would be autotuned.\n    gemm_kernel[grid](\n        A_gpu, B_gpu, C,\n        M, N, K,\n        A_gpu.stride(0), A_gpu.stride(1),\n        B_gpu.stride(0), B_gpu.stride(1),\n        C.stride(0), C.stride(1),\n        # --- Kernel meta-parameters ---\n        BLOCK_SIZE_M=128,\n        BLOCK_SIZE_N=128,\n        BLOCK_SIZE_K=64,\n        GROUP_SIZE_M=8,\n        # num_stages and num_warps are passed to the Triton compiler\n        # For B200, 8 warps and 3+ stages are good starting points\n        num_warps=8,\n        num_stages=3\n    )\n\n    # -----------------------------------------------------------\n    # Final device management\n    # -----------------------------------------------------------\n    if is_cpu:\n        return C.to(original_device)\n    else:\n        return C\n"
    }
  ],
  "description": "gemini-2.5-pro optimized kernel for gemm_n4096_k14336 (round 1)"
}