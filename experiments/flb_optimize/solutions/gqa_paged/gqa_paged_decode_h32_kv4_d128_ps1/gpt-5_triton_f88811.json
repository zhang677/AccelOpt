{
  "name": "gpt-5_triton_f88811",
  "definition": "gqa_paged_decode_h32_kv4_d128_ps1",
  "author": "gpt-5-2025-08-07",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "B200"
    ],
    "entry_point": "main.py::run",
    "dependencies": []
  },
  "sources": [
    {
      "path": "main.py",
      "content": "import math\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef gqa_paged_decode_h32_kv4_d128_ps1_kernel(\n    q_ptr,                         # *bf16 [B, Hq, D]\n    k_cache_ptr,                   # *bf16 [P, S=1, Hk=4, D]\n    v_cache_ptr,                   # *bf16 [P, S=1, Hk=4, D]\n    kv_indptr_ptr,                 # *i32  [B+1]\n    kv_indices_ptr,                # *i32  [N]\n    sm_scale,                      # f32 scalar\n    output_ptr,                    # *bf16 [B, Hq, D]\n    lse_ptr,                       # *f32  [B, Hq]\n    batch_size,                    # i32\n    num_qo_heads,                  # i32 (should be 32)\n    # strides for q\n    stride_q_b, stride_q_h, stride_q_d,\n    # strides for k_cache\n    stride_k_p, stride_k_s, stride_k_h, stride_k_d,\n    # strides for v_cache\n    stride_v_p, stride_v_s, stride_v_h, stride_v_d,\n    # strides for output\n    stride_o_b, stride_o_h, stride_o_d,\n    # strides for lse\n    stride_lse_b, stride_lse_h,\n    BLOCK_M: tl.constexpr,         # token block size\n    D_HEAD: tl.constexpr,          # head dim = 128\n    GQA_RATIO: tl.constexpr        # 8\n):\n    pid = tl.program_id(0)\n    # Compute (b, h)\n    h = pid % num_qo_heads\n    b = pid // num_qo_heads\n    if b >= batch_size:\n        return\n\n    # Load KV range for this batch\n    page_start = tl.load(kv_indptr_ptr + b).to(tl.int32)\n    page_end = tl.load(kv_indptr_ptr + (b + 1)).to(tl.int32)\n    seq_len = page_end - page_start\n\n    # Early exit for empty sequences\n    if seq_len <= 0:\n        d_offsets = tl.arange(0, D_HEAD)\n        o_ptrs = output_ptr + b * stride_o_b + h * stride_o_h + d_offsets * stride_o_d\n        tl.store(o_ptrs, tl.zeros((D_HEAD,), dtype=tl.bfloat16))\n        lse_out_ptr = lse_ptr + b * stride_lse_b + h * stride_lse_h\n        neg_inf = -float(\"inf\")\n        tl.store(lse_out_ptr, tl.full((), neg_inf, dtype=tl.float32))\n        return\n\n    # Load Q vector for this (b, h)\n    d_offsets = tl.arange(0, D_HEAD)\n    q_ptrs = q_ptr + b * stride_q_b + h * stride_q_h + d_offsets * stride_q_d\n    q_vec = tl.load(q_ptrs).to(tl.float32)\n\n    # Initialize streaming softmax state\n    neg_inf = tl.full((), -float(\"inf\"), dtype=tl.float32)\n    m_i = neg_inf\n    l_i = tl.zeros((), dtype=tl.float32)\n    acc = tl.zeros((D_HEAD,), dtype=tl.float32)\n\n    # Determine KV head from GQA mapping\n    kv_head = (h // GQA_RATIO).to(tl.int32)\n\n    # Iterate over tokens in blocks\n    pos = tl.zeros((), dtype=tl.int32)\n    while pos < seq_len:\n        t_offsets = tl.arange(0, BLOCK_M)\n        curr = pos + t_offsets\n        mask_t = curr < seq_len\n\n        # Gather page ids\n        page_ids = tl.load(kv_indices_ptr + page_start + curr, mask=mask_t, other=0).to(tl.int32)\n\n        # K pointers [BLOCK_M, D_HEAD]\n        k_ptrs = (\n            k_cache_ptr\n            + page_ids[:, None] * stride_k_p\n            + 0 * stride_k_s\n            + kv_head * stride_k_h\n            + d_offsets[None, :] * stride_k_d\n        )\n        k_block = tl.load(k_ptrs, mask=mask_t[:, None], other=0).to(tl.float32)\n\n        # Compute logits for this block: [BLOCK_M]\n        logits = tl.sum(k_block * q_vec[None, :], axis=1)\n        logits_scaled = logits * sm_scale\n        logits_scaled = tl.where(mask_t, logits_scaled, neg_inf)\n\n        # Block-level max\n        m_curr = tl.max(logits_scaled, axis=0)\n        m_new = tl.maximum(m_i, m_curr)\n\n        # Compute p = exp(logits - m_new)\n        p = tl.exp(logits_scaled - m_new)\n        # sum of p\n        l_part = tl.sum(p, axis=0)\n        # Update l_i\n        l_i = l_i * tl.exp(m_i - m_new) + l_part\n\n        # V pointers and weighted accumulation\n        v_ptrs = (\n            v_cache_ptr\n            + page_ids[:, None] * stride_v_p\n            + 0 * stride_v_s\n            + kv_head * stride_v_h\n            + d_offsets[None, :] * stride_v_d\n        )\n        v_block = tl.load(v_ptrs, mask=mask_t[:, None], other=0).to(tl.float32)\n        weighted = tl.sum(v_block * p[:, None], axis=0)\n\n        # Update accumulator and max\n        acc = acc * tl.exp(m_i - m_new) + weighted\n        m_i = m_new\n\n        pos += BLOCK_M\n\n    # Finalize output\n    nonempty = l_i > 0.0\n    out_vec = tl.where(nonempty, acc / l_i, tl.zeros((D_HEAD,), dtype=tl.float32))\n    inv_ln2 = 1.4426950408889634  # 1 / ln(2)\n    lse_val = tl.where(nonempty, (tl.log(l_i) + m_i) * inv_ln2, neg_inf)\n\n    # Store output and lse\n    o_ptrs = output_ptr + b * stride_o_b + h * stride_o_h + d_offsets * stride_o_d\n    tl.store(o_ptrs, out_vec.to(tl.bfloat16))\n    lse_out_ptr = lse_ptr + b * stride_lse_b + h * stride_lse_h\n    tl.store(lse_out_ptr, lse_val)\n\n\ndef _ensure_cuda(t: torch.Tensor, device: torch.device):\n    if t.device.type == \"cuda\":\n        if t.device != device:\n            return t.to(device)\n        return t\n    else:\n        if not torch.cuda.is_available():\n            raise RuntimeError(\"CUDA is not available, but GPU execution is required.\")\n        return t.to(device, non_blocking=True)\n\n\ndef run(q, k_cache, v_cache, kv_indptr, kv_indices, sm_scale=None):\n    # Validate and default sm_scale\n    HEAD_DIM = 128\n    NUM_QO_HEADS = 32\n    NUM_KV_HEADS = 4\n    PAGE_SIZE = 1\n    GQA_RATIO = NUM_QO_HEADS // NUM_KV_HEADS\n    if sm_scale is None:\n        sm_scale = 1.0 / math.sqrt(HEAD_DIM)\n\n    # Convert sm_scale to Python float\n    if isinstance(sm_scale, (float, int)):\n        sm_scale_val = float(sm_scale)\n    elif isinstance(sm_scale, torch.Tensor):\n        if sm_scale.numel() != 1:\n            raise ValueError(\"sm_scale must be a scalar.\")\n        sm_scale_val = float(sm_scale.detach().cpu().item())\n    else:\n        raise TypeError(\"sm_scale must be a float, int, or 0-dim torch.Tensor\")\n\n    # Extract shapes and validate\n    if q.ndim != 3:\n        raise ValueError(\"q must have shape [batch_size, num_qo_heads, head_dim]\")\n    batch_size, num_qo_heads, head_dim = q.shape\n    if k_cache.ndim != 4:\n        raise ValueError(\"k_cache must have shape [num_pages, page_size, num_kv_heads, head_dim]\")\n    if v_cache.ndim != 4:\n        raise ValueError(\"v_cache must have shape [num_pages, page_size, num_kv_heads, head_dim]\")\n\n    num_pages, page_size, num_kv_heads, head_dim_k = k_cache.shape\n    num_pages_v, page_size_v, num_kv_heads_v, head_dim_v = v_cache.shape\n\n    if num_pages != num_pages_v or page_size != page_size_v or num_kv_heads != num_kv_heads_v or head_dim_k != head_dim_v:\n        raise ValueError(\"k_cache and v_cache shapes must match\")\n    if num_qo_heads != NUM_QO_HEADS:\n        raise AssertionError(\"num_qo_heads must be 32\")\n    if num_kv_heads != NUM_KV_HEADS:\n        raise AssertionError(\"num_kv_heads must be 4\")\n    if head_dim != HEAD_DIM or head_dim_k != HEAD_DIM:\n        raise AssertionError(\"head_dim must be 128\")\n    if page_size != PAGE_SIZE:\n        raise AssertionError(\"page_size must be 1\")\n\n    if kv_indptr.ndim != 1:\n        raise ValueError(\"kv_indptr must be 1-D\")\n    if kv_indices.ndim != 1:\n        raise ValueError(\"kv_indices must be 1-D\")\n    len_indptr = kv_indptr.shape[0]\n    num_kv_indices = kv_indices.shape[0]\n    if len_indptr != batch_size + 1:\n        raise AssertionError(\"len_indptr must be batch_size + 1\")\n    # kv_indptr[-1] value (synchronize to host once)\n    last_ind = int(kv_indptr[-1].detach().cpu().item())\n    if num_kv_indices != last_ind:\n        raise AssertionError(\"num_kv_indices must equal kv_indptr[-1].item()\")\n\n    # Dtypes\n    if q.dtype != torch.bfloat16:\n        raise TypeError(\"q must be bfloat16\")\n    if k_cache.dtype != torch.bfloat16 or v_cache.dtype != torch.bfloat16:\n        raise TypeError(\"k_cache and v_cache must be bfloat16\")\n    if kv_indptr.dtype != torch.int32 or kv_indices.dtype != torch.int32:\n        raise TypeError(\"kv_indptr and kv_indices must be int32\")\n\n    # Determine working device\n    if not torch.cuda.is_available():\n        for t in (q, k_cache, v_cache, kv_indptr, kv_indices):\n            if t.is_cuda:\n                raise RuntimeError(\"Input tensor is on CUDA device, but CUDA is not available.\")\n        raise RuntimeError(\"CUDA is not available. Triton kernel cannot run on CPU.\")\n\n    if q.device.type == \"cuda\":\n        work_device = q.device\n    elif k_cache.device.type == \"cuda\":\n        work_device = k_cache.device\n    elif v_cache.device.type == \"cuda\":\n        work_device = v_cache.device\n    elif kv_indptr.device.type == \"cuda\":\n        work_device = kv_indptr.device\n    elif kv_indices.device.type == \"cuda\":\n        work_device = kv_indices.device\n    else:\n        work_device = torch.device(\"cuda\")\n\n    orig_q_device = q.device\n    # Move tensors to working device\n    q_dev = _ensure_cuda(q.contiguous(), work_device)\n    k_cache_dev = _ensure_cuda(k_cache.contiguous(), work_device)\n    v_cache_dev = _ensure_cuda(v_cache.contiguous(), work_device)\n    kv_indptr_dev = _ensure_cuda(kv_indptr.contiguous(), work_device)\n    kv_indices_dev = _ensure_cuda(kv_indices.contiguous(), work_device)\n\n    # Allocate outputs on working device\n    output_dev = torch.empty((batch_size, NUM_QO_HEADS, HEAD_DIM), dtype=torch.bfloat16, device=work_device)\n    lse_dev = torch.empty((batch_size, NUM_QO_HEADS), dtype=torch.float32, device=work_device)\n\n    # Launch kernel\n    BLOCK_M = 128  # tokens per block\n    grid = (batch_size * NUM_QO_HEADS,)\n\n    gqa_paged_decode_h32_kv4_d128_ps1_kernel[grid](\n        q_dev,\n        k_cache_dev,\n        v_cache_dev,\n        kv_indptr_dev,\n        kv_indices_dev,\n        sm_scale_val,\n        output_dev,\n        lse_dev,\n        batch_size,\n        NUM_QO_HEADS,\n        # q strides\n        q_dev.stride(0), q_dev.stride(1), q_dev.stride(2),\n        # k strides\n        k_cache_dev.stride(0), k_cache_dev.stride(1), k_cache_dev.stride(2), k_cache_dev.stride(3),\n        # v strides\n        v_cache_dev.stride(0), v_cache_dev.stride(1), v_cache_dev.stride(2), v_cache_dev.stride(3),\n        # output strides\n        output_dev.stride(0), output_dev.stride(1), output_dev.stride(2),\n        # lse strides\n        lse_dev.stride(0), lse_dev.stride(1),\n        BLOCK_M=BLOCK_M,\n        D_HEAD=HEAD_DIM,\n        GQA_RATIO=GQA_RATIO,\n        num_warps=8,\n        num_stages=2,\n    )\n\n    # Move outputs back to original device of q\n    if orig_q_device != work_device:\n        output = output_dev.to(orig_q_device)\n        lse = lse_dev.to(orig_q_device)\n    else:\n        output = output_dev\n        lse = lse_dev\n\n    return output, lse"
    }
  ],
  "description": "gpt-5-2025-08-07 high reasoning effort optimized kernel for gqa_paged_decode_h32_kv4_d128_ps1 (round 2)"
}