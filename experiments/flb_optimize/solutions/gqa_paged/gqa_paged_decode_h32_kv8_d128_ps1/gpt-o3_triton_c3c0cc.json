{
  "name": "gpt-o3_triton_c3c0cc",
  "definition": "gqa_paged_decode_h32_kv8_d128_ps1",
  "author": "gpt-o3",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "B200"
    ],
    "entry_point": "main.py::run",
    "dependencies": []
  },
  "sources": [
    {
      "path": "main.py",
      "content": "import math\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef gqa_paged_decode_kernel(\n    q_ptr,               # *bf16  [B, 32, 128]\n    k_ptr,               # *bf16  [N_pages, 8, 128]  (page_size squeezed)\n    v_ptr,               # *bf16  [N_pages, 8, 128]  (page_size squeezed)\n    kv_indptr_ptr,       # *int32 [B + 1]\n    kv_indices_ptr,      # *int32 [num_kv_indices]\n    sm_scale,            # fp32 scalar\n    out_ptr,             # *bf16  [B, 32, 128]\n    lse_ptr,             # *fp32  [B, 32]\n    BLOCK_T: tl.constexpr,\n    HEAD_DIM: tl.constexpr,\n    NUM_QO_HEADS: tl.constexpr,\n    NUM_KV_HEADS: tl.constexpr,\n):\n    pid = tl.program_id(0)\n\n    batch_idx = pid // NUM_QO_HEADS\n    qo_head   = pid % NUM_QO_HEADS\n    gqa_ratio = NUM_QO_HEADS // NUM_KV_HEADS\n    kv_head   = qo_head // gqa_ratio\n\n    # ---- strides (in elements, not bytes) ----\n    stride_q_batch    = NUM_QO_HEADS * HEAD_DIM\n    stride_q_head     = HEAD_DIM\n\n    stride_k_page     = NUM_KV_HEADS * HEAD_DIM         # page_size = 1\n    stride_k_kv_head  = HEAD_DIM\n\n    stride_v_page     = stride_k_page\n    stride_v_kv_head  = HEAD_DIM\n\n    # ---- load query vector ----\n    d_offs = tl.arange(0, HEAD_DIM)\n    q_ptr_head = q_ptr + batch_idx * stride_q_batch + qo_head * stride_q_head + d_offs\n    q_vec = tl.cast(tl.load(q_ptr_head), tl.float32)\n\n    # ---- sequence token range ----\n    start = tl.load(kv_indptr_ptr + batch_idx)\n    end   = tl.load(kv_indptr_ptr + batch_idx + 1)\n    num_tokens = end - start\n\n    # ---- streaming softmax vars ----\n    m_val   = tl.full([], -1e30, tl.float32)          # running max\n    d_val   = tl.zeros([], tl.float32)                # running sum exp\n    o_vec   = tl.zeros([HEAD_DIM], tl.float32)        # running output vector\n\n    offset = tl.zeros([], tl.int32)\n\n    while offset < num_tokens:\n        t_offs      = tl.arange(0, BLOCK_T)\n        remain      = num_tokens - offset\n        tok_mask    = t_offs < remain\n\n        # ---- load page indices ----\n        pages = tl.load(kv_indices_ptr + start + offset + t_offs,\n                        mask=tok_mask, other=0)\n\n        # ---- gather K / V ----\n        k_ptrs = k_ptr + pages[:, None] * stride_k_page + kv_head * stride_k_kv_head + d_offs[None, :]\n        v_ptrs = v_ptr + pages[:, None] * stride_v_page + kv_head * stride_v_kv_head + d_offs[None, :]\n\n        k_block = tl.cast(tl.load(k_ptrs, mask=tok_mask[:, None], other=0), tl.float32)\n        v_block = tl.cast(tl.load(v_ptrs, mask=tok_mask[:, None], other=0), tl.float32)\n\n        # ---- logits ----\n        logits = tl.sum(k_block * q_vec[None, :], axis=1) * sm_scale\n        logits = tl.where(tok_mask, logits, -1e30)\n\n        # ---- block softmax ----\n        m_block        = tl.max(logits, axis=0)\n        exp_logits     = tl.exp(logits - m_block)\n        sum_exp_block  = tl.sum(exp_logits, axis=0)\n        weighted_v     = tl.sum(exp_logits[:, None] * v_block, axis=0)\n\n        # ---- merge with running values ----\n        new_m      = tl.maximum(m_val, m_block)\n        alpha_prev = tl.exp(m_val - new_m)\n        alpha_blk  = tl.exp(m_block - new_m)\n\n        o_vec = o_vec * alpha_prev + weighted_v * alpha_blk\n        d_val = d_val * alpha_prev + sum_exp_block * alpha_blk\n        m_val = new_m\n\n        offset += BLOCK_T\n\n    inv_d   = tl.where(d_val == 0, 0.0, 1.0 / d_val)\n    out_vec = o_vec * inv_d\n    log2e   = 1.4426950408889634\n    lse_val = tl.where(d_val == 0,\n                       -1e30,\n                       (tl.log(d_val) + m_val) * log2e)\n\n    # ---- store ----\n    out_ptr_head = out_ptr + batch_idx * stride_q_batch + qo_head * stride_q_head + d_offs\n    tl.store(out_ptr_head, tl.cast(out_vec, tl.bfloat16))\n\n    lse_ptr_head = lse_ptr + batch_idx * NUM_QO_HEADS + qo_head\n    tl.store(lse_ptr_head, lse_val)\n\n\ndef run(q,\n        k_cache,\n        v_cache,\n        kv_indptr,\n        kv_indices,\n        sm_scale: float | None = None):\n    \"\"\"\n    Entry point for gqa_paged_decode_h32_kv8_d128_ps1.\n    Returns (output, lse).\n    \"\"\"\n    if sm_scale is None:\n        sm_scale = 1.0 / math.sqrt(128.0)\n\n    if not torch.cuda.is_available():\n        raise RuntimeError(\"CUDA device is required to run Triton kernels.\")\n\n    # move tensors to GPU if necessary\n    tensors = [q, k_cache, v_cache, kv_indptr, kv_indices]\n    device_tensors = [t.cuda() if not t.is_cuda else t for t in tensors]\n    q_dev, k_dev, v_dev, iptr_dev, idx_dev = [t.contiguous() for t in device_tensors]\n\n    batch_size = q_dev.shape[0]\n    num_qo_heads = 32\n    head_dim = 128\n\n    # squeeze page dimension (=1)\n    k_dev_flat = k_dev.squeeze(1).contiguous()\n    v_dev_flat = v_dev.squeeze(1).contiguous()\n\n    out_dev = torch.empty((batch_size, num_qo_heads, head_dim),\n                          dtype=torch.bfloat16,\n                          device=q_dev.device)\n    lse_dev = torch.empty((batch_size, num_qo_heads),\n                          dtype=torch.float32,\n                          device=q_dev.device)\n\n    # launch kernel\n    BLOCK_T = 128\n    grid = (batch_size * num_qo_heads,)\n\n    gqa_paged_decode_kernel[grid](\n        q_dev, k_dev_flat, v_dev_flat,\n        iptr_dev, idx_dev,\n        sm_scale,\n        out_dev, lse_dev,\n        BLOCK_T=BLOCK_T,\n        HEAD_DIM=128,\n        NUM_QO_HEADS=32,\n        NUM_KV_HEADS=8,\n        num_warps=4,\n        num_stages=4,\n    )\n\n    # move back to original device if needed\n    if not q.is_cuda:\n        return out_dev.cpu(), lse_dev.cpu()\n    return out_dev, lse_dev"
    }
  ],
  "description": "o3 optimized kernel for gqa_paged_decode_h32_kv8_d128_ps1 (round 1)"
}