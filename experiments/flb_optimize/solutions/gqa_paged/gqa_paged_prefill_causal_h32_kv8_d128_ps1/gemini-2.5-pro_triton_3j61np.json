{
  "name": "gemini-2.5-pro_triton_3j61np",
  "definition": "gqa_paged_prefill_causal_h32_kv8_d128_ps1",
  "author": "gemini-2.5-pro",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "B200"
    ],
    "entry_point": "main.py::run",
    "dependencies": []
  },
  "sources": [
    {
      "path": "main.py",
      "content": "import torch\nimport triton\nimport triton.language as tl\nimport math\nimport inspect\n\n# -------------------\n# Triton Kernel\n# -------------------\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_N': 64}, num_warps=4, num_stages=2),\n        triton.Config({'BLOCK_N': 128}, num_warps=4, num_stages=2),\n        triton.Config({'BLOCK_N': 256}, num_warps=4, num_stages=2),\n        triton.Config({'BLOCK_N': 64}, num_warps=8, num_stages=2),\n        triton.Config({'BLOCK_N': 128}, num_warps=8, num_stages=2),\n        triton.Config({'BLOCK_N': 256}, num_warps=8, num_stages=2),\n        triton.Config({'BLOCK_N': 64}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_N': 128}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_N': 256}, num_warps=4, num_stages=3),\n    ],\n    key=['total_q'],\n)\n@triton.jit\ndef _gqa_paged_prefill_causal_kernel(\n    # Pointers to Tensors\n    Q, K_cache, V_cache,\n    QO_indptr, KV_indptr, KV_indices,\n    Q_seq_idx_map,\n    sm_scale,\n    Output, LSE,\n    # Stride Args\n    stride_q_total, stride_q_head, stride_q_dim,\n    stride_k_page, stride_k_ps, stride_k_head, stride_k_dim,\n    stride_v_page, stride_v_ps, stride_v_head, stride_v_dim,\n    # Metadata\n    total_q,\n    num_qo_heads,\n    num_kv_heads,\n    # Constexpr\n    GQA_RATIO: tl.constexpr,\n    HEAD_DIM: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    PAGE_SIZE: tl.constexpr,\n):\n    \"\"\"\n    Triton kernel for GQA paged prefill with causal masking.\n    Each program computes the attention output for one query token and one query head.\n    Grid: (total_q, num_qo_heads)\n    \"\"\"\n    # 1. Get Program IDs for the current query token and head\n    global_q_idx = tl.program_id(0)\n    qo_head_idx = tl.program_id(1)\n\n    # 2. Find sequence boundaries for the current query token using the precomputed map\n    seq_idx = tl.load(Q_seq_idx_map + global_q_idx)\n    q_start = tl.load(QO_indptr + seq_idx)\n    q_end = tl.load(QO_indptr + seq_idx + 1)\n    kv_start = tl.load(KV_indptr + seq_idx)\n    kv_end = tl.load(KV_indptr + seq_idx + 1)\n\n    # 3. Determine causal boundary for attention\n    num_q_tokens = q_end - q_start\n    num_kv_tokens = kv_end - kv_start\n    q_idx_local = global_q_idx - q_start\n    delta = num_kv_tokens - num_q_tokens\n    max_kv_idx_for_q = q_idx_local + delta + 1\n\n    # 4. Handle edge case where a query has no keys to attend to.\n    if max_kv_idx_for_q <= 0:\n        # Store default values (0 for output, -inf for LSE) and exit.\n        output_ptr = Output + global_q_idx * stride_q_total + qo_head_idx * stride_q_head\n        offs_d = tl.arange(0, HEAD_DIM)\n        tl.store(output_ptr + offs_d, tl.zeros([HEAD_DIM], dtype=tl.bfloat16))\n\n        lse_ptr = LSE + global_q_idx * num_qo_heads + qo_head_idx\n        tl.store(lse_ptr, -float(\"inf\"))\n        return\n\n    # 5. Load Q vector for the current query token and head\n    offs_d = tl.arange(0, HEAD_DIM)\n    q_offset = global_q_idx * stride_q_total + qo_head_idx * stride_q_head\n    q_ptr = Q + q_offset\n    q = tl.load(q_ptr + offs_d).to(tl.float32)\n\n    # 6. Initialize accumulator, max logit, and lse for online softmax\n    acc = tl.zeros([HEAD_DIM], dtype=tl.float32)\n    m_i = -float(\"inf\")\n    l_i = 0.0\n\n    # 7. Loop over KV cache blocks\n    kv_head_idx = qo_head_idx // GQA_RATIO\n    for n_offset in range(0, max_kv_idx_for_q, BLOCK_N):\n        # a. Create indices and masks for the current KV block\n        offs_n = n_offset + tl.arange(0, BLOCK_N)\n        kv_indices_offs = kv_start + offs_n\n        kv_mask = (offs_n < max_kv_idx_for_q)\n        page_indices_mask = kv_mask & (kv_indices_offs < kv_end)\n\n        # b. Load page indices from KV_indices global memory\n        page_ids = tl.load(KV_indices + kv_indices_offs, mask=page_indices_mask, other=0)\n\n        # c. Load K block using gathered page_ids\n        k_ptr = K_cache + (page_ids[:, None] * stride_k_page +\n                           kv_head_idx * stride_k_head +\n                           offs_d[None, :])\n        k = tl.load(k_ptr, mask=page_indices_mask[:, None], other=0.0).to(tl.float32)\n\n        # d. FIX: Compute Q @ K^T scores using element-wise multiplication and reduction.\n        # tl.dot is not suitable for matrix-vector products due to tensor core dimension constraints.\n        s = tl.sum(k * q[None, :], axis=1)\n\n        s *= sm_scale\n        s = tl.where(kv_mask, s, -float(\"inf\"))\n\n        # e. Online softmax update\n        m_ij = tl.maximum(m_i, tl.max(s, 0))\n        p = tl.exp(s - m_ij)\n        l_ij = tl.exp(m_i - m_ij) * l_i + tl.sum(p, 0)\n\n        # f. Update accumulator (rescale previous accumulator)\n        acc_scale = tl.exp(m_i - m_ij)\n        acc = acc * acc_scale\n\n        # Load V block, transposing it on the fly for calculation\n        v_ptr_T = V_cache + (page_ids[None, :] * stride_v_page +\n                             kv_head_idx * stride_v_head +\n                             offs_d[:, None])\n        v_T = tl.load(v_ptr_T, mask=page_indices_mask[None, :], other=0.0)\n\n        p_typed = p.to(v_T.dtype)\n\n        # FIX: Update accumulator using element-wise multiplication and reduction.\n        # This correctly computes the weighted sum of value vectors (V.T @ p)\n        acc_update = tl.sum(v_T * p_typed[None, :], axis=1)\n        acc += acc_update\n\n        # g. Update softmax stats for next iteration\n        m_i = m_ij\n        l_i = l_ij\n\n    # 8. Finalize output and LSE\n    o = acc / l_i\n    lse_val = m_i + tl.log(l_i)\n    lse_val *= 1.4426950408889634  # 1.0 / math.log(2)\n\n    # 9. Store results to global memory\n    output_ptr = Output + global_q_idx * stride_q_total + qo_head_idx * stride_q_head\n    tl.store(output_ptr + offs_d, o.to(tl.bfloat16))\n\n    lse_ptr = LSE + global_q_idx * num_qo_heads + qo_head_idx\n    tl.store(lse_ptr, lse_val)\n\n\n# -------------------\n# Host-side Wrapper\n# -------------------\n\ndef gqa_paged_prefill_causal_h32_kv8_d128_ps1(\n    q: torch.Tensor,\n    k_cache: torch.Tensor,\n    v_cache: torch.Tensor,\n    qo_indptr: torch.Tensor,\n    kv_indptr: torch.Tensor,\n    kv_indices: torch.Tensor,\n    sm_scale: float = None,\n):\n    \"\"\"\n    Computes Grouped-Query Attention for paged prefill phase with causal masking.\n\n    Args:\n        q (torch.Tensor): Query tensor of shape [total_q, num_qo_heads, head_dim].\n        k_cache (torch.Tensor): Key cache tensor of shape [num_pages, page_size, num_kv_heads, head_dim].\n        v_cache (torch.Tensor): Value cache tensor of shape [num_pages, page_size, num_kv_heads, head_dim].\n        qo_indptr (torch.Tensor): Query offsets for each sequence of shape [batch_size + 1].\n        kv_indptr (torch.Tensor): KV page offsets for each sequence of shape [batch_size + 1].\n        kv_indices (torch.Tensor): Page IDs for KV cache lookups of shape [num_kv_indices].\n        sm_scale (float, optional): Softmax scale. Defaults to 1/sqrt(head_dim).\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor]:\n            - The attention output tensor of shape [total_q, num_qo_heads, head_dim].\n            - The log-sum-exp of attention logits (base 2) of shape [total_q, num_qo_heads].\n    \"\"\"\n    # 1. Validate inputs and extract dimensions\n    assert q.dim() == 3, \"q must be a 3D tensor\"\n    assert k_cache.dim() == 4, \"k_cache must be a 4D tensor\"\n    assert v_cache.dim() == 4, \"v_cache must be a 4D tensor\"\n    assert q.dtype == torch.bfloat16\n    assert k_cache.dtype == torch.bfloat16\n    assert v_cache.dtype == torch.bfloat16\n    assert qo_indptr.dtype == torch.int32\n    assert kv_indptr.dtype == torch.int32\n    assert kv_indices.dtype == torch.int32\n\n    total_q, num_qo_heads, head_dim = q.shape\n    num_pages, page_size, num_kv_heads, _ = k_cache.shape\n\n    # Constants from spec\n    assert num_qo_heads == 32\n    assert num_kv_heads == 8\n    assert head_dim == 128\n    assert page_size == 1\n\n    if sm_scale is None:\n        sm_scale = 1.0 / math.sqrt(head_dim)\n\n    # 2. Prepare outputs\n    output = torch.empty_like(q)\n    lse = torch.empty((total_q, num_qo_heads), device=q.device, dtype=torch.float32)\n\n    # 3. Pre-compute a map from global query index to sequence index.\n    # This avoids a slow and divergent search loop inside the kernel.\n    batch_size = qo_indptr.numel() - 1\n    q_seq_len = qo_indptr[1:] - qo_indptr[:-1]\n    q_seq_idx_map = torch.arange(batch_size, device=q.device, dtype=torch.int32).repeat_interleave(q_seq_len)\n\n    # 4. Set up grid and call kernel\n    grid = (total_q, num_qo_heads)\n\n    _gqa_paged_prefill_causal_kernel[grid](\n        # Tensors\n        q, k_cache, v_cache,\n        qo_indptr, kv_indptr, kv_indices,\n        q_seq_idx_map,\n        sm_scale,\n        output, lse,\n        # Strides\n        q.stride(0), q.stride(1), q.stride(2),\n        k_cache.stride(0), k_cache.stride(1), k_cache.stride(2), k_cache.stride(3),\n        v_cache.stride(0), v_cache.stride(1), v_cache.stride(2), v_cache.stride(3),\n        # Metadata\n        total_q,\n        num_qo_heads,\n        num_kv_heads,\n        # Constexpr\n        GQA_RATIO=num_qo_heads // num_kv_heads,\n        HEAD_DIM=head_dim,\n        PAGE_SIZE=page_size,\n    )\n\n    return output, lse\n\n\n# -------------------\n# Entry Point\n# -------------------\n\ndef run(*args, **kwargs):\n    \"\"\"\n    Public entry point for the Triton kernel.\n    Handles device management and calls the main logic.\n    \"\"\"\n    # 1. Get the signature of the core logic function\n    sig = inspect.signature(gqa_paged_prefill_causal_h32_kv8_d128_ps1)\n\n    # 2. Bind the passed arguments to the signature\n    try:\n        bound_args = sig.bind(*args, **kwargs)\n        bound_args.apply_defaults()\n    except TypeError as e:\n        raise TypeError(f\"Error binding arguments: {e}\") from e\n\n    # 3. Extract tensor arguments and sm_scale\n    all_args = bound_args.arguments\n    q = all_args['q']\n    k_cache = all_args['k_cache']\n    v_cache = all_args['v_cache']\n    qo_indptr = all_args['qo_indptr']\n    kv_indptr = all_args['kv_indptr']\n    kv_indices = all_args['kv_indices']\n    sm_scale = all_args['sm_scale']\n\n    # 4. Device management\n    if not torch.cuda.is_available():\n        raise RuntimeError(\"Triton kernel requires CUDA.\")\n\n    tensor_args = [q, k_cache, v_cache, qo_indptr, kv_indptr, kv_indices]\n\n    # Determine the target device from the first available CUDA tensor, or default to 'cuda'\n    target_device = 'cuda'\n    for t in tensor_args:\n        if isinstance(t, torch.Tensor) and t.is_cuda:\n            target_device = t.device\n            break\n\n    # Store original devices to move results back later\n    original_devices = [t.device for t in tensor_args]\n\n    # Move all tensors to the target device\n    moved_tensors = [t.to(target_device) for t in tensor_args]\n    q, k_cache, v_cache, qo_indptr, kv_indptr, kv_indices = moved_tensors\n\n    # 5. Call the kernel\n    output, lse = gqa_paged_prefill_causal_h32_kv8_d128_ps1(\n        q, k_cache, v_cache, qo_indptr, kv_indptr, kv_indices, sm_scale\n    )\n\n    # 6. Move results back to the original device of the 'q' tensor\n    q_orig_device = original_devices[0]\n    output = output.to(q_orig_device)\n    lse = lse.to(q_orig_device)\n\n    return output, lse"
    }
  ],
  "description": "gemini-2.5-pro optimized kernel for gqa_paged_prefill_causal_h32_kv8_d128_ps1 (round 10)"
}