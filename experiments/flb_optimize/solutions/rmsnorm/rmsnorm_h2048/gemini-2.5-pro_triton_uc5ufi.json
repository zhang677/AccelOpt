{
  "name": "gemini-2.5-pro_triton_uc5ufi",
  "definition": "rmsnorm_h2048",
  "author": "gemini-2.5-pro",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "B200"
    ],
    "entry_point": "main.py::run",
    "dependencies": []
  },
  "sources": [
    {
      "path": "main.py",
      "content": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef rmsnorm_h2048_kernel(\n    # Pointers to tensors\n    X_ptr,\n    Weight_ptr,\n    Output_ptr,\n    # Stride to move to the next row\n    stride_x_batch,\n    stride_output_batch,\n    # Constants\n    HIDDEN_SIZE: tl.constexpr,\n    EPS: tl.constexpr,\n):\n    \"\"\"\n    Triton kernel for RMS Normalization.\n    This kernel is optimized for hidden_size=2048 on B200 GPUs.\n    Each program instance handles a single row of the input tensor.\n    \"\"\"\n    # --- Grid and Block Configuration ---\n    # Each program instance processes one row of the input tensor.\n    row_idx = tl.program_id(0)\n\n    # --- Pointer Setup ---\n    # Pointers to the start of the current row for input and output.\n    x_row_ptr = X_ptr + row_idx * stride_x_batch\n    output_row_ptr = Output_ptr + row_idx * stride_output_batch\n\n    # Pointers to the columns of the current row.\n    col_offsets = tl.arange(0, HIDDEN_SIZE)\n    x_ptrs = x_row_ptr + col_offsets\n    weight_ptrs = Weight_ptr + col_offsets\n    output_ptrs = output_row_ptr + col_offsets\n\n    # --- Computation ---\n    # Load the input row and weights.\n    # For HIDDEN_SIZE=2048, the entire row and weight vector fit into SRAM,\n    # enabling a single-pass algorithm.\n    x_bf16 = tl.load(x_ptrs)\n    w_bf16 = tl.load(weight_ptrs)\n\n    # Upcast to float32 for high-precision calculation of variance.\n    x_fp32 = x_bf16.to(tl.float32)\n    \n    # Calculate the sum of squares.\n    # This is a block-level reduction handled efficiently by Triton.\n    sum_of_squares = tl.sum(x_fp32 * x_fp32, axis=0)\n    \n    # Calculate variance and the inverse root mean square.\n    variance = sum_of_squares / HIDDEN_SIZE\n    inv_rms = tl.rsqrt(variance + EPS)\n\n    # Normalize the input and apply the learned scaling factor (weight).\n    # Reuse the loaded `x_fp32` from registers/SRAM.\n    w_fp32 = w_bf16.to(tl.float32)\n    output_fp32 = x_fp32 * inv_rms * w_fp32\n\n    # Cast the final result back to bfloat16 and store it.\n    output_bf16 = output_fp32.to(tl.bfloat16)\n    tl.store(output_ptrs, output_bf16)\n\n\ndef rmsnorm_h2048(hidden_states: torch.Tensor, weight: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Host-side wrapper for the RMSNorm Triton kernel.\n\n    This function handles device management, input validation, kernel launching,\n    and moving the final result back to the original device.\n    \n    Args:\n        hidden_states (torch.Tensor): Input tensor of shape [batch_size, 2048] and dtype bfloat16.\n        weight (torch.Tensor): Weight tensor of shape [2048] and dtype bfloat16.\n        \n    Returns:\n        torch.Tensor: The normalized output tensor of the same shape and dtype as the input.\n    \"\"\"\n    # --- Device Management and Input Validation ---\n    original_device = hidden_states.device\n    \n    # Triton requires a CUDA-enabled GPU.\n    if not torch.cuda.is_available():\n        if original_device.type == 'cuda':\n            raise RuntimeError(\"CUDA is not available, but input tensor is on a CUDA device.\")\n        raise RuntimeError(\"Triton requires a CUDA-enabled GPU, but none was found.\")\n    \n    # Move tensors to the GPU if they are on the CPU.\n    device = torch.device(\"cuda\")\n    hidden_states_gpu = hidden_states.to(device) if hidden_states.device.type == 'cpu' else hidden_states\n    weight_gpu = weight.to(device) if weight.device.type == 'cpu' else weight\n\n    # Ensure all tensors are on the same CUDA device.\n    if weight_gpu.device != hidden_states_gpu.device:\n        weight_gpu = weight_gpu.to(hidden_states_gpu.device)\n        \n    # --- Shape and DType Validation ---\n    batch_size, hidden_size = hidden_states_gpu.shape\n    \n    if hidden_size != 2048:\n        raise ValueError(f\"Expected hidden_size=2048, but got {hidden_size}\")\n    if weight_gpu.shape != (2048,):\n        raise ValueError(f\"Expected weight shape=(2048,), but got {weight_gpu.shape}\")\n    if hidden_states_gpu.dtype != torch.bfloat16:\n        raise TypeError(f\"Expected hidden_states dtype=torch.bfloat16, but got {hidden_states_gpu.dtype}\")\n    if weight_gpu.dtype != torch.bfloat16:\n        raise TypeError(f\"Expected weight dtype=torch.bfloat16, but got {weight_gpu.dtype}\")\n    \n    # Ensure inputs are contiguous for safe memory access.\n    if not hidden_states_gpu.is_contiguous():\n        hidden_states_gpu = hidden_states_gpu.contiguous()\n    if not weight_gpu.is_contiguous():\n        weight_gpu = weight_gpu.contiguous()\n\n    # --- Kernel Launch ---\n    # Allocate the output tensor on the GPU.\n    output = torch.empty_like(hidden_states_gpu)\n\n    # The grid is 1D, with one program instance per row in the batch.\n    grid = (batch_size,)\n    \n    # Constants for the kernel.\n    EPS = 1e-6\n    \n    # For modern GPUs like B200, 8 warps is a good choice for a 2048-element reduction.\n    # This gives 256 threads, each handling 8 elements, balancing parallelism and per-thread work.\n    num_warps = 8\n\n    # Call the Triton kernel.\n    rmsnorm_h2048_kernel[grid](\n        hidden_states_gpu,\n        weight_gpu,\n        output,\n        hidden_states_gpu.stride(0),\n        output.stride(0),\n        HIDDEN_SIZE=hidden_size,\n        EPS=EPS,\n        num_warps=num_warps,\n    )\n\n    # --- Post-computation Device Management ---\n    # Move the output back to the original device of the input tensor.\n    if output.device != original_device:\n        output = output.to(original_device)\n        \n    return output\n\n\ndef run(*args, **kwargs) -> torch.Tensor:\n    \"\"\"\n    Public entry point for the rmsnorm_h2048 operation.\n    \n    This function handles generic `*args` and `**kwargs` inputs, dispatches\n    to the main `rmsnorm_h2048` implementation, and ensures compatibility\n    with various calling conventions.\n    \n    Args:\n        *args: Positional arguments. Expected: `hidden_states`, `weight`.\n        **kwargs: Keyword arguments. Expected: `hidden_states`, `weight`.\n        \n    Returns:\n        torch.Tensor: The result of the RMS normalization, with the same shape,\n                      dtype, and device as the `hidden_states` input.\n    \"\"\"\n    # Extract arguments by name from kwargs first, then fall back to args.\n    hidden_states = kwargs.get('hidden_states')\n    if hidden_states is None:\n        if len(args) > 0:\n            hidden_states = args[0]\n        else:\n            raise TypeError(\"run() missing 1 required positional argument: 'hidden_states'\")\n\n    weight = kwargs.get('weight')\n    if weight is None:\n        if len(args) > 1:\n            weight = args[1]\n        else:\n            raise TypeError(\"run() missing 1 required positional argument: 'weight'\")\n            \n    return rmsnorm_h2048(hidden_states, weight)"
    }
  ],
  "description": "gemini-2.5-pro optimized kernel for rmsnorm_h2048 (round 1)"
}