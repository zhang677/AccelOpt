{
  "name": "claude-opus-4-1_triton_d347a1",
  "definition": "rmsnorm_h512",
  "author": "claude-opus-4-1-20250805",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "B200"
    ],
    "entry_point": "main.py::run",
    "dependencies": []
  },
  "sources": [
    {
      "path": "main.py",
      "content": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef rmsnorm_h512_kernel(\n    hidden_states_ptr,\n    weight_ptr,\n    output_ptr,\n    batch_size,\n    HIDDEN_SIZE: tl.constexpr,\n    EPS: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    # Process one row per program\n    row_idx = tl.program_id(0)\n    \n    if row_idx >= batch_size:\n        return\n    \n    # Compute row offset\n    row_start = row_idx * HIDDEN_SIZE\n    \n    # Load and accumulate in multiple passes for better precision\n    acc = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    \n    for offset in range(0, HIDDEN_SIZE, BLOCK_SIZE):\n        cols = offset + tl.arange(0, BLOCK_SIZE)\n        mask = cols < HIDDEN_SIZE\n        \n        # Load hidden states for this block\n        hidden_states_ptrs = hidden_states_ptr + row_start + cols\n        hidden_vals = tl.load(hidden_states_ptrs, mask=mask, other=0.0).to(tl.float32)\n        \n        # Accumulate squared values\n        acc += hidden_vals * hidden_vals\n    \n    # Compute mean of squared values\n    mean_sq = tl.sum(acc) / HIDDEN_SIZE\n    \n    # Compute inverse RMS\n    inv_rms = tl.rsqrt(mean_sq + EPS)\n    \n    # Apply normalization and weight in blocks\n    for offset in range(0, HIDDEN_SIZE, BLOCK_SIZE):\n        cols = offset + tl.arange(0, BLOCK_SIZE)\n        mask = cols < HIDDEN_SIZE\n        \n        # Load hidden states\n        hidden_states_ptrs = hidden_states_ptr + row_start + cols\n        hidden_vals = tl.load(hidden_states_ptrs, mask=mask, other=0.0).to(tl.float32)\n        \n        # Load weights\n        weight_ptrs = weight_ptr + cols\n        weight_vals = tl.load(weight_ptrs, mask=mask, other=0.0).to(tl.float32)\n        \n        # Apply RMSNorm: (x * inv_rms) * weight\n        normed = hidden_vals * inv_rms * weight_vals\n        \n        # Store output\n        output_ptrs = output_ptr + row_start + cols\n        tl.store(output_ptrs, normed.to(tl.bfloat16), mask=mask)\n\n\ndef run(hidden_states, weight):\n    # Validate inputs\n    assert hidden_states.ndim == 2, f\"hidden_states must be 2D, got {hidden_states.ndim}D\"\n    assert weight.ndim == 1, f\"weight must be 1D, got {weight.ndim}D\"\n    \n    batch_size, hidden_size = hidden_states.shape\n    assert hidden_size == 512, f\"hidden_size must be 512, got {hidden_size}\"\n    assert weight.shape[0] == hidden_size, f\"weight shape mismatch: {weight.shape[0]} != {hidden_size}\"\n    \n    # Handle device management\n    original_device = hidden_states.device\n    \n    # Move to GPU if needed\n    if hidden_states.device.type == 'cpu':\n        if not torch.cuda.is_available():\n            raise RuntimeError(\"CUDA is not available but GPU tensors are required for Triton kernels\")\n        hidden_states = hidden_states.cuda()\n    \n    if weight.device.type == 'cpu':\n        if not torch.cuda.is_available():\n            raise RuntimeError(\"CUDA is not available but GPU tensors are required for Triton kernels\")\n        weight = weight.cuda()\n    \n    # Ensure both tensors are on the same GPU\n    if hidden_states.device != weight.device:\n        weight = weight.to(hidden_states.device)\n    \n    # Ensure correct dtype\n    if hidden_states.dtype != torch.bfloat16:\n        hidden_states = hidden_states.to(torch.bfloat16)\n    if weight.dtype != torch.bfloat16:\n        weight = weight.to(torch.bfloat16)\n    \n    # Allocate output tensor\n    output = torch.empty_like(hidden_states, dtype=torch.bfloat16, device=hidden_states.device)\n    \n    # Constants\n    HIDDEN_SIZE = 512\n    EPS = 1e-6\n    BLOCK_SIZE = 128  # Optimized for B200's memory hierarchy\n    \n    # Launch kernel with one program per row\n    grid = (batch_size,)\n    \n    rmsnorm_h512_kernel[grid](\n        hidden_states,\n        weight,\n        output,\n        batch_size,\n        HIDDEN_SIZE=HIDDEN_SIZE,\n        EPS=EPS,\n        BLOCK_SIZE=BLOCK_SIZE,\n    )\n    \n    # Move result back to original device if needed\n    if original_device.type == 'cpu':\n        output = output.cpu()\n    \n    return output"
    }
  ],
  "description": "claude-opus-4-1-20250805 optimized kernel for rmsnorm_h512 (round 1)"
}