{
  "name": "gemini-2.5-pro_triton_xndzsl",
  "definition": "rmsnorm_h1536",
  "author": "gemini-2.5-pro",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "B200"
    ],
    "entry_point": "main.py::run",
    "dependencies": []
  },
  "sources": [
    {
      "path": "main.py",
      "content": "import torch\nimport triton\nimport triton.language as tl\nimport math\n\n# Reference implementation for fallback and verification\ndef _reference_run(hidden_states, weight):\n    \"\"\"\n    Reference PyTorch implementation for RMSNorm.\n    \"\"\"\n    batch_size, hidden_size = hidden_states.shape\n    assert hidden_size == 1536\n\n    EPS = 1e-6\n\n    x = hidden_states.to(torch.float32)\n    inv_rms = torch.rsqrt(x.pow(2).mean(dim=-1, keepdim=True) + EPS)\n    y = (x * inv_rms) * weight.to(torch.float32)\n    return y.to(hidden_states.dtype)\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n        triton.Config({}, num_warps=16),\n    ],\n    key=['HIDDEN_SIZE'],\n)\n@triton.jit\ndef _rmsnorm_h1536_kernel(\n    hidden_states_ptr,\n    weight_ptr,\n    output_ptr,\n    stride_hidden_states_batch,\n    stride_output_batch,\n    HIDDEN_SIZE: tl.constexpr,\n    EPS: tl.constexpr,\n    BLOCK_SIZE_H: tl.constexpr,\n):\n    \"\"\"\n    Triton kernel for RMS Normalization optimized for a fixed hidden_size.\n    \n    This kernel is fused to perform the normalization in a single pass over the data,\n    minimizing global memory access. It processes one row per program instance.\n    \n    1. It computes the sum of squares for a row.\n    2. It calculates the inverse root mean square (inv_rms).\n    3. It normalizes the input row with inv_rms, scales it by the weight vector,\n       and writes the result to the output.\n    \n    The use of tl.float32 for intermediate calculations (sum of squares) ensures\n    numerical accuracy.\n    \"\"\"\n    # Grid is 1D, so each program instance processes one row.\n    pid_row = tl.program_id(0)\n\n    # Pointers to the current row for inputs and output.\n    row_hidden_states_ptr = hidden_states_ptr + pid_row * stride_hidden_states_batch\n    row_output_ptr = output_ptr + pid_row * stride_output_batch\n\n    # --- 1. Compute mean of squares ---\n    # Create a block of offsets for the hidden dimension.\n    offs_h = tl.arange(0, BLOCK_SIZE_H)\n    mask_h = offs_h < HIDDEN_SIZE\n\n    # Load the row of hidden_states. Use masking to handle cases where\n    # BLOCK_SIZE_H > HIDDEN_SIZE. `other=0.0` ensures that padding\n    # doesn't affect the sum of squares.\n    x_block = tl.load(row_hidden_states_ptr + offs_h, mask=mask_h, other=0.0)\n    \n    # Promote to float32 for high-precision reduction.\n    x_f32 = x_block.to(tl.float32)\n    \n    # Calculate sum of squares.\n    sum_sq = tl.sum(x_f32 * x_f32, axis=0)\n    \n    # Calculate mean of squares. We divide by the actual HIDDEN_SIZE.\n    mean_sq = sum_sq / HIDDEN_SIZE\n    \n    # --- 2. Compute inverse root mean square ---\n    # Add epsilon for numerical stability and compute rsqrt.\n    inv_rms = tl.rsqrt(mean_sq + EPS)\n\n    # --- 3. Normalize, scale, and store ---\n    # Load the corresponding weights.\n    weight_block = tl.load(weight_ptr + offs_h, mask=mask_h)\n    \n    # Perform normalization and scaling.\n    # We reuse x_f32, which is already in registers from the initial load.\n    output_f32 = x_f32 * inv_rms * weight_block.to(tl.float32)\n\n    # Convert back to the output dtype (bfloat16) and store.\n    tl.store(row_output_ptr + offs_h, output_f32.to(tl.bfloat16), mask=mask_h)\n\n\ndef run(*args, **kwargs):\n    \"\"\"\n    Wrapper function for the RMSNorm Triton kernel.\n\n    Handles device management, argument parsing, and kernel launching.\n    It ensures that tensors are on the correct device (CUDA) for kernel\n    execution and that the output is moved back to the original device\n    of the input tensors.\n\n    Args:\n        hidden_states (torch.Tensor): The input tensor of shape [batch_size, 1536]\n                                      and dtype bfloat16.\n        weight (torch.Tensor): The weight tensor of shape [1536] and dtype bfloat16.\n    \n    Returns:\n        torch.Tensor: The normalized and scaled output tensor with the same shape\n                      and dtype as hidden_states.\n    \"\"\"\n    # --- Argument Parsing ---\n    hidden_states = kwargs.get('hidden_states')\n    weight = kwargs.get('weight')\n    \n    if len(args) > 0:\n        if hidden_states is not None:\n            raise TypeError(\"run() got multiple values for argument 'hidden_states'\")\n        hidden_states = args[0]\n    if len(args) > 1:\n        if weight is not None:\n            raise TypeError(\"run() got multiple values for argument 'weight'\")\n        weight = args[1]\n    if len(args) > 2:\n        raise TypeError(f\"run() takes 2 positional arguments but {len(args)} were given\")\n    \n    if hidden_states is None or weight is None:\n        raise TypeError(\"run() missing required arguments: 'hidden_states' or 'weight'\")\n\n    # --- Input Validation ---\n    assert isinstance(hidden_states, torch.Tensor), \"hidden_states must be a torch.Tensor\"\n    assert isinstance(weight, torch.Tensor), \"weight must be a torch.Tensor\"\n    assert hidden_states.shape[1] == 1536, \"hidden_size must be 1536\"\n    assert hidden_states.shape[1] == weight.shape[0], \"hidden_states and weight dimensions must match\"\n    assert hidden_states.ndim == 2, \"hidden_states must be a 2D tensor\"\n    assert weight.ndim == 1, \"weight must be a 1D tensor\"\n    assert hidden_states.dtype == torch.bfloat16, \"hidden_states dtype must be bfloat16\"\n    assert weight.dtype == torch.bfloat16, \"weight dtype must be bfloat16\"\n\n    # --- Device Management ---\n    original_device = hidden_states.device\n    \n    if not torch.cuda.is_available():\n        if original_device.type == 'cuda':\n             raise RuntimeError(\"CUDA is not available, but input tensors are on a CUDA device.\")\n        # As Triton is unavailable, fall back to a native PyTorch implementation on CPU.\n        print(\"Warning: Triton requires a CUDA-enabled GPU. Falling back to reference implementation on CPU.\")\n        return _reference_run(hidden_states, weight)\n\n    device = torch.device(\"cuda\")\n    hidden_states_gpu = hidden_states.to(device)\n    weight_gpu = weight.to(device)\n\n    # --- Allocate Output Tensor ---\n    output_gpu = torch.empty_like(hidden_states_gpu)\n    \n    # --- Kernel Launch ---\n    batch_size, hidden_size = hidden_states_gpu.shape\n    \n    # Use a block size that is the next power of 2 to allow for efficient hardware utilization\n    BLOCK_SIZE_H = triton.next_power_of_2(hidden_size)\n\n    grid = (batch_size, )\n    \n    _rmsnorm_h1536_kernel[grid](\n        hidden_states_ptr=hidden_states_gpu,\n        weight_ptr=weight_gpu,\n        output_ptr=output_gpu,\n        stride_hidden_states_batch=hidden_states_gpu.stride(0),\n        stride_output_batch=output_gpu.stride(0),\n        HIDDEN_SIZE=hidden_size,\n        EPS=1e-6,\n        BLOCK_SIZE_H=BLOCK_SIZE_H,\n    )\n    \n    # --- Move Result to Original Device ---\n    return output_gpu.to(original_device)"
    }
  ],
  "description": "gemini-2.5-pro optimized kernel for rmsnorm_h1536 (round 1)"
}