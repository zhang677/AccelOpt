{
  "name": "claude-opus-4-1_triton_91c9a3",
  "definition": "rmsnorm_h7168",
  "author": "claude-opus-4-1-20250805",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "B200"
    ],
    "entry_point": "main.py::run",
    "dependencies": []
  },
  "sources": [
    {
      "path": "main.py",
      "content": "import torch\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef rmsnorm_kernel(\n    hidden_states_ptr,\n    weight_ptr,\n    output_ptr,\n    hidden_size,\n    eps,\n    BLOCK_SIZE: tl.constexpr,\n):\n    # Get the batch index\n    batch_idx = tl.program_id(0)\n    \n    # Initialize accumulator for variance calculation\n    acc = tl.zeros([1], dtype=tl.float32)\n    \n    # Compute variance in multiple passes if needed\n    num_iters = tl.cdiv(hidden_size, BLOCK_SIZE)\n    \n    for iter in range(num_iters):\n        # Load block of hidden states\n        offsets = iter * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < hidden_size\n        \n        hidden_states_offset = batch_idx * hidden_size + offsets\n        x = tl.load(hidden_states_ptr + hidden_states_offset, mask=mask, other=0.0).to(tl.float32)\n        \n        # Accumulate squared values\n        acc += tl.sum(x * x, axis=0)\n    \n    # Compute inverse RMS\n    mean_sq = acc / hidden_size\n    inv_rms = tl.rsqrt(mean_sq + eps)\n    \n    # Apply normalization and weight in a second pass\n    for iter in range(num_iters):\n        offsets = iter * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < hidden_size\n        \n        hidden_states_offset = batch_idx * hidden_size + offsets\n        x = tl.load(hidden_states_ptr + hidden_states_offset, mask=mask, other=0.0).to(tl.float32)\n        \n        # Load weights\n        w = tl.load(weight_ptr + offsets, mask=mask, other=0.0).to(tl.float32)\n        \n        # Apply RMSNorm\n        y = (x * inv_rms) * w\n        \n        # Store output\n        output_offset = batch_idx * hidden_size + offsets\n        tl.store(output_ptr + output_offset, y.to(tl.bfloat16), mask=mask)\n\n\n@triton.jit\ndef rmsnorm_kernel_optimized(\n    hidden_states_ptr,\n    weight_ptr,\n    output_ptr,\n    hidden_size,\n    eps,\n    BLOCK_SIZE: tl.constexpr,\n):\n    # Get the batch index\n    batch_idx = tl.program_id(0)\n    \n    # For B200, we can use larger block sizes and more efficient memory access\n    # Process the entire row in chunks\n    acc = 0.0\n    \n    # First pass: compute variance\n    for offset in range(0, hidden_size, BLOCK_SIZE):\n        cols = offset + tl.arange(0, BLOCK_SIZE)\n        mask = cols < hidden_size\n        \n        hidden_states_offset = batch_idx * hidden_size + cols\n        x = tl.load(hidden_states_ptr + hidden_states_offset, mask=mask, other=0.0).to(tl.float32)\n        \n        # Accumulate squared values\n        acc += tl.sum(x * x, axis=0)\n    \n    # Compute inverse RMS\n    mean_sq = acc / hidden_size\n    inv_rms = tl.rsqrt(mean_sq + eps)\n    \n    # Second pass: apply normalization and weight\n    for offset in range(0, hidden_size, BLOCK_SIZE):\n        cols = offset + tl.arange(0, BLOCK_SIZE)\n        mask = cols < hidden_size\n        \n        hidden_states_offset = batch_idx * hidden_size + cols\n        x = tl.load(hidden_states_ptr + hidden_states_offset, mask=mask, other=0.0).to(tl.float32)\n        \n        # Load weights\n        w = tl.load(weight_ptr + cols, mask=mask, other=0.0).to(tl.float32)\n        \n        # Apply RMSNorm\n        y = (x * inv_rms) * w\n        \n        # Store output\n        output_offset = batch_idx * hidden_size + cols\n        tl.store(output_ptr + output_offset, y.to(tl.bfloat16), mask=mask)\n\n\ndef run(hidden_states, weight):\n    # Handle device management\n    original_device = hidden_states.device\n    \n    if not torch.cuda.is_available() and (hidden_states.is_cuda or weight.is_cuda):\n        raise RuntimeError(\"CUDA is not available but GPU tensors were provided\")\n    \n    # Move tensors to GPU if needed\n    if torch.cuda.is_available():\n        if not hidden_states.is_cuda:\n            hidden_states = hidden_states.cuda()\n        if not weight.is_cuda:\n            weight = weight.cuda()\n    else:\n        raise RuntimeError(\"CUDA is required for Triton kernels\")\n    \n    batch_size, hidden_size = hidden_states.shape\n    \n    # Check constants\n    assert hidden_size == 7168, f\"hidden_size must be 7168, got {hidden_size}\"\n    \n    # Allocate output tensor\n    output = torch.empty_like(hidden_states, device=hidden_states.device)\n    \n    # Constants\n    EPS = 1e-6\n    BLOCK_SIZE = 1024  # Optimized for B200 GPU\n    \n    # Launch kernel\n    grid = (batch_size,)\n    \n    # Use optimized kernel for B200\n    rmsnorm_kernel_optimized[grid](\n        hidden_states,\n        weight,\n        output,\n        hidden_size,\n        EPS,\n        BLOCK_SIZE=BLOCK_SIZE,\n    )\n    \n    # Move result back to original device if needed\n    if original_device != output.device:\n        output = output.to(original_device)\n    \n    return output"
    }
  ],
  "description": "claude-opus-4-1-20250805 optimized kernel for rmsnorm_h7168 (round 1)"
}