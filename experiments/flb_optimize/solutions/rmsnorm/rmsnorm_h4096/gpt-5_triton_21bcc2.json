{
  "name": "gpt-5_triton_21bcc2",
  "definition": "rmsnorm_h4096",
  "author": "gpt-5-2025-08-07",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "B200"
    ],
    "entry_point": "main.py::run",
    "dependencies": []
  },
  "sources": [
    {
      "path": "main.py",
      "content": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _rmsnorm_h4096_kernel(\n    X_ptr, W_ptr, Y_ptr,\n    stride_x_row, stride_x_col,\n    stride_w,\n    stride_y_row, stride_y_col,\n    N: tl.constexpr,\n    EPSILON: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    x_row = X_ptr + pid * stride_x_row\n    y_row = Y_ptr + pid * stride_y_row\n\n    tl.static_assert(N == 4096)\n    tl.static_assert(BLOCK_SIZE > 0)\n    tl.static_assert(BLOCK_SIZE % 128 == 0)\n\n    sum_sq = tl.zeros((), dtype=tl.float32)\n\n    # First pass: compute sum of squares\n    for offs in range(0, N, BLOCK_SIZE):\n        idx = offs + tl.arange(0, BLOCK_SIZE)\n        mask = idx < N\n        x = tl.load(x_row + idx * stride_x_col, mask=mask, other=0).to(tl.float32)\n        sum_sq += tl.sum(x * x, axis=0)\n\n    mean = sum_sq / N\n    inv_rms = tl.rsqrt(mean + EPSILON)\n\n    # Second pass: normalize and scale\n    for offs in range(0, N, BLOCK_SIZE):\n        idx = offs + tl.arange(0, BLOCK_SIZE)\n        mask = idx < N\n        x = tl.load(x_row + idx * stride_x_col, mask=mask, other=0).to(tl.float32)\n        w = tl.load(W_ptr + idx * stride_w, mask=mask, other=1).to(tl.float32)\n        y = (x * inv_rms) * w\n        tl.store(y_row + idx * stride_y_col, y.to(tl.bfloat16), mask=mask)\n\n\ndef run(hidden_states, weight):\n    if hidden_states is None or weight is None:\n        raise ValueError(\"hidden_states and weight must be provided\")\n\n    if hidden_states.ndim != 2:\n        raise ValueError(f\"hidden_states must be 2D [batch, hidden], got shape {tuple(hidden_states.shape)}\")\n    if weight.ndim != 1:\n        raise ValueError(f\"weight must be 1D [hidden], got shape {tuple(weight.shape)}\")\n\n    batch_size, hidden_size = hidden_states.shape\n    if hidden_size != 4096:\n        raise AssertionError(f\"hidden_size must be 4096, got {hidden_size}\")\n    if weight.shape[0] != hidden_size:\n        raise ValueError(f\"weight shape mismatch: expected {hidden_size}, got {weight.shape[0]}\")\n\n    # Enforce dtype\n    if hidden_states.dtype != torch.bfloat16 or weight.dtype != torch.bfloat16:\n        raise TypeError(\"hidden_states and weight must be torch.bfloat16\")\n\n    # Device management\n    if not torch.cuda.is_available():\n        raise RuntimeError(\"CUDA is required to run this Triton kernel but torch.cuda.is_available() is False.\")\n\n    # Select target CUDA device\n    target_device = None\n    if hidden_states.is_cuda:\n        target_device = hidden_states.device\n    elif weight.is_cuda:\n        target_device = weight.device\n    else:\n        target_device = torch.device('cuda')\n\n    # Move inputs to device as needed\n    hs_in_dev = hidden_states.to(target_device, non_blocking=True)\n    w_in_dev = weight.to(target_device, non_blocking=True)\n\n    # Strides in elements (PyTorch gives element strides)\n    stride_x_row = hs_in_dev.stride(0)\n    stride_x_col = hs_in_dev.stride(1)\n    stride_w = w_in_dev.stride(0)\n    stride_y_row = stride_x_row\n    stride_y_col = stride_x_col\n\n    # Allocate output on device\n    y_dev = torch.empty_like(hs_in_dev, device=target_device, dtype=torch.bfloat16)\n\n    # Launch configuration tuned for 4096 hidden size on B200\n    BLOCK_SIZE = 1024  # process 1024 elements per iteration, 4 iterations total\n    num_warps = 8\n    num_stages = 4\n    EPSILON = 1e-5\n\n    grid = (batch_size,)\n\n    _rmsnorm_h4096_kernel[grid](\n        hs_in_dev, w_in_dev, y_dev,\n        stride_x_row, stride_x_col,\n        stride_w,\n        stride_y_row, stride_y_col,\n        N=hidden_size,\n        EPSILON=EPSILON,\n        BLOCK_SIZE=BLOCK_SIZE,\n        num_warps=num_warps,\n        num_stages=num_stages,\n    )\n\n    # Move back to the original device of hidden_states\n    if hidden_states.device != target_device:\n        y_out = y_dev.to(hidden_states.device, non_blocking=True)\n    else:\n        y_out = y_dev\n\n    return y_out"
    }
  ],
  "description": "gpt-5-2025-08-07 optimized kernel for rmsnorm_h4096 (round 1, reasoning effort: high)"
}