{
  "name": "gemini-2.5-pro_triton_dc28mj",
  "definition": "fused_add_rmsnorm_h4096",
  "author": "gemini-2.5-pro",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "B200"
    ],
    "entry_point": "main.py::run",
    "dependencies": []
  },
  "sources": [
    {
      "path": "main.py",
      "content": "import torch\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_H': 1024}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_H': 2048}, num_warps=8),\n        triton.Config({'BLOCK_SIZE_H': 4096}, num_warps=16),\n    ],\n    key=['HIDDEN_SIZE'],\n)\n@triton.jit\ndef _fused_add_rmsnorm_h4096_kernel(\n    # Pointers to tensors\n    hidden_states_ptr,\n    residual_ptr,\n    weight_ptr,\n    output_ptr,\n    # Other parameters\n    batch_size,\n    # Constants\n    HIDDEN_SIZE: tl.constexpr,\n    EPS: tl.constexpr,\n    BLOCK_SIZE_H: tl.constexpr,\n):\n    \"\"\"\n    Triton kernel for fused add and RMSNorm.\n    Each program instance processes a single row of the input tensors.\n    \"\"\"\n    # Get the row index for the current program\n    row_idx = tl.program_id(0)\n\n    # --- Pointers for the current row ---\n    # Note: This assumes inputs are contiguous, which is a common case.\n    row_hidden_states_ptr = hidden_states_ptr + row_idx * HIDDEN_SIZE\n    row_residual_ptr = residual_ptr + row_idx * HIDDEN_SIZE\n    row_output_ptr = output_ptr + row_idx * HIDDEN_SIZE\n    # Weight pointer is the same for all rows\n    weight_base_ptr = weight_ptr\n\n    # --- Pass 1: Calculate the sum of squares for RMSNorm ---\n    # Accumulator for the variance, initialized to zero.\n    # We use float32 for precision.\n    var_accumulator = tl.zeros([1], dtype=tl.float32)\n\n    # Iterate over the hidden dimension in blocks of size BLOCK_SIZE_H\n    for h_offset in range(0, HIDDEN_SIZE, BLOCK_SIZE_H):\n        # Create a vector of offsets for the current block\n        h_offsets = h_offset + tl.arange(0, BLOCK_SIZE_H)\n        # Create a mask to handle the last block if HIDDEN_SIZE is not a multiple of BLOCK_SIZE_H\n        mask = h_offsets < HIDDEN_SIZE\n\n        # Load one block of hidden_states and residual\n        hidden_states_block = tl.load(row_hidden_states_ptr + h_offsets, mask=mask, other=0.0)\n        residual_block = tl.load(row_residual_ptr + h_offsets, mask=mask, other=0.0)\n\n        # Perform the addition: x = hidden_states + residual\n        # Cast to float32 for high-precision computation\n        x = hidden_states_block.to(tl.float32) + residual_block.to(tl.float32)\n\n        # Accumulate the sum of squares of x\n        var_accumulator += tl.sum(x * x, axis=0)\n\n    # After iterating through all blocks, compute the mean and inverse RMS\n    mean_var = var_accumulator / HIDDEN_SIZE\n    inv_rms = tl.rsqrt(mean_var + EPS)\n\n    # --- Pass 2: Apply normalization and scaling, and store the result ---\n    # Re-iterate over the hidden dimension to apply the calculated inv_rms\n    for h_offset in range(0, HIDDEN_SIZE, BLOCK_SIZE_H):\n        h_offsets = h_offset + tl.arange(0, BLOCK_SIZE_H)\n        mask = h_offsets < HIDDEN_SIZE\n\n        # Reload the input blocks for this pass\n        hidden_states_block = tl.load(row_hidden_states_ptr + h_offsets, mask=mask, other=0.0)\n        residual_block = tl.load(row_residual_ptr + h_offsets, mask=mask, other=0.0)\n\n        # Recompute x, same as in Pass 1\n        x = hidden_states_block.to(tl.float32) + residual_block.to(tl.float32)\n\n        # Load the corresponding block of weights\n        weight_block = tl.load(weight_base_ptr + h_offsets, mask=mask, other=0.0)\n\n        # Apply the normalization and scaling\n        normalized_x = x * inv_rms\n        output_block = normalized_x * weight_block.to(tl.float32)\n\n        # Convert the final result back to bfloat16 and store it in the output tensor\n        tl.store(row_output_ptr + h_offsets, output_block.to(tl.bfloat16), mask=mask)\n\n\ndef fused_add_rmsnorm_h4096(hidden_states: torch.Tensor, residual: torch.Tensor, weight: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Wrapper function for the fused_add_rmsnorm_h4096 Triton kernel.\n\n    Args:\n        hidden_states (torch.Tensor): Input tensor of shape [batch_size, 4096] and dtype bfloat16.\n        residual (torch.Tensor): Residual tensor of shape [batch_size, 4096] and dtype bfloat16.\n        weight (torch.Tensor): Weight tensor of shape [4096] and dtype bfloat16.\n\n    Returns:\n        torch.Tensor: The output tensor of shape [batch_size, 4096] and dtype bfloat16.\n    \"\"\"\n    # --- Device Management ---\n    if not torch.cuda.is_available():\n        raise RuntimeError(\"This Triton kernel requires a CUDA-enabled GPU.\")\n\n    original_device = hidden_states.device\n    is_cpu = original_device.type == 'cpu'\n\n    # Move tensors to GPU if they are on CPU\n    if is_cpu:\n        hidden_states = hidden_states.cuda()\n        residual = residual.cuda()\n        weight = weight.cuda()\n\n    # --- Input Validation ---\n    batch_size, hidden_size = hidden_states.shape\n    \n    if hidden_size != 4096:\n        raise ValueError(f\"hidden_size must be 4096, but got {hidden_size}\")\n    if hidden_states.shape != residual.shape:\n        raise ValueError(\"hidden_states and residual must have the same shape.\")\n    if weight.shape != (hidden_size,):\n        raise ValueError(f\"weight must have shape [{hidden_size}], but got {weight.shape}\")\n    \n    expected_dtype = torch.bfloat16\n    if hidden_states.dtype != expected_dtype or residual.dtype != expected_dtype or weight.dtype != expected_dtype:\n        raise TypeError(f\"All input tensors must have dtype {expected_dtype}.\")\n\n    # --- Kernel Launch ---\n    # Allocate output tensor on the same device as the inputs\n    output = torch.empty_like(hidden_states)\n\n    # The grid is 1D, with one program instance per row in the batch\n    grid = (batch_size,)\n\n    _fused_add_rmsnorm_h4096_kernel[grid](\n        hidden_states,\n        residual,\n        weight,\n        output,\n        batch_size,\n        HIDDEN_SIZE=hidden_size,\n        EPS=1e-5,\n    )\n\n    # --- Finalization ---\n    # Move output back to the original device if necessary\n    if is_cpu:\n        output = output.cpu()\n\n    return output\n\n\ndef run(*args, **kwargs):\n    \"\"\"\n    Public entry point for the kernel. Handles flexible argument passing.\n    This function allows calling with either positional or keyword arguments.\n    \"\"\"\n    # A robust way to handle both *args and **kwargs by forwarding them\n    # to the main function.\n    if args:\n        return fused_add_rmsnorm_h4096(*args, **kwargs)\n    else:\n        return fused_add_rmsnorm_h4096(**kwargs)\n"
    }
  ],
  "description": "gemini-2.5-pro optimized kernel for fused_add_rmsnorm_h4096 (round 1)"
}