{
  "name": "gpt-o3_triton_35b90e",
  "definition": "rmsnorm_h128",
  "author": "gpt-o3",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "B200"
    ],
    "entry_point": "main.py::run",
    "dependencies": []
  },
  "sources": [
    {
      "path": "main.py",
      "content": "import math\nimport torch\nimport triton\nimport triton.language as tl\n\n# -----------------------------------------------------------------------------#\n# Constants\n# -----------------------------------------------------------------------------#\nEPS: float = 1e-6          # numerical stability\nHIDDEN_SIZE: int = 128     # problem-specific constant\n\n# -----------------------------------------------------------------------------#\n# Triton Kernel\n# -----------------------------------------------------------------------------#\n@triton.jit\ndef _rmsnorm_kernel(\n    x_ptr,                      # [batch, hidden]  (BF16)\n    w_ptr,                      # [hidden]         (BF16)\n    o_ptr,                      # [batch, hidden]  (BF16)\n    stride_x,                   # leading dimension of x\n    stride_o,                   # leading dimension of o\n    eps: tl.constexpr,          # epsilon\n    hidden: tl.constexpr        # hidden size (128)\n):\n    pid = tl.program_id(axis=0)                         # one program = one row\n    offs = tl.arange(0, hidden)                         # [0 .. 127]\n    mask = offs < hidden                                # always true, kept for safety\n\n    # -------------------------------------------------------------------------#\n    # Load input row and weight vector\n    # -------------------------------------------------------------------------#\n    x_row_ptr = x_ptr + pid * stride_x + offs\n    w_ptrs    = w_ptr + offs\n    x_bf16    = tl.load(x_row_ptr, mask=mask, other=0.0)\n    w_bf16    = tl.load(w_ptrs,    mask=mask, other=0.0)\n\n    x_f32 = x_bf16.to(tl.float32)\n    w_f32 = w_bf16.to(tl.float32)\n\n    # -------------------------------------------------------------------------#\n    # RMS computation\n    # -------------------------------------------------------------------------#\n    rsq   = x_f32 * x_f32\n    mean  = tl.sum(rsq) / hidden\n    inv_r = tl.rsqrt(mean + eps)\n\n    # -------------------------------------------------------------------------#\n    # Final output:  y = (x * inv_rms) * weight\n    # -------------------------------------------------------------------------#\n    y_f32 = (x_f32 * inv_r) * w_f32\n    y_bf16 = y_f32.to(tl.bfloat16)\n\n    # -------------------------------------------------------------------------#\n    # Store\n    # -------------------------------------------------------------------------#\n    o_row_ptr = o_ptr + pid * stride_o + offs\n    tl.store(o_row_ptr, y_bf16, mask=mask)\n\n\n# -----------------------------------------------------------------------------#\n# Python Wrapper\n# -----------------------------------------------------------------------------#\ndef run(*args, **kwargs):\n    \"\"\"\n    Entry point.\n\n    Parameters (positional or keyword):\n      hidden_states: Tensor[batch, 128] (bfloat16)\n      weight:        Tensor[128]        (bfloat16)\n\n    Returns:\n      output Tensor with same shape/dtype/device as `hidden_states`\n    \"\"\"\n    # -------------------------------------------------------------------------#\n    # Argument extraction\n    # -------------------------------------------------------------------------#\n    if len(args) + len(kwargs) < 2:\n        raise TypeError(\"run() missing required arguments: 'hidden_states' and 'weight'\")\n\n    hidden_states = kwargs.pop('hidden_states') if 'hidden_states' in kwargs else args[0]\n    weight        = kwargs.pop('weight')        if 'weight'        in kwargs else args[1] if len(args) > 1 else None\n    if weight is None:\n        raise TypeError(\"run() missing required argument: 'weight'\")\n    if kwargs:\n        raise TypeError(f\"run() got unexpected keyword arguments {list(kwargs.keys())}\")\n\n    # -------------------------------------------------------------------------#\n    # Shape / dtype checks\n    # -------------------------------------------------------------------------#\n    if hidden_states.ndim != 2:\n        raise ValueError(\"hidden_states must be 2-D [batch, hidden]\")\n    batch, hidden = hidden_states.shape\n    if hidden != HIDDEN_SIZE:\n        raise ValueError(f\"hidden dimension must be {HIDDEN_SIZE}\")\n    if weight.shape != (HIDDEN_SIZE,):\n        raise ValueError(f\"weight shape must be ({HIDDEN_SIZE},)\")\n\n    # -------------------------------------------------------------------------#\n    # Device handling\n    # -------------------------------------------------------------------------#\n    if not torch.cuda.is_available():\n        if hidden_states.is_cuda or weight.is_cuda:\n            raise RuntimeError(\"CUDA tensors provided but CUDA is not available\")\n        # CPU fallback (reference implementation)\n        x = hidden_states.to(torch.float32)\n        inv_rms = torch.rsqrt(x.pow(2).mean(dim=-1, keepdim=True) + EPS)\n        y = (x * inv_rms) * weight.to(torch.float32)\n        return y.to(hidden_states.dtype)\n\n    orig_device = hidden_states.device\n    x_gpu = hidden_states if hidden_states.is_cuda else hidden_states.cuda()\n    w_gpu = weight        if weight.is_cuda        else weight.cuda()\n\n    # Ensure contiguous layout for predictable strides\n    x_gpu = x_gpu.contiguous()\n    w_gpu = w_gpu.contiguous()\n\n    # Allocate output\n    o_gpu = torch.empty_like(x_gpu)\n\n    # -------------------------------------------------------------------------#\n    # Kernel launch\n    # -------------------------------------------------------------------------#\n    grid = (batch,)\n    _rmsnorm_kernel[grid](\n        x_gpu, w_gpu, o_gpu,\n        x_gpu.stride(0), o_gpu.stride(0),\n        EPS, HIDDEN_SIZE,\n        num_warps=4\n    )\n\n    # -------------------------------------------------------------------------#\n    # Move back to original device if necessary\n    # -------------------------------------------------------------------------#\n    if orig_device.type == 'cpu':\n        return o_gpu.cpu()\n    return o_gpu\n\n\n# -----------------------------------------------------------------------------#\n# This file exposes a single callable `run` for external use\n# -----------------------------------------------------------------------------#\n__all__ = [\"run\"]"
    }
  ],
  "description": "o3 optimized kernel for rmsnorm_h128 (round 1, reasoning effort: high)"
}