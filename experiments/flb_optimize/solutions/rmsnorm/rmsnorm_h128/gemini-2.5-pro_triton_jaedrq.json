{
  "name": "gemini-2.5-pro_triton_jaedrq",
  "definition": "rmsnorm_h128",
  "author": "gemini-2.5-pro",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "B200"
    ],
    "entry_point": "main.py::run",
    "dependencies": []
  },
  "sources": [
    {
      "path": "main.py",
      "content": "import torch\nimport triton\nimport triton.language as tl\nimport math\n\n#\n# Reference Implementation:\n# import torch\n#\n# @torch.no_grad()\n# def run(hidden_states, weight):\n#     batch_size, hidden_size = hidden_states.shape\n#     # Check constants\n#     assert hidden_size == 128\n#\n#     EPS = 1e-6\n#\n#     x = hidden_states.to(torch.float32)\n#     inv_rms = torch.rsqrt(x.pow(2).mean(dim=-1, keepdim=True) + EPS)\n#     y = (x * inv_rms) * weight.to(torch.float32)\n#     return y.to(hidden_states.dtype)\n#\n\n@triton.jit\ndef rmsnorm_h128_kernel(\n    hidden_states_ptr,\n    weight_ptr,\n    output_ptr,\n    batch_size,\n    hidden_size,\n    stride_b,\n    EPS: tl.constexpr,\n    BLOCK_SIZE_H: tl.constexpr,\n):\n    \"\"\"\n    Triton kernel for RMS Normalization with a fixed hidden_size of 128.\n    Each program in the grid processes one row from the batch.\n    \"\"\"\n    # Get the row index for the current program\n    pid_b = tl.program_id(axis=0)\n\n    # Create pointers to the start of the current row for inputs and output\n    row_x_ptr = hidden_states_ptr + pid_b * stride_b\n    row_y_ptr = output_ptr + pid_b * stride_b\n\n    # Create a range of offsets for the hidden dimension\n    # Since BLOCK_SIZE_H is fixed to hidden_size (128), we load the entire row\n    offsets_h = tl.arange(0, BLOCK_SIZE_H)\n    \n    # Load the full row of hidden_states and the full weight vector\n    # No mask is needed as hidden_size == BLOCK_SIZE_H\n    x = tl.load(row_x_ptr + offsets_h)\n    w = tl.load(weight_ptr + offsets_h)\n\n    # --- Computation is performed in float32 for precision ---\n    x_fp32 = x.to(tl.float32)\n    w_fp32 = w.to(tl.float32)\n\n    # 1. Square the elements\n    x_sq = x_fp32 * x_fp32\n\n    # 2. Compute the mean of the squares (reduction)\n    # tl.sum performs an efficient reduction over the block of 128 elements\n    var = tl.sum(x_sq, axis=0) / hidden_size\n\n    # 3. Compute the inverse root mean square\n    inv_rms = tl.rsqrt(var + EPS)\n\n    # 4. Normalize the hidden states and apply the learned scaling factor (weight)\n    y = x_fp32 * inv_rms * w_fp32\n\n    # --- Cast back to bfloat16 and store the result ---\n    y_bf16 = y.to(tl.bfloat16)\n    tl.store(row_y_ptr + offsets_h, y_bf16)\n\n\ndef run(*args, **kwargs):\n    \"\"\"\n    Wrapper function to run the RMSNorm Triton kernel.\n\n    Handles device management, tensor validation, and kernel launching.\n    It preserves the device of the input tensors for the output.\n\n    Args:\n        hidden_states (torch.Tensor): Input tensor of shape [batch_size, 128]\n                                      and dtype bfloat16.\n        weight (torch.Tensor): Weight tensor of shape [128] and dtype bfloat16.\n    \n    Returns:\n        torch.Tensor: The normalized output tensor of the same shape and dtype\n                      as hidden_states.\n    \"\"\"\n    # 1. Parse arguments\n    if args:\n        if len(args) != 2:\n            raise ValueError(f\"Expected 2 positional arguments, but got {len(args)}\")\n        hidden_states, weight = args\n    else:\n        hidden_states = kwargs.get('hidden_states')\n        weight = kwargs.get('weight')\n        if hidden_states is None or weight is None:\n            raise ValueError(\"Missing required keyword arguments: 'hidden_states' and/or 'weight'\")\n\n    # 2. Validate tensor properties\n    if hidden_states.dim() != 2 or hidden_states.shape[1] != 128:\n        raise ValueError(f\"Expected hidden_states to have shape [batch_size, 128], but got {hidden_states.shape}\")\n    if weight.dim() != 1 or weight.shape[0] != 128:\n        raise ValueError(f\"Expected weight to have shape [128], but got {weight.shape}\")\n    if hidden_states.dtype != torch.bfloat16:\n        raise TypeError(f\"Expected hidden_states to have dtype torch.bfloat16, but got {hidden_states.dtype}\")\n    if weight.dtype != torch.bfloat16:\n        raise TypeError(f\"Expected weight to have dtype torch.bfloat16, but got {weight.dtype}\")\n\n    # 3. Device management\n    original_device = hidden_states.device\n    is_cpu_input = original_device.type == 'cpu'\n    \n    # If inputs are on CPU, they must be moved to a GPU to run the Triton kernel.\n    if is_cpu_input:\n        if not torch.cuda.is_available():\n            raise RuntimeError(\"CUDA is not available, but input tensors are on CPU.\")\n        target_device = 'cuda'\n        hidden_states = hidden_states.to(target_device)\n        weight = weight.to(target_device)\n    # If inputs are already on a GPU, ensure they are on the same device.\n    else:\n        if hidden_states.device != weight.device:\n            raise ValueError(f\"hidden_states and weight must be on the same device, \"\n                             f\"but got {hidden_states.device} and {weight.device}\")\n        target_device = hidden_states.device\n\n    # 4. Prepare for kernel launch\n    batch_size, hidden_size = hidden_states.shape\n    \n    # Allocate the output tensor on the target GPU device\n    output = torch.empty_like(hidden_states)\n\n    # The grid is 1D, with one program per row in the batch.\n    grid = (batch_size,)\n\n    # Constants for the kernel\n    EPS = 1e-6\n    BLOCK_SIZE_H = 128\n\n    # 5. Launch the Triton kernel\n    # num_warps=4 is a robust choice for a block size of 128 on modern GPUs like B200.\n    rmsnorm_h128_kernel[grid](\n        hidden_states,\n        weight,\n        output,\n        batch_size,\n        hidden_size,\n        hidden_states.stride(0),\n        EPS=EPS,\n        BLOCK_SIZE_H=BLOCK_SIZE_H,\n        num_warps=4,\n    )\n\n    # 6. Move the result back to the original device if necessary\n    if is_cpu_input:\n        output = output.to(original_device)\n\n    return output"
    }
  ],
  "description": "gemini-2.5-pro optimized kernel for rmsnorm_h128 (round 1)"
}