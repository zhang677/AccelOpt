{
  "name": "gpt-5_triton_7308c5",
  "definition": "gqa_ragged_prefill_causal_h32_kv8_d128",
  "author": "gpt-5-2025-08-07",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "B200"
    ],
    "entry_point": "main.py::run",
    "dependencies": []
  },
  "sources": [
    {
      "path": "main.py",
      "content": "import math\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef gqa_ragged_prefill_causal_h32_kv8_d128_kernel(\n    q_ptr, k_ptr, v_ptr,\n    stride_q_q, stride_q_h, stride_q_d,\n    stride_k_k, stride_k_h, stride_k_d,\n    stride_v_k, stride_v_h, stride_v_d,\n    out_ptr, stride_out_q, stride_out_h, stride_out_d,\n    lse_ptr, stride_lse_q, stride_lse_h,\n    q_kv_start_ptr, q_kv_max_ptr,\n    total_q,\n    sm_scale, ln2,\n    RATIO: tl.constexpr, HEAD_DIM: tl.constexpr,\n    BLOCK_N: tl.constexpr, BLOCK_DK: tl.constexpr, BLOCK_DV: tl.constexpr,\n):\n    pid_q = tl.program_id(0)\n    kvh = tl.program_id(1)\n    if pid_q >= total_q:\n        return\n\n    kv_start = tl.load(q_kv_start_ptr + pid_q, mask=True, other=0).to(tl.int32)\n    kv_max = tl.load(q_kv_max_ptr + pid_q, mask=True, other=0).to(tl.int32)\n\n    heads_base = kvh * RATIO\n    neg_inf = tl.full([], -float(\"inf\"), tl.float32)\n\n    if kv_max <= 0:\n        # No available keys for this query; set LSE to -inf and outputs to 0\n        for r in range(RATIO):\n            lse_ptr_r = lse_ptr + pid_q * stride_lse_q + (heads_base + r) * stride_lse_h\n            tl.store(lse_ptr_r, neg_inf)\n            # store output zeros\n            for dv0 in range(0, HEAD_DIM, BLOCK_DV):\n                d_voffs = dv0 + tl.arange(0, BLOCK_DV)\n                out_ptrs = out_ptr + pid_q * stride_out_q + (heads_base + r) * stride_out_h + d_voffs * stride_out_d\n                tl.store(out_ptrs, tl.zeros([BLOCK_DV], dtype=tl.bfloat16))\n        return\n\n    # Initialize streaming softmax stats per head (RATIO=4)\n    m0 = neg_inf\n    m1 = neg_inf\n    m2 = neg_inf\n    m3 = neg_inf\n    l0 = tl.zeros([], dtype=tl.float32)\n    l1 = tl.zeros([], dtype=tl.float32)\n    l2 = tl.zeros([], dtype=tl.float32)\n    l3 = tl.zeros([], dtype=tl.float32)\n\n    # Output accumulators per head, split along D into 4 segments (HEAD_DIM=128, BLOCK_DV=32)\n    o0_s0 = tl.zeros([BLOCK_DV], dtype=tl.float32)\n    o0_s1 = tl.zeros([BLOCK_DV], dtype=tl.float32)\n    o0_s2 = tl.zeros([BLOCK_DV], dtype=tl.float32)\n    o0_s3 = tl.zeros([BLOCK_DV], dtype=tl.float32)\n\n    o1_s0 = tl.zeros([BLOCK_DV], dtype=tl.float32)\n    o1_s1 = tl.zeros([BLOCK_DV], dtype=tl.float32)\n    o1_s2 = tl.zeros([BLOCK_DV], dtype=tl.float32)\n    o1_s3 = tl.zeros([BLOCK_DV], dtype=tl.float32)\n\n    o2_s0 = tl.zeros([BLOCK_DV], dtype=tl.float32)\n    o2_s1 = tl.zeros([BLOCK_DV], dtype=tl.float32)\n    o2_s2 = tl.zeros([BLOCK_DV], dtype=tl.float32)\n    o2_s3 = tl.zeros([BLOCK_DV], dtype=tl.float32)\n\n    o3_s0 = tl.zeros([BLOCK_DV], dtype=tl.float32)\n    o3_s1 = tl.zeros([BLOCK_DV], dtype=tl.float32)\n    o3_s2 = tl.zeros([BLOCK_DV], dtype=tl.float32)\n    o3_s3 = tl.zeros([BLOCK_DV], dtype=tl.float32)\n\n    # Loop over key tiles\n    for start_n in range(0, kv_max, BLOCK_N):\n        key_offsets = start_n + tl.arange(0, BLOCK_N)\n        key_mask = key_offsets < kv_max\n\n        # Accumulate logits per head for this tile\n        logits0 = tl.zeros([BLOCK_N], dtype=tl.float32)\n        logits1 = tl.zeros([BLOCK_N], dtype=tl.float32)\n        logits2 = tl.zeros([BLOCK_N], dtype=tl.float32)\n        logits3 = tl.zeros([BLOCK_N], dtype=tl.float32)\n\n        for d0 in range(0, HEAD_DIM, BLOCK_DK):\n            d_off = d0 + tl.arange(0, BLOCK_DK)\n\n            # Load K chunk: [BLOCK_N, BLOCK_DK] -> fp32\n            k_ptrs = k_ptr + (kv_start + key_offsets)[:, None] * stride_k_k + kvh * stride_k_h + d_off[None, :] * stride_k_d\n            k_chunk = tl.load(\n                k_ptrs,\n                mask=key_mask[:, None] & (d_off[None, :] < HEAD_DIM),\n                other=0\n            ).to(tl.float32)\n\n            # Load Q chunk and accumulate logits for each of the 4 query heads in this kv head group\n            # Head 0\n            q_ptrs0 = q_ptr + pid_q * stride_q_q + (heads_base + 0) * stride_q_h + d_off * stride_q_d\n            q_vec0 = tl.load(q_ptrs0, mask=d_off < HEAD_DIM, other=0).to(tl.float32)\n            logits0 += tl.sum(k_chunk * q_vec0[None, :], axis=1)\n\n            # Head 1\n            q_ptrs1 = q_ptr + pid_q * stride_q_q + (heads_base + 1) * stride_q_h + d_off * stride_q_d\n            q_vec1 = tl.load(q_ptrs1, mask=d_off < HEAD_DIM, other=0).to(tl.float32)\n            logits1 += tl.sum(k_chunk * q_vec1[None, :], axis=1)\n\n            # Head 2\n            q_ptrs2 = q_ptr + pid_q * stride_q_q + (heads_base + 2) * stride_q_h + d_off * stride_q_d\n            q_vec2 = tl.load(q_ptrs2, mask=d_off < HEAD_DIM, other=0).to(tl.float32)\n            logits2 += tl.sum(k_chunk * q_vec2[None, :], axis=1)\n\n            # Head 3\n            q_ptrs3 = q_ptr + pid_q * stride_q_q + (heads_base + 3) * stride_q_h + d_off * stride_q_d\n            q_vec3 = tl.load(q_ptrs3, mask=d_off < HEAD_DIM, other=0).to(tl.float32)\n            logits3 += tl.sum(k_chunk * q_vec3[None, :], axis=1)\n\n        # Scale and apply mask\n        p0 = logits0 * sm_scale\n        p1 = logits1 * sm_scale\n        p2 = logits2 * sm_scale\n        p3 = logits3 * sm_scale\n\n        p0 = tl.where(key_mask, p0, neg_inf)\n        p1 = tl.where(key_mask, p1, neg_inf)\n        p2 = tl.where(key_mask, p2, neg_inf)\n        p3 = tl.where(key_mask, p3, neg_inf)\n\n        # Preload V chunks once for the tile (reused across heads)\n        d0_idx = 0 + tl.arange(0, BLOCK_DV)\n        d1_idx = BLOCK_DV + tl.arange(0, BLOCK_DV)\n        d2_idx = 2 * BLOCK_DV + tl.arange(0, BLOCK_DV)\n        d3_idx = 3 * BLOCK_DV + tl.arange(0, BLOCK_DV)\n\n        v_ptrs0 = v_ptr + (kv_start + key_offsets)[:, None] * stride_v_k + kvh * stride_v_h + d0_idx[None, :] * stride_v_d\n        v_ptrs1 = v_ptr + (kv_start + key_offsets)[:, None] * stride_v_k + kvh * stride_v_h + d1_idx[None, :] * stride_v_d\n        v_ptrs2 = v_ptr + (kv_start + key_offsets)[:, None] * stride_v_k + kvh * stride_v_h + d2_idx[None, :] * stride_v_d\n        v_ptrs3 = v_ptr + (kv_start + key_offsets)[:, None] * stride_v_k + kvh * stride_v_h + d3_idx[None, :] * stride_v_d\n\n        v_chunk0 = tl.load(v_ptrs0, mask=key_mask[:, None], other=0).to(tl.float32)\n        v_chunk1 = tl.load(v_ptrs1, mask=key_mask[:, None], other=0).to(tl.float32)\n        v_chunk2 = tl.load(v_ptrs2, mask=key_mask[:, None], other=0).to(tl.float32)\n        v_chunk3 = tl.load(v_ptrs3, mask=key_mask[:, None], other=0).to(tl.float32)\n\n        # Head 0\n        m0_tile = tl.max(p0, axis=0)\n        m0_new = tl.maximum(m0, m0_tile)\n        alpha0 = tl.exp(m0 - m0_new)\n        o0_s0 = o0_s0 * alpha0\n        o0_s1 = o0_s1 * alpha0\n        o0_s2 = o0_s2 * alpha0\n        o0_s3 = o0_s3 * alpha0\n        w0 = tl.exp(p0 - m0_new)\n        l0 = l0 * alpha0 + tl.sum(w0, axis=0)\n        o0_s0 = o0_s0 + tl.sum(v_chunk0 * w0[:, None], axis=0)\n        o0_s1 = o0_s1 + tl.sum(v_chunk1 * w0[:, None], axis=0)\n        o0_s2 = o0_s2 + tl.sum(v_chunk2 * w0[:, None], axis=0)\n        o0_s3 = o0_s3 + tl.sum(v_chunk3 * w0[:, None], axis=0)\n        m0 = m0_new\n\n        # Head 1\n        m1_tile = tl.max(p1, axis=0)\n        m1_new = tl.maximum(m1, m1_tile)\n        alpha1 = tl.exp(m1 - m1_new)\n        o1_s0 = o1_s0 * alpha1\n        o1_s1 = o1_s1 * alpha1\n        o1_s2 = o1_s2 * alpha1\n        o1_s3 = o1_s3 * alpha1\n        w1 = tl.exp(p1 - m1_new)\n        l1 = l1 * alpha1 + tl.sum(w1, axis=0)\n        o1_s0 = o1_s0 + tl.sum(v_chunk0 * w1[:, None], axis=0)\n        o1_s1 = o1_s1 + tl.sum(v_chunk1 * w1[:, None], axis=0)\n        o1_s2 = o1_s2 + tl.sum(v_chunk2 * w1[:, None], axis=0)\n        o1_s3 = o1_s3 + tl.sum(v_chunk3 * w1[:, None], axis=0)\n        m1 = m1_new\n\n        # Head 2\n        m2_tile = tl.max(p2, axis=0)\n        m2_new = tl.maximum(m2, m2_tile)\n        alpha2 = tl.exp(m2 - m2_new)\n        o2_s0 = o2_s0 * alpha2\n        o2_s1 = o2_s1 * alpha2\n        o2_s2 = o2_s2 * alpha2\n        o2_s3 = o2_s3 * alpha2\n        w2 = tl.exp(p2 - m2_new)\n        l2 = l2 * alpha2 + tl.sum(w2, axis=0)\n        o2_s0 = o2_s0 + tl.sum(v_chunk0 * w2[:, None], axis=0)\n        o2_s1 = o2_s1 + tl.sum(v_chunk1 * w2[:, None], axis=0)\n        o2_s2 = o2_s2 + tl.sum(v_chunk2 * w2[:, None], axis=0)\n        o2_s3 = o2_s3 + tl.sum(v_chunk3 * w2[:, None], axis=0)\n        m2 = m2_new\n\n        # Head 3\n        m3_tile = tl.max(p3, axis=0)\n        m3_new = tl.maximum(m3, m3_tile)\n        alpha3 = tl.exp(m3 - m3_new)\n        o3_s0 = o3_s0 * alpha3\n        o3_s1 = o3_s1 * alpha3\n        o3_s2 = o3_s2 * alpha3\n        o3_s3 = o3_s3 * alpha3\n        w3 = tl.exp(p3 - m3_new)\n        l3 = l3 * alpha3 + tl.sum(w3, axis=0)\n        o3_s0 = o3_s0 + tl.sum(v_chunk0 * w3[:, None], axis=0)\n        o3_s1 = o3_s1 + tl.sum(v_chunk1 * w3[:, None], axis=0)\n        o3_s2 = o3_s2 + tl.sum(v_chunk2 * w3[:, None], axis=0)\n        o3_s3 = o3_s3 + tl.sum(v_chunk3 * w3[:, None], axis=0)\n        m3 = m3_new\n\n    # Finalize: compute output = O / l, and lse = (m + log(l)) / ln2\n    d0 = 0 + tl.arange(0, BLOCK_DV)\n    d1 = BLOCK_DV + tl.arange(0, BLOCK_DV)\n    d2 = 2 * BLOCK_DV + tl.arange(0, BLOCK_DV)\n    d3 = 3 * BLOCK_DV + tl.arange(0, BLOCK_DV)\n\n    # Head 0\n    l0_pos = l0 > 0\n    lse0 = tl.where(l0_pos, (m0 + tl.log(l0)) / ln2, neg_inf)\n    lse_ptr0 = lse_ptr + pid_q * stride_lse_q + (heads_base + 0) * stride_lse_h\n    tl.store(lse_ptr0, lse0)\n    out_ptrs0 = out_ptr + pid_q * stride_out_q + (heads_base + 0) * stride_out_h\n    o0_s0_out = tl.where(l0_pos, o0_s0 / l0, tl.zeros([BLOCK_DV], dtype=tl.float32))\n    o0_s1_out = tl.where(l0_pos, o0_s1 / l0, tl.zeros([BLOCK_DV], dtype=tl.float32))\n    o0_s2_out = tl.where(l0_pos, o0_s2 / l0, tl.zeros([BLOCK_DV], dtype=tl.float32))\n    o0_s3_out = tl.where(l0_pos, o0_s3 / l0, tl.zeros([BLOCK_DV], dtype=tl.float32))\n    tl.store(out_ptrs0 + d0 * stride_out_d, o0_s0_out.to(tl.bfloat16))\n    tl.store(out_ptrs0 + d1 * stride_out_d, o0_s1_out.to(tl.bfloat16))\n    tl.store(out_ptrs0 + d2 * stride_out_d, o0_s2_out.to(tl.bfloat16))\n    tl.store(out_ptrs0 + d3 * stride_out_d, o0_s3_out.to(tl.bfloat16))\n\n    # Head 1\n    l1_pos = l1 > 0\n    lse1 = tl.where(l1_pos, (m1 + tl.log(l1)) / ln2, neg_inf)\n    lse_ptr1 = lse_ptr + pid_q * stride_lse_q + (heads_base + 1) * stride_lse_h\n    tl.store(lse_ptr1, lse1)\n    out_ptrs1 = out_ptr + pid_q * stride_out_q + (heads_base + 1) * stride_out_h\n    o1_s0_out = tl.where(l1_pos, o1_s0 / l1, tl.zeros([BLOCK_DV], dtype=tl.float32))\n    o1_s1_out = tl.where(l1_pos, o1_s1 / l1, tl.zeros([BLOCK_DV], dtype=tl.float32))\n    o1_s2_out = tl.where(l1_pos, o1_s2 / l1, tl.zeros([BLOCK_DV], dtype=tl.float32))\n    o1_s3_out = tl.where(l1_pos, o1_s3 / l1, tl.zeros([BLOCK_DV], dtype=tl.float32))\n    tl.store(out_ptrs1 + d0 * stride_out_d, o1_s0_out.to(tl.bfloat16))\n    tl.store(out_ptrs1 + d1 * stride_out_d, o1_s1_out.to(tl.bfloat16))\n    tl.store(out_ptrs1 + d2 * stride_out_d, o1_s2_out.to(tl.bfloat16))\n    tl.store(out_ptrs1 + d3 * stride_out_d, o1_s3_out.to(tl.bfloat16))\n\n    # Head 2\n    l2_pos = l2 > 0\n    lse2 = tl.where(l2_pos, (m2 + tl.log(l2)) / ln2, neg_inf)\n    lse_ptr2 = lse_ptr + pid_q * stride_lse_q + (heads_base + 2) * stride_lse_h\n    tl.store(lse_ptr2, lse2)\n    out_ptrs2 = out_ptr + pid_q * stride_out_q + (heads_base + 2) * stride_out_h\n    o2_s0_out = tl.where(l2_pos, o2_s0 / l2, tl.zeros([BLOCK_DV], dtype=tl.float32))\n    o2_s1_out = tl.where(l2_pos, o2_s1 / l2, tl.zeros([BLOCK_DV], dtype=tl.float32))\n    o2_s2_out = tl.where(l2_pos, o2_s2 / l2, tl.zeros([BLOCK_DV], dtype=tl.float32))\n    o2_s3_out = tl.where(l2_pos, o2_s3 / l2, tl.zeros([BLOCK_DV], dtype=tl.float32))\n    tl.store(out_ptrs2 + d0 * stride_out_d, o2_s0_out.to(tl.bfloat16))\n    tl.store(out_ptrs2 + d1 * stride_out_d, o2_s1_out.to(tl.bfloat16))\n    tl.store(out_ptrs2 + d2 * stride_out_d, o2_s2_out.to(tl.bfloat16))\n    tl.store(out_ptrs2 + d3 * stride_out_d, o2_s3_out.to(tl.bfloat16))\n\n    # Head 3\n    l3_pos = l3 > 0\n    lse3 = tl.where(l3_pos, (m3 + tl.log(l3)) / ln2, neg_inf)\n    lse_ptr3 = lse_ptr + pid_q * stride_lse_q + (heads_base + 3) * stride_lse_h\n    tl.store(lse_ptr3, lse3)\n    out_ptrs3 = out_ptr + pid_q * stride_out_q + (heads_base + 3) * stride_out_h\n    o3_s0_out = tl.where(l3_pos, o3_s0 / l3, tl.zeros([BLOCK_DV], dtype=tl.float32))\n    o3_s1_out = tl.where(l3_pos, o3_s1 / l3, tl.zeros([BLOCK_DV], dtype=tl.float32))\n    o3_s2_out = tl.where(l3_pos, o3_s2 / l3, tl.zeros([BLOCK_DV], dtype=tl.float32))\n    o3_s3_out = tl.where(l3_pos, o3_s3 / l3, tl.zeros([BLOCK_DV], dtype=tl.float32))\n    tl.store(out_ptrs3 + d0 * stride_out_d, o3_s0_out.to(tl.bfloat16))\n    tl.store(out_ptrs3 + d1 * stride_out_d, o3_s1_out.to(tl.bfloat16))\n    tl.store(out_ptrs3 + d2 * stride_out_d, o3_s2_out.to(tl.bfloat16))\n    tl.store(out_ptrs3 + d3 * stride_out_d, o3_s3_out.to(tl.bfloat16))\n\n\ndef _prepare_q_meta_from_indptr(qo_indptr: torch.Tensor, kv_indptr: torch.Tensor):\n    # Build per-query arrays: kv_start[q], kv_max[q]\n    qo_indptr_cpu = qo_indptr.to(\"cpu\", non_blocking=False)\n    kv_indptr_cpu = kv_indptr.to(\"cpu\", non_blocking=False)\n    len_indptr = qo_indptr_cpu.numel()\n    total_q = int(qo_indptr_cpu[-1].item())\n    q_kv_start = torch.empty(total_q, dtype=torch.int32)\n    q_kv_max = torch.empty(total_q, dtype=torch.int32)\n    for b in range(len_indptr - 1):\n        q_start = int(qo_indptr_cpu[b].item())\n        q_end = int(qo_indptr_cpu[b + 1].item())\n        kv_start = int(kv_indptr_cpu[b].item())\n        kv_end = int(kv_indptr_cpu[b + 1].item())\n        q_len = q_end - q_start\n        kv_len = kv_end - kv_start\n        if q_len <= 0:\n            continue\n        delta = kv_len - q_len\n        pos = torch.arange(q_len, dtype=torch.int32)\n        kv_max = pos + 1 + int(delta)\n        kv_max = torch.clamp(kv_max, min=0, max=kv_len)\n        q_kv_start[q_start:q_end] = int(kv_start)\n        q_kv_max[q_start:q_end] = kv_max\n    return q_kv_start, q_kv_max\n\n\n@torch.no_grad()\ndef run(q, k, v, qo_indptr, kv_indptr, sm_scale=None):\n    if not torch.cuda.is_available():\n        raise RuntimeError(\"CUDA is required to run Triton kernels. No CUDA device is available.\")\n\n    HEAD_DIM = 128\n    NUM_QO = 32\n    NUM_KV = 8\n    RATIO = NUM_QO // NUM_KV  # 4\n\n    inputs = [q, k, v, qo_indptr, kv_indptr]\n    orig_devices = [t.device for t in inputs]\n    target_device = None\n    for t in inputs:\n        if t.is_cuda:\n            target_device = t.device\n            break\n    if target_device is None:\n        target_device = torch.device(\"cuda\")\n\n    q = q.to(device=target_device, dtype=torch.bfloat16, non_blocking=True)\n    k = k.to(device=target_device, dtype=torch.bfloat16, non_blocking=True)\n    v = v.to(device=target_device, dtype=torch.bfloat16, non_blocking=True)\n    qo_indptr = qo_indptr.to(device=target_device, dtype=torch.int32, non_blocking=True)\n    kv_indptr = kv_indptr.to(device=target_device, dtype=torch.int32, non_blocking=True)\n\n    total_q, num_qo_heads, head_dim = q.shape\n    total_kv, num_kv_heads, _ = k.shape\n\n    if num_qo_heads != NUM_QO:\n        raise ValueError(f\"num_qo_heads must be {NUM_QO}, got {num_qo_heads}\")\n    if num_kv_heads != NUM_KV:\n        raise ValueError(f\"num_kv_heads must be {NUM_KV}, got {num_kv_heads}\")\n    if head_dim != HEAD_DIM:\n        raise ValueError(f\"head_dim must be {HEAD_DIM}, got {head_dim}\")\n\n    if int(qo_indptr[-1].item()) != total_q:\n        raise ValueError(\"Constraint violated: total_q must equal qo_indptr[-1]\")\n    if int(kv_indptr[-1].item()) != total_kv:\n        raise ValueError(\"Constraint violated: total_kv must equal kv_indptr[-1]\")\n\n    if sm_scale is None:\n        sm_scale = 1.0 / math.sqrt(HEAD_DIM)\n    sm_scale = float(sm_scale)\n    ln2 = float(math.log(2.0))\n\n    # Prepare per-query kv_start and kv_max on CPU for simplicity, then move to target device\n    qo_indptr_cpu = qo_indptr.to(\"cpu\")\n    kv_indptr_cpu = kv_indptr.to(\"cpu\")\n    q_kv_start_cpu, q_kv_max_cpu = _prepare_q_meta_from_indptr(qo_indptr_cpu, kv_indptr_cpu)\n    q_kv_start = q_kv_start_cpu.to(device=target_device, non_blocking=True)\n    q_kv_max = q_kv_max_cpu.to(device=target_device, non_blocking=True)\n\n    out_gpu = torch.empty((total_q, NUM_QO, HEAD_DIM), dtype=torch.bfloat16, device=target_device)\n    lse_gpu = torch.empty((total_q, NUM_QO), dtype=torch.float32, device=target_device)\n\n    stride_q_q, stride_q_h, stride_q_d = q.stride()\n    stride_k_k, stride_k_h, stride_k_d = k.stride()\n    stride_v_k, stride_v_h, stride_v_d = v.stride()\n    stride_out_q, stride_out_h, stride_out_d = out_gpu.stride()\n    stride_lse_q, stride_lse_h = lse_gpu.stride()\n\n    grid = (total_q, NUM_KV)\n    BLOCK_N = 64\n    BLOCK_DK = 32\n    BLOCK_DV = 32\n\n    gqa_ragged_prefill_causal_h32_kv8_d128_kernel[grid](\n        q, k, v,\n        stride_q_q, stride_q_h, stride_q_d,\n        stride_k_k, stride_k_h, stride_k_d,\n        stride_v_k, stride_v_h, stride_v_d,\n        out_gpu, stride_out_q, stride_out_h, stride_out_d,\n        lse_gpu, stride_lse_q, stride_lse_h,\n        q_kv_start, q_kv_max,\n        total_q,\n        sm_scale, ln2,\n        RATIO=RATIO, HEAD_DIM=HEAD_DIM,\n        BLOCK_N=BLOCK_N, BLOCK_DK=BLOCK_DK, BLOCK_DV=BLOCK_DV,\n        num_warps=4, num_stages=2,\n    )\n\n    out = out_gpu.to(orig_devices[0], non_blocking=True)\n    lse = lse_gpu.to(orig_devices[0], non_blocking=True)\n    return out, lse"
    }
  ],
  "description": "gpt-5-2025-08-07 high reasoning effort optimized kernel for gqa_ragged_prefill_causal_h32_kv8_d128 (round 4)"
}