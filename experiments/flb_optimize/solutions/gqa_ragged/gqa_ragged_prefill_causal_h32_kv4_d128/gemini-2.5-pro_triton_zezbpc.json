{
  "name": "gemini-2.5-pro_triton_zezbpc",
  "definition": "gqa_ragged_prefill_causal_h32_kv4_d128",
  "author": "gemini-2.5-pro",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "B200"
    ],
    "entry_point": "main.py::run",
    "dependencies": []
  },
  "sources": [
    {
      "path": "main.py",
      "content": "import torch\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef gqa_ragged_prefill_causal_h32_kv4_d128_kernel(\n    # Pointers to tensors\n    q_ptr, k_ptr, v_ptr,\n    qo_indptr_ptr, kv_indptr_ptr, q_to_b_idx_ptr,\n    output_ptr, lse_ptr,\n    # Scalar\n    sm_scale,\n    # Strides\n    q_stride_tq, q_stride_h,\n    k_stride_tk, k_stride_h,\n    v_stride_tk, v_stride_h,\n    # Other metadata\n    total_q,\n    # Constants for clarity and performance\n    GQA_RATIO: tl.constexpr,\n    NUM_QO_HEADS: tl.constexpr,\n    # Compile-time constants\n    HEAD_DIM: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    \"\"\"\n    Triton kernel for Grouped-Query Attention on ragged tensors for prefill.\n    This kernel is specialized for causal attention with specific head dimensions.\n    Each program instance computes the attention output for one query token and one query head.\n    \"\"\"\n    # Get program IDs to identify the current query token and head\n    global_q_idx = tl.program_id(0)\n    h_qo_idx = tl.program_id(1)\n\n    # Find the sequence (batch element) index for the current query token\n    b_idx = tl.load(q_to_b_idx_ptr + global_q_idx)\n    \n    # Load sequence boundaries from indptr tensors\n    q_start = tl.load(qo_indptr_ptr + b_idx)\n    q_end = tl.load(qo_indptr_ptr + b_idx + 1)\n    kv_start = tl.load(kv_indptr_ptr + b_idx)\n    kv_end = tl.load(kv_indptr_ptr + b_idx + 1)\n\n    # Calculate causal attention length limit\n    q_idx_in_seq = global_q_idx - q_start\n    delta = (kv_end - kv_start) - (q_end - q_start)\n    max_kv_len = q_idx_in_seq + 1 + delta\n\n    # Initialize accumulators for online softmax\n    m_i = -float('inf')\n    l_i = 0.0\n    acc = tl.zeros([HEAD_DIM], dtype=tl.float32)\n\n    # Determine the corresponding KV head for the current QO head\n    h_kv_idx = h_qo_idx // GQA_RATIO\n\n    # Load the query vector\n    d_offsets = tl.arange(0, HEAD_DIM)\n    q_offset = global_q_idx * q_stride_tq + h_qo_idx * q_stride_h\n    q_ptrs = q_ptr + q_offset + d_offsets\n    q_vec = tl.load(q_ptrs).to(tl.float32)\n\n    # Loop over the key/value sequence in blocks\n    num_n_blocks = (max_kv_len + BLOCK_N - 1) // BLOCK_N\n    for block_n_idx in range(num_n_blocks):\n        # --- Compute offsets and mask for the current block of K/V ---\n        kv_idx_in_seq_start = block_n_idx * BLOCK_N\n        n_offsets = kv_idx_in_seq_start + tl.arange(0, BLOCK_N)\n        kv_mask = n_offsets < max_kv_len\n        global_kv_indices = kv_start + n_offsets\n\n        # --- Load K block ---\n        k_offset = global_kv_indices * k_stride_tk + h_kv_idx * k_stride_h\n        k_ptrs = k_ptr + k_offset[:, None] + d_offsets[None, :]\n        k_block = tl.load(k_ptrs, mask=kv_mask[:, None], other=0.0).to(tl.float32)\n        \n        # --- Compute S = Q @ K.T ---\n        s_block = tl.sum(q_vec[None, :] * k_block, axis=1)\n        s_block = s_block * sm_scale\n        s_block = tl.where(kv_mask, s_block, -float('inf'))\n\n        # --- Online softmax update ---\n        m_i_prev = m_i\n        m_i = tl.maximum(m_i, tl.max(s_block, axis=0))\n        p = tl.exp(s_block - m_i)\n        l_i = l_i * tl.exp(m_i_prev - m_i) + tl.sum(p, axis=0)\n\n        # --- Load V block and update accumulator ---\n        v_offset = global_kv_indices * v_stride_tk + h_kv_idx * v_stride_h\n        v_ptrs = v_ptr + v_offset[:, None] + d_offsets[None, :]\n        v_block = tl.load(v_ptrs, mask=kv_mask[:, None], other=0.0).to(tl.float32)\n\n        # Rescale accumulator before adding new values\n        acc = acc * tl.exp(m_i_prev - m_i)\n\n        # FIX: The original tl.dot(p, v_block) caused a compilation error because `p` is 1D\n        # while tl.dot requires 2D inputs for matrix multiplication.\n        # The correct operation is a weighted sum of value vectors: sum(p[i] * v_block[i]).\n        # This is implemented by reshaping p to [BLOCK_N, 1] for broadcasting,\n        # multiplying with v_block, and then summing over the block dimension (axis=0).\n        acc += tl.sum(p[:, None] * v_block, axis=0)\n\n    # Finalize and store output vector\n    # Guard against division by zero if l_i is 0 (e.g., empty sequence)\n    o = tl.where(l_i > 0, acc / l_i, 0.0)\n    output_offset = global_q_idx * q_stride_tq + h_qo_idx * q_stride_h\n    output_ptrs = output_ptr + output_offset + d_offsets\n    tl.store(output_ptrs, o.to(tl.bfloat16))\n\n    # Finalize and store log-sum-exp (LSE)\n    LOG2_E = 1.4426950408889634  # 1.0 / math.log(2.0)\n    # Guard against log(0)\n    lse = m_i + tl.log(l_i + 1e-9)\n    lse = lse * LOG2_E\n    lse_offset = global_q_idx * NUM_QO_HEADS + h_qo_idx\n    tl.store(lse_ptr + lse_offset, lse)\n\n\ndef _get_device(*tensors):\n    \"\"\"\n    Gets the common device of a list of tensors, handling CPU/CUDA logic.\n    \"\"\"\n    devices = {t.device.type for t in tensors if hasattr(t, 'device')}\n    if not devices:\n        return torch.device('cpu')\n    \n    if 'cuda' in devices:\n        if not torch.cuda.is_available():\n            raise RuntimeError(\"CUDA is not available, but input tensors are on CUDA.\")\n        cuda_devices = {t.device for t in tensors if t.device.type == 'cuda'}\n        if len(cuda_devices) > 1:\n            raise RuntimeError(f\"Input tensors are on multiple CUDA devices: {cuda_devices}\")\n        return list(cuda_devices)[0]\n    \n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        raise RuntimeError(\"Triton kernels require a CUDA-enabled GPU, but none was found.\")\n\n\ndef run(*args, **kwargs):\n    \"\"\"\n    Entry point for the GQA Ragged Prefill Causal Attention kernel.\n\n    Args:\n        q (torch.Tensor): Query tensor of shape [total_q, num_qo_heads, head_dim].\n        k (torch.Tensor): Key tensor of shape [total_kv, num_kv_heads, head_dim].\n        v (torch.Tensor): Value tensor of shape [total_kv, num_kv_heads, head_dim].\n        qo_indptr (torch.Tensor): Query offsets for each sequence of shape [len_indptr].\n        kv_indptr (torch.Tensor): Key-value offsets for each sequence of shape [len_indptr].\n        sm_scale (float, optional): Softmax scale. Defaults to 1/sqrt(head_dim).\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor]:\n            - output (torch.Tensor): Attention output of shape [total_q, num_qo_heads, head_dim].\n            - lse (torch.Tensor): Log-sum-exp of attention logits of shape [total_q, num_qo_heads].\n    \"\"\"\n    # 1. Argument parsing\n    arg_names = ['q', 'k', 'v', 'qo_indptr', 'kv_indptr', 'sm_scale']\n    expected_arg_count = 5\n    \n    if len(args) > len(arg_names):\n        raise TypeError(f\"run() takes at most {len(arg_names)} positional arguments but {len(args)} were given\")\n\n    params = {name: val for name, val in zip(arg_names, args)}\n    params.update(kwargs)\n\n    missing_args = [name for name in arg_names[:expected_arg_count] if name not in params]\n    if missing_args:\n        raise TypeError(f\"run() missing {len(missing_args)} required positional argument(s): {', '.join(missing_args)}\")\n\n    q, k, v, qo_indptr, kv_indptr = [params[name] for name in arg_names[:expected_arg_count]]\n    sm_scale = params.get('sm_scale')\n\n    # 2. Constants and shape assertions\n    NUM_QO_HEADS = 32\n    NUM_KV_HEADS = 4\n    HEAD_DIM = 128\n    \n    total_q, num_qo_heads, head_dim = q.shape\n    total_kv, num_kv_heads, _ = k.shape\n    len_indptr = qo_indptr.shape[0]\n\n    assert num_qo_heads == NUM_QO_HEADS, f\"Expected num_qo_heads={NUM_QO_HEADS}, got {num_qo_heads}\"\n    assert num_kv_heads == NUM_KV_HEADS, f\"Expected num_kv_heads={NUM_KV_HEADS}, got {num_kv_heads}\"\n    assert head_dim == HEAD_DIM, f\"Expected head_dim={HEAD_DIM}, got {head_dim}\"\n    assert qo_indptr.dim() == 1 and kv_indptr.dim() == 1, \"indptr tensors must be 1D\"\n    assert len_indptr > 0, \"indptr tensors cannot be empty\"\n    assert total_q == qo_indptr[-1].item(), f\"total_q ({total_q}) must match qo_indptr[-1] ({qo_indptr[-1].item()})\"\n    assert total_kv == kv_indptr[-1].item(), f\"total_kv ({total_kv}) must match kv_indptr[-1] ({kv_indptr[-1].item()})\"\n    assert qo_indptr.shape == kv_indptr.shape, \"qo_indptr and kv_indptr must have the same shape\"\n\n    if sm_scale is None:\n        sm_scale = 1.0 / math.sqrt(HEAD_DIM)\n\n    # 3. Device management\n    initial_device = q.device\n    kernel_device = _get_device(q, k, v, qo_indptr, kv_indptr)\n    \n    q, k, v, qo_indptr, kv_indptr = [t.to(kernel_device) for t in [q, k, v, qo_indptr, kv_indptr]]\n    \n    q, k, v = [t.contiguous() for t in [q, k, v]]\n\n    # 4. Prepare kernel inputs and outputs\n    output = torch.empty_like(q, dtype=torch.bfloat16)\n    lse = torch.full((total_q, NUM_QO_HEADS), -float(\"inf\"), dtype=torch.float32, device=kernel_device)\n\n    # 5. Launch kernel\n    grid = (total_q, NUM_QO_HEADS)\n    \n    BLOCK_N = 64\n    \n    if total_q > 0:\n        # Precompute a mapping from global query index to batch index for efficient lookup in the kernel\n        q_indices = torch.arange(total_q, device=kernel_device)\n        qo_ends = qo_indptr[1:]\n        q_to_b_idx = torch.searchsorted(qo_ends, q_indices, right=True)\n        \n        gqa_ragged_prefill_causal_h32_kv4_d128_kernel[grid](\n            q, k, v,\n            qo_indptr, kv_indptr, q_to_b_idx,\n            output, lse,\n            sm_scale,\n            q.stride(0), q.stride(1),\n            k.stride(0), k.stride(1),\n            v.stride(0), v.stride(1),\n            total_q,\n            GQA_RATIO=NUM_QO_HEADS // NUM_KV_HEADS,\n            NUM_QO_HEADS=NUM_QO_HEADS,\n            HEAD_DIM=HEAD_DIM,\n            BLOCK_N=BLOCK_N,\n        )\n\n    # 6. Restore output device\n    output = output.to(initial_device)\n    lse = lse.to(initial_device)\n\n    return output, lse"
    }
  ],
  "description": "gemini-2.5-pro optimized kernel for gqa_ragged_prefill_causal_h32_kv4_d128 (round 2)"
}