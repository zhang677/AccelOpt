{
  "name": "claude-opus-4-1_triton_28277f",
  "definition": "gqa_ragged_prefill_causal_h32_kv4_d128",
  "author": "claude-opus-4-1-20250805",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "B200"
    ],
    "entry_point": "main.py::run",
    "dependencies": []
  },
  "sources": [
    {
      "path": "main.py",
      "content": "import torch\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef gqa_ragged_prefill_causal_kernel(\n    q_ptr, k_ptr, v_ptr,\n    output_ptr, lse_ptr,\n    qo_indptr_ptr, kv_indptr_ptr,\n    sm_scale,\n    batch_idx,\n    total_q, total_kv,\n    stride_q_tok, stride_q_h, stride_q_d,\n    stride_kv_tok, stride_kv_h, stride_kv_d,\n    stride_out_tok, stride_out_h, stride_out_d,\n    stride_lse_tok, stride_lse_h,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_D: tl.constexpr,\n    NUM_QO_HEADS: tl.constexpr,\n    NUM_KV_HEADS: tl.constexpr,\n    GQA_RATIO: tl.constexpr,\n):\n    # Get sequence boundaries\n    q_start = tl.load(qo_indptr_ptr + batch_idx)\n    q_end = tl.load(qo_indptr_ptr + batch_idx + 1)\n    kv_start = tl.load(kv_indptr_ptr + batch_idx)\n    kv_end = tl.load(kv_indptr_ptr + batch_idx + 1)\n    \n    num_q_tokens = q_end - q_start\n    num_kv_tokens = kv_end - kv_start\n    \n    # Block indices\n    block_m = tl.program_id(0)\n    head_idx = tl.program_id(1)\n    \n    # Calculate KV head for GQA\n    kv_head = head_idx // GQA_RATIO\n    \n    # Calculate query token indices for this block\n    q_block_start = block_m * BLOCK_M\n    \n    # Early exit if out of bounds\n    if q_block_start >= num_q_tokens:\n        return\n    \n    # Calculate causal mask boundary\n    delta = num_kv_tokens - num_q_tokens\n    \n    # Initialize accumulators for each query in the block\n    m_i = tl.full([BLOCK_M], value=-float('inf'), dtype=tl.float32)\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_D], dtype=tl.float32)\n    \n    # Load query block\n    q_offs_m = tl.arange(0, BLOCK_M)\n    q_offs_d = tl.arange(0, BLOCK_D)\n    q_mask = (q_block_start + q_offs_m[:, None] < num_q_tokens) & (q_offs_d[None, :] < BLOCK_D)\n    q_ptrs = q_ptr + (q_start + q_block_start + q_offs_m[:, None]) * stride_q_tok + head_idx * stride_q_h + q_offs_d[None, :] * stride_q_d\n    q = tl.load(q_ptrs, mask=q_mask, other=0.0).to(tl.float32)\n    \n    # Process KV blocks\n    for kv_block_start in range(0, num_kv_tokens, BLOCK_N):\n        # Load K block\n        k_offs_n = tl.arange(0, BLOCK_N)\n        k_offs_d = tl.arange(0, BLOCK_D)\n        k_mask = (kv_block_start + k_offs_n[:, None] < num_kv_tokens) & (k_offs_d[None, :] < BLOCK_D)\n        k_ptrs = k_ptr + (kv_start + kv_block_start + k_offs_n[:, None]) * stride_kv_tok + kv_head * stride_kv_h + k_offs_d[None, :] * stride_kv_d\n        k = tl.load(k_ptrs, mask=k_mask, other=0.0).to(tl.float32)\n        \n        # Compute QK^T\n        qk = tl.dot(q, tl.trans(k))\n        \n        # Apply scale\n        qk = qk * sm_scale\n        \n        # Apply causal mask\n        q_offs_m_2 = tl.arange(0, BLOCK_M)\n        k_offs_n_2 = tl.arange(0, BLOCK_N)\n        \n        # Calculate the maximum KV index each query can attend to\n        q_positions = q_block_start + q_offs_m_2\n        max_kv_idx = q_positions + 1 + delta\n        \n        # Create causal mask\n        kv_positions = kv_block_start + k_offs_n_2\n        causal_mask = kv_positions[None, :] < max_kv_idx[:, None]\n        \n        # Also ensure we don't go beyond actual tokens\n        valid_q = (q_block_start + q_offs_m_2[:, None]) < num_q_tokens\n        valid_kv = (kv_block_start + k_offs_n_2[None, :]) < num_kv_tokens\n        qk_mask = causal_mask & valid_q & valid_kv\n        \n        qk = tl.where(qk_mask, qk, -float('inf'))\n        \n        # Online softmax update\n        m_i_new = tl.maximum(m_i, tl.max(qk, axis=1))\n        \n        # Compute attention weights with numerical stability\n        p = tl.exp(qk - m_i_new[:, None])\n        \n        # Update running sum with correction\n        alpha = tl.exp(m_i - m_i_new)\n        l_i_new = alpha * l_i + tl.sum(p, axis=1)\n        \n        # Load V block\n        v_offs_n = tl.arange(0, BLOCK_N)\n        v_offs_d = tl.arange(0, BLOCK_D)\n        v_mask = (kv_block_start + v_offs_n[:, None] < num_kv_tokens) & (v_offs_d[None, :] < BLOCK_D)\n        v_ptrs = v_ptr + (kv_start + kv_block_start + v_offs_n[:, None]) * stride_kv_tok + kv_head * stride_kv_h + v_offs_d[None, :] * stride_kv_d\n        v = tl.load(v_ptrs, mask=v_mask, other=0.0).to(tl.float32)\n        \n        # Update accumulator with proper scaling\n        acc = acc * alpha[:, None]\n        acc += tl.dot(p, v)\n        \n        # Update running max and sum\n        m_i = m_i_new\n        l_i = l_i_new\n    \n    # Normalize output\n    acc = acc / l_i[:, None]\n    \n    # Store output\n    out_offs_m = tl.arange(0, BLOCK_M)\n    out_offs_d = tl.arange(0, BLOCK_D)\n    out_mask = (q_block_start + out_offs_m[:, None] < num_q_tokens) & (out_offs_d[None, :] < BLOCK_D)\n    out_ptrs = output_ptr + (q_start + q_block_start + out_offs_m[:, None]) * stride_out_tok + head_idx * stride_out_h + out_offs_d[None, :] * stride_out_d\n    tl.store(out_ptrs, acc.to(tl.bfloat16), mask=out_mask)\n    \n    # Store LSE (log2 scale)\n    lse_offs = tl.arange(0, BLOCK_M)\n    lse_mask = q_block_start + lse_offs < num_q_tokens\n    lse_ptrs = lse_ptr + (q_start + q_block_start + lse_offs) * stride_lse_tok + head_idx * stride_lse_h\n    # Convert to log2 scale\n    log2_e = 1.4426950408889634  # 1.0 / ln(2)\n    lse_val = (m_i + tl.log(l_i)) * log2_e\n    tl.store(lse_ptrs, lse_val, mask=lse_mask)\n\ndef run(q, k, v, qo_indptr, kv_indptr, sm_scale):\n    # Handle device management\n    device = None\n    inputs_on_cuda = []\n    original_devices = []\n    \n    # Check and move tensors to CUDA if needed\n    for tensor, name in [(q, 'q'), (k, 'k'), (v, 'v'), (qo_indptr, 'qo_indptr'), (kv_indptr, 'kv_indptr')]:\n        original_devices.append(tensor.device)\n        if tensor.is_cuda:\n            if device is None:\n                device = tensor.device\n            inputs_on_cuda.append(True)\n        else:\n            if not torch.cuda.is_available():\n                raise RuntimeError(f\"Tensor '{name}' is on CPU but CUDA is not available\")\n            if device is None:\n                device = torch.device('cuda')\n            inputs_on_cuda.append(False)\n    \n    # Move CPU tensors to GPU\n    if not q.is_cuda:\n        q = q.cuda()\n    if not k.is_cuda:\n        k = k.cuda()\n    if not v.is_cuda:\n        v = v.cuda()\n    if not qo_indptr.is_cuda:\n        qo_indptr = qo_indptr.cuda()\n    if not kv_indptr.is_cuda:\n        kv_indptr = kv_indptr.cuda()\n    \n    # Get dimensions\n    total_q, num_qo_heads, head_dim = q.shape\n    total_kv, num_kv_heads, _ = k.shape\n    len_indptr = qo_indptr.shape[0]\n    \n    # Verify constants\n    assert num_qo_heads == 32\n    assert num_kv_heads == 4\n    assert head_dim == 128\n    \n    # Verify constraints\n    assert total_q == qo_indptr[-1].item()\n    assert total_kv == kv_indptr[-1].item()\n    \n    # Initialize outputs\n    output = torch.zeros((total_q, num_qo_heads, head_dim), dtype=torch.bfloat16, device=device)\n    lse = torch.full((total_q, num_qo_heads), -float('inf'), dtype=torch.float32, device=device)\n    \n    # Constants optimized for B200\n    BLOCK_M = 32\n    BLOCK_N = 64\n    BLOCK_D = 128\n    GQA_RATIO = num_qo_heads // num_kv_heads\n    \n    # Process each batch\n    for batch_idx in range(len_indptr - 1):\n        q_start = qo_indptr[batch_idx].item()\n        q_end = qo_indptr[batch_idx + 1].item()\n        kv_start = kv_indptr[batch_idx].item()\n        kv_end = kv_indptr[batch_idx + 1].item()\n        \n        num_q_tokens = q_end - q_start\n        num_kv_tokens = kv_end - kv_start\n        \n        if num_q_tokens <= 0 or num_kv_tokens <= 0:\n            continue\n        \n        grid = (triton.cdiv(num_q_tokens, BLOCK_M), num_qo_heads)\n        \n        gqa_ragged_prefill_causal_kernel[grid](\n            q, k, v,\n            output, lse,\n            qo_indptr, kv_indptr,\n            sm_scale,\n            batch_idx,\n            total_q, total_kv,\n            q.stride(0), q.stride(1), q.stride(2),\n            k.stride(0), k.stride(1), k.stride(2),\n            output.stride(0), output.stride(1), output.stride(2),\n            lse.stride(0), lse.stride(1),\n            BLOCK_M=BLOCK_M,\n            BLOCK_N=BLOCK_N,\n            BLOCK_D=BLOCK_D,\n            NUM_QO_HEADS=num_qo_heads,\n            NUM_KV_HEADS=num_kv_heads,\n            GQA_RATIO=GQA_RATIO,\n            num_warps=4,\n            num_stages=2,\n        )\n    \n    # Move outputs back to original device if needed\n    if not inputs_on_cuda[0]:  # q was originally on CPU\n        output = output.to(original_devices[0])\n        lse = lse.to(original_devices[0])\n    \n    return output, lse"
    }
  ],
  "description": "claude-opus-4-1-20250805 optimized kernel for gqa_ragged_prefill_causal_h32_kv4_d128 (round 10)"
}