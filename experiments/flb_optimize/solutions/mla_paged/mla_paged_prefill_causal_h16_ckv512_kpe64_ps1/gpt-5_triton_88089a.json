{
  "name": "gpt-5_triton_88089a",
  "definition": "mla_paged_prefill_causal_h16_ckv512_kpe64_ps1",
  "author": "gpt-5-2025-08-07",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "B200"
    ],
    "entry_point": "main.py::run",
    "dependencies": []
  },
  "sources": [
    {
      "path": "main.py",
      "content": "import math\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef mla_paged_prefill_causal_h16_ckv512_kpe64_ps1_kernel(\n    q_nope_ptr,  # bf16 [total_q, H, D_CKV]\n    q_pe_ptr,    # bf16 [total_q, H, D_KPE]\n    ckv_ptr,     # bf16 [num_pages, D_CKV]\n    kpe_ptr,     # bf16 [num_pages, D_KPE]\n    qo_indptr_ptr,  # int32 [len_indptr]\n    kv_indptr_ptr,  # int32 [len_indptr]\n    kv_indices_ptr,  # int32 [num_kv_indices]\n    q_to_seq_ptr,   # int32 [total_q]\n    sm_scale,       # float32\n    output_ptr,     # bf16 [total_q, H, D_CKV]\n    lse_ptr,        # float32 [total_q, H]\n    total_q,\n    len_indptr,\n    num_pages,\n    num_kv_indices,\n    H: tl.constexpr,\n    D_CKV: tl.constexpr,\n    D_KPE: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_DC: tl.constexpr,\n    BLOCK_DK: tl.constexpr,\n):\n    pid_q = tl.program_id(0)\n    pid_h = tl.program_id(1)\n\n    if pid_h >= H:\n        return\n    if pid_q >= total_q:\n        return\n\n    # Load sequence id for this query\n    seq_id = tl.load(q_to_seq_ptr + pid_q, mask=True, other=0).to(tl.int32)\n\n    # Load q range for this seq\n    q_start = tl.load(qo_indptr_ptr + seq_id, mask=True, other=0).to(tl.int32)\n    q_end = tl.load(qo_indptr_ptr + seq_id + 1, mask=True, other=0).to(tl.int32)\n    q_len = q_end - q_start\n    q_rel = pid_q - q_start\n\n    # Load kv range for this seq\n    kv_beg = tl.load(kv_indptr_ptr + seq_id, mask=True, other=0).to(tl.int32)\n    kv_end = tl.load(kv_indptr_ptr + seq_id + 1, mask=True, other=0).to(tl.int32)\n    kv_len = kv_end - kv_beg\n\n    # Early exit flags\n    do_work = (kv_len > 0) & (q_len > 0)\n\n    # Causal mask parameters\n    prefix_len = kv_len - q_len\n    query_abs_pos = prefix_len + q_rel\n\n    # Base pointers for Q (row-major [Q, H, D])\n    qn_base = (pid_q * H + pid_h) * D_CKV\n    qp_base = (pid_q * H + pid_h) * D_KPE\n\n    # Streaming softmax in base-2\n    inv_ln2 = 1.4426950408889634  # 1 / ln(2)\n    m_i2 = -float(\"inf\")\n    l_i2 = 0.0\n\n    # Accumulators for output: split into 4 chunks of 128 dims each\n    O0 = tl.zeros((BLOCK_DC,), dtype=tl.float32)\n    O1 = tl.zeros((BLOCK_DC,), dtype=tl.float32)\n    O2 = tl.zeros((BLOCK_DC,), dtype=tl.float32)\n    O3 = tl.zeros((BLOCK_DC,), dtype=tl.float32)\n\n    start = tl.zeros((), dtype=tl.int32)\n    while start < kv_len:\n        # KV indices for this tile\n        offs_n = start + tl.arange(0, BLOCK_N)\n        mask_n = offs_n < kv_len\n        page_idx = tl.load(kv_indices_ptr + kv_beg + offs_n, mask=mask_n, other=0).to(tl.int32)\n\n        # Compute logits for this tile\n        logits_tile = tl.zeros((BLOCK_N,), dtype=tl.float32)\n\n        # q_nope dot ckv\n        for d0 in tl.static_range(0, D_CKV, BLOCK_DC):\n            d_offsets = d0 + tl.arange(0, BLOCK_DC)\n            qn_chunk = tl.load(q_nope_ptr + qn_base + d_offsets, mask=True, other=0).to(tl.float32)\n            kc_tile = tl.load(\n                ckv_ptr + page_idx[:, None] * D_CKV + d_offsets[None, :],\n                mask=mask_n[:, None],\n                other=0\n            ).to(tl.float32)\n            logits_tile += tl.sum(kc_tile * qn_chunk[None, :], axis=1)\n\n        # q_pe dot kpe\n        for d0 in tl.static_range(0, D_KPE, BLOCK_DK):\n            d_offsets = d0 + tl.arange(0, BLOCK_DK)\n            qp_chunk = tl.load(q_pe_ptr + qp_base + d_offsets, mask=True, other=0).to(tl.float32)\n            kp_tile = tl.load(\n                kpe_ptr + page_idx[:, None] * D_KPE + d_offsets[None, :],\n                mask=mask_n[:, None],\n                other=0\n            ).to(tl.float32)\n            logits_tile += tl.sum(kp_tile * qp_chunk[None, :], axis=1)\n\n        # Scale logits\n        logits_tile = logits_tile * sm_scale\n        # Convert to base-2 space\n        z_tile = logits_tile * inv_ln2\n\n        # Causal mask: allow only indices <= query_abs_pos\n        causal_mask = offs_n <= query_abs_pos\n        valid_mask = mask_n & causal_mask\n        neg_inf = -float(\"inf\")\n        z_tile = tl.where(valid_mask, z_tile, neg_inf)\n\n        # Streaming softmax update (base-2)\n        m_tile = tl.max(z_tile, axis=0)\n        m_new = tl.maximum(m_i2, m_tile)\n        # Guard against -inf - -inf\n        alpha = tl.where(m_i2 == -float(\"inf\"), 0.0, tl.exp2(m_i2 - m_new))\n        p_tile = tl.exp2(z_tile - m_new)\n        p_tile = tl.where(valid_mask, p_tile, 0.0)\n        sum_p = tl.sum(p_tile, axis=0)\n\n        # Update l and O with scaling\n        l_i2 = l_i2 * alpha + sum_p\n        O0 = O0 * alpha\n        O1 = O1 * alpha\n        O2 = O2 * alpha\n        O3 = O3 * alpha\n\n        # Accumulate O chunks: O += sum_n p_tile[n] * Kc[n, :]\n        # Chunk 0\n        d_offsets0 = 0 + tl.arange(0, BLOCK_DC)\n        kc0 = tl.load(\n            ckv_ptr + page_idx[:, None] * D_CKV + d_offsets0[None, :],\n            mask=mask_n[:, None],\n            other=0\n        ).to(tl.float32)\n        O0 += tl.sum(p_tile[:, None] * kc0, axis=0)\n\n        # Chunk 1\n        d_offsets1 = BLOCK_DC + tl.arange(0, BLOCK_DC)\n        kc1 = tl.load(\n            ckv_ptr + page_idx[:, None] * D_CKV + d_offsets1[None, :],\n            mask=mask_n[:, None],\n            other=0\n        ).to(tl.float32)\n        O1 += tl.sum(p_tile[:, None] * kc1, axis=0)\n\n        # Chunk 2\n        d_offsets2 = (2 * BLOCK_DC) + tl.arange(0, BLOCK_DC)\n        kc2 = tl.load(\n            ckv_ptr + page_idx[:, None] * D_CKV + d_offsets2[None, :],\n            mask=mask_n[:, None],\n            other=0\n        ).to(tl.float32)\n        O2 += tl.sum(p_tile[:, None] * kc2, axis=0)\n\n        # Chunk 3\n        d_offsets3 = (3 * BLOCK_DC) + tl.arange(0, BLOCK_DC)\n        kc3 = tl.load(\n            ckv_ptr + page_idx[:, None] * D_CKV + d_offsets3[None, :],\n            mask=mask_n[:, None],\n            other=0\n        ).to(tl.float32)\n        O3 += tl.sum(p_tile[:, None] * kc3, axis=0)\n\n        m_i2 = m_new\n        start += BLOCK_N\n\n    # Finalize and store\n    if do_work:\n        # lse in base-2\n        lse_val = m_i2 + tl.log2(l_i2)\n        # Normalize output\n        inv_l = 1.0 / l_i2\n        O0 = O0 * inv_l\n        O1 = O1 * inv_l\n        O2 = O2 * inv_l\n        O3 = O3 * inv_l\n\n        # Store O to output bf16\n        out_base = (pid_q * H + pid_h) * D_CKV\n        # Chunk 0\n        d_offsets0 = 0 + tl.arange(0, BLOCK_DC)\n        tl.store(output_ptr + out_base + d_offsets0, O0.to(tl.bfloat16), mask=True)\n        # Chunk 1\n        d_offsets1 = BLOCK_DC + tl.arange(0, BLOCK_DC)\n        tl.store(output_ptr + out_base + d_offsets1, O1.to(tl.bfloat16), mask=True)\n        # Chunk 2\n        d_offsets2 = (2 * BLOCK_DC) + tl.arange(0, BLOCK_DC)\n        tl.store(output_ptr + out_base + d_offsets2, O2.to(tl.bfloat16), mask=True)\n        # Chunk 3\n        d_offsets3 = (3 * BLOCK_DC) + tl.arange(0, BLOCK_DC)\n        tl.store(output_ptr + out_base + d_offsets3, O3.to(tl.bfloat16), mask=True)\n\n        # Store lse\n        lse_off = pid_q * H + pid_h\n        tl.store(lse_ptr + lse_off, lse_val)\n\n\ndef run(q_nope, q_pe, ckv_cache, kpe_cache, qo_indptr, kv_indptr, kv_indices, sm_scale=None):\n    # Validate CUDA availability and manage devices\n    def to_cuda_if_needed(t):\n        if not t.is_cuda:\n            if not torch.cuda.is_available():\n                raise RuntimeError(\"CUDA is not available, but input tensors are on CPU. Please enable CUDA or move inputs to GPU.\")\n            return t.cuda()\n        return t\n\n    # Dtypes and shapes\n    assert q_nope.dtype in (torch.bfloat16, torch.float16, torch.float32)\n    assert q_pe.dtype in (torch.bfloat16, torch.float16, torch.float32)\n    assert ckv_cache.dtype in (torch.bfloat16, torch.float16, torch.float32)\n    assert kpe_cache.dtype in (torch.bfloat16, torch.float16, torch.float32)\n    assert qo_indptr.dtype == torch.int32\n    assert kv_indptr.dtype == torch.int32\n    assert kv_indices.dtype == torch.int32\n\n    # Original device for outputs\n    orig_device = q_nope.device\n\n    # Move to CUDA if needed\n    q_nope = to_cuda_if_needed(q_nope)\n    q_pe = to_cuda_if_needed(q_pe)\n    ckv_cache = to_cuda_if_needed(ckv_cache)\n    kpe_cache = to_cuda_if_needed(kpe_cache)\n    qo_indptr = to_cuda_if_needed(qo_indptr)\n    kv_indptr = to_cuda_if_needed(kv_indptr)\n    kv_indices = to_cuda_if_needed(kv_indices)\n\n    # Ensure contiguous layouts and correct dtypes (bf16 for caches/Q)\n    q_nope = q_nope.to(torch.bfloat16).contiguous()\n    q_pe = q_pe.to(torch.bfloat16).contiguous()\n    ckv_cache = ckv_cache.to(torch.bfloat16).contiguous()\n    kpe_cache = kpe_cache.to(torch.bfloat16).contiguous()\n    qo_indptr = qo_indptr.contiguous()\n    kv_indptr = kv_indptr.contiguous()\n    kv_indices = kv_indices.contiguous()\n\n    # Shapes and constants\n    total_q, num_qo_heads, head_dim_ckv = q_nope.shape\n    head_dim_kpe = q_pe.shape[-1]\n    page_size = ckv_cache.shape[1]\n    len_indptr = qo_indptr.shape[0]\n    batch_size = len_indptr - 1\n    num_pages = ckv_cache.shape[0]\n    num_kv_indices = kv_indices.shape[0]\n\n    # Checks for constants\n    assert num_qo_heads == 16, \"num_qo_heads must be 16\"\n    assert head_dim_ckv == 512, \"head_dim_ckv must be 512\"\n    assert head_dim_kpe == 64, \"head_dim_kpe must be 64\"\n    assert page_size == 1, \"page_size must be 1\"\n    assert total_q == int(qo_indptr[-1].item()), \"total_q must equal qo_indptr[-1]\"\n    assert num_kv_indices == int(kv_indptr[-1].item()), \"num_kv_indices must equal kv_indptr[-1]\"\n\n    # Squeeze page dimension as page_size == 1\n    ckv_squeezed = ckv_cache.squeeze(1).contiguous()  # [num_pages, 512]\n    kpe_squeezed = kpe_cache.squeeze(1).contiguous()  # [num_pages, 64]\n\n    # Default sm_scale\n    if sm_scale is None:\n        sm_scale = 1.0 / math.sqrt(float(head_dim_ckv))\n    sm_scale = float(sm_scale)\n\n    # Build mapping from query index to sequence id\n    q_to_seq = torch.empty((total_q,), dtype=torch.int32, device=q_nope.device)\n    qo_indptr_cpu = qo_indptr.to(torch.int64).cpu()\n    for b in range(batch_size):\n        start = int(qo_indptr_cpu[b].item())\n        end = int(qo_indptr_cpu[b + 1].item())\n        if end > start:\n            q_to_seq[start:end] = b\n\n    # Allocate outputs on device, initialize as in reference\n    output = torch.zeros((total_q, num_qo_heads, head_dim_ckv), dtype=torch.bfloat16, device=q_nope.device)\n    lse = torch.full((total_q, num_qo_heads), -float(\"inf\"), dtype=torch.float32, device=q_nope.device)\n\n    # Launch kernel\n    H = 16\n    D_CKV = 512\n    D_KPE = 64\n    BLOCK_N = 128\n    BLOCK_DC = 128\n    BLOCK_DK = 64\n\n    grid = (total_q, H)\n\n    mla_paged_prefill_causal_h16_ckv512_kpe64_ps1_kernel[grid](\n        q_nope,  # bf16\n        q_pe,    # bf16\n        ckv_squeezed,  # bf16\n        kpe_squeezed,  # bf16\n        qo_indptr,     # int32\n        kv_indptr,     # int32\n        kv_indices,    # int32\n        q_to_seq,      # int32\n        sm_scale,      # float32\n        output,        # bf16\n        lse,           # float32\n        total_q,\n        len_indptr,\n        num_pages,\n        num_kv_indices,\n        H=H,\n        D_CKV=D_CKV,\n        D_KPE=D_KPE,\n        BLOCK_N=BLOCK_N,\n        BLOCK_DC=BLOCK_DC,\n        BLOCK_DK=BLOCK_DK,\n        num_warps=8,\n        num_stages=2,\n    )\n\n    # Move results back to the original device if necessary\n    if orig_device.type != 'cuda':\n        output = output.to(orig_device)\n        lse = lse.to(orig_device)\n\n    return output, lse"
    }
  ],
  "description": "gpt-5-2025-08-07 high reasoning effort optimized kernel for mla_paged_prefill_causal_h16_ckv512_kpe64_ps1 (round 2)"
}