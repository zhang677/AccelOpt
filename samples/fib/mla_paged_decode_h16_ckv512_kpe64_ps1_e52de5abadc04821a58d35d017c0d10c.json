{
  "name": "e52de5abadc04821a58d35d017c0d10c",
  "definition": "mla_paged_decode_h16_ckv512_kpe64_ps1",
  "author": "AccelOpt",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "H100"
    ],
    "entry_point": "main.py::run"
  },
  "sources": [
    {
      "path": "main.py",
      "content": "import torch\nimport triton\nimport triton.language as tl\nimport math\n\n# ------------------------------------------------------------------\n# 1. Tiled MLA Kernel (First Pass)\n# ------------------------------------------------------------------\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_TOK': 32, 'NUM_STAGES': 2}, num_warps=4, num_stages=2),\n        triton.Config({'BLOCK_TOK': 64, 'NUM_STAGES': 2}, num_warps=8, num_stages=2),\n        triton.Config({'BLOCK_TOK': 64, 'NUM_STAGES': 3}, num_warps=8, num_stages=3),\n    ],\n    key=['max_kv_len'],\n)\n@triton.jit\ndef _mla_tile_kernel(\n    QN, QP, KC, KP,\n    KV_INDICES, KV_INDPTR,\n    SM_SCALE,\n    P_M, P_L, P_ACC,\n    max_kv_len, # For autotune key\n    num_tiles,\n    H: tl.constexpr,            # Number of heads (16)\n    D_CKV: tl.constexpr,        # Head dimension for CKV (512)\n    D_KPE: tl.constexpr,        # Head dimension for KPE (64)\n    TILE_TOK: tl.constexpr,     # Tokens per program\n    BLOCK_TOK: tl.constexpr,    # Tokens per loop iteration\n    NUM_STAGES: tl.constexpr,   # Pipelining stages\n):\n    b_idx = tl.program_id(axis=0)\n    t_idx = tl.program_id(axis=1)\n\n    # Sequence boundaries\n    kv_beg = tl.load(KV_INDPTR + b_idx)\n    kv_end = tl.load(KV_INDPTR + b_idx + 1)\n    kv_len = kv_end - kv_beg\n\n    tile_start_offset = t_idx * TILE_TOK\n    if tile_start_offset >= kv_len:\n        return\n\n    tile_kv_beg = kv_beg + tile_start_offset\n    tile_kv_end = tl.minimum(kv_beg + (t_idx + 1) * TILE_TOK, kv_end)\n    tile_len = tile_kv_end - tile_kv_beg\n\n    # Dimension offsets\n    offs_h = tl.arange(0, H)\n    offs_ckv = tl.arange(0, D_CKV)\n    offs_kpe = tl.arange(0, D_KPE)\n\n    # Load Query Components\n    qn = tl.load(QN + (b_idx * H + offs_h[:, None]) * D_CKV + offs_ckv[None, :])\n    qp = tl.load(QP + (b_idx * H + offs_h[:, None]) * D_KPE + offs_kpe[None, :])\n    \n    qn_t = tl.trans(qn).to(tl.bfloat16)\n    qp_t = tl.trans(qp).to(tl.bfloat16)\n\n    # Online softmax accumulators for this tile\n    m_i = tl.full([H], -float('inf'), dtype=tl.float32)\n    l_i = tl.zeros([H], dtype=tl.float32)\n    acc = tl.zeros([H, D_CKV], dtype=tl.float32)\n\n    # Iterate through the tile in BLOCK_TOK increments\n    for tok_start in tl.range(0, tile_len, BLOCK_TOK, num_stages=NUM_STAGES, flatten=True):\n        offs_t = tl.arange(0, BLOCK_TOK)\n        mask_t = (tok_start + offs_t) < tile_len\n        \n        # Load physical token indices\n        tok_idx = tl.load(KV_INDICES + tile_kv_beg + tok_start + offs_t, mask=mask_t, other=0)\n        \n        # Gather KV cache components\n        kc_blk = tl.load(KC + (tok_idx[:, None] * D_CKV) + offs_ckv[None, :], mask=mask_t[:, None], other=0.0).to(tl.bfloat16)\n        kp_blk = tl.load(KP + (tok_idx[:, None] * D_KPE) + offs_kpe[None, :], mask=mask_t[:, None], other=0.0).to(tl.bfloat16)\n\n        # Compute Attention Logits: (KC @ QN^T) + (KP @ QP^T)\n        logits = tl.dot(kc_blk, qn_t)\n        logits = tl.dot(kp_blk, qp_t, acc=logits)\n        logits *= SM_SCALE\n        \n        logits = tl.where(mask_t[:, None], logits, -float('inf'))\n\n        # Online softmax update logic\n        m_curr = tl.max(logits, axis=0) \n        m_next = tl.maximum(m_i, m_curr)\n        \n        alpha = tl.exp(m_i - m_next)\n        p = tl.exp(logits - m_next[None, :])\n        \n        l_i = l_i * alpha + tl.sum(p, axis=0)\n        acc = acc * alpha[:, None]\n        \n        p_t = tl.trans(p.to(tl.bfloat16))\n        acc = tl.dot(p_t, kc_blk, acc=acc)\n        m_i = m_next\n\n    # Store tile results to global workspace\n    # PM: [batch, num_tiles, H]\n    # PL: [batch, num_tiles, H]\n    # P_ACC: [batch, num_tiles, H, D_CKV]\n    pm_ptr = P_M + (b_idx * num_tiles + t_idx) * H + offs_h\n    tl.store(pm_ptr, m_i)\n    \n    pl_ptr = P_L + (b_idx * num_tiles + t_idx) * H + offs_h\n    tl.store(pl_ptr, l_i)\n    \n    pacc_ptr = P_ACC + ((b_idx * num_tiles + t_idx) * H + offs_h[:, None]) * D_CKV + offs_ckv[None, :]\n    tl.store(pacc_ptr, acc.to(tl.bfloat16))\n\n\n# ------------------------------------------------------------------\n# 2. Reduction MLA Kernel (Second Pass)\n# ------------------------------------------------------------------\n@triton.jit\ndef _mla_reduce_kernel(\n    P_M, P_L, P_ACC,\n    OUT, LSE,\n    KV_INDPTR,\n    num_tiles,\n    H: tl.constexpr,\n    D_CKV: tl.constexpr,\n    TILE_TOK: tl.constexpr,\n):\n    b_idx = tl.program_id(0)\n    h_idx = tl.program_id(1)\n\n    kv_beg = tl.load(KV_INDPTR + b_idx)\n    kv_end = tl.load(KV_INDPTR + b_idx + 1)\n    kv_len = kv_end - kv_beg\n\n    offs_ckv = tl.arange(0, D_CKV)\n    out_ptr = OUT + (b_idx * H + h_idx) * D_CKV + offs_ckv\n    lse_ptr = LSE + b_idx * H + h_idx\n\n    if kv_len <= 0:\n        tl.store(out_ptr, tl.zeros([D_CKV], dtype=tl.bfloat16))\n        tl.store(lse_ptr, -float(\"inf\"))\n        return\n\n    m_i = -float('inf')\n    l_i = 0.0\n    acc = tl.zeros([D_CKV], dtype=tl.float32)\n\n    actual_num_tiles = (kv_len + TILE_TOK - 1) // TILE_TOK\n\n    for t_idx in range(0, num_tiles):\n        if t_idx < actual_num_tiles:\n            # Load stats for this tile and head\n            m_t = tl.load(P_M + (b_idx * num_tiles + t_idx) * H + h_idx)\n            l_t = tl.load(P_L + (b_idx * num_tiles + t_idx) * H + h_idx)\n            \n            pacc_ptr = P_ACC + ((b_idx * num_tiles + t_idx) * H + h_idx) * D_CKV + offs_ckv\n            acc_t = tl.load(pacc_ptr).to(tl.float32)\n\n            m_next = tl.maximum(m_i, m_t)\n            alpha_i = tl.exp(m_i - m_next)\n            alpha_t = tl.exp(m_t - m_next)\n            \n            acc = acc * alpha_i + acc_t * alpha_t\n            l_i = l_i * alpha_i + l_t * alpha_t\n            m_i = m_next\n\n    # Normalization\n    out = acc / l_i\n    \n    # Compute base-2 LSE\n    inv_ln2 = 1.4426950408889634\n    lse_val = (m_i + tl.log(l_i)) * inv_ln2\n\n    tl.store(out_ptr, out.to(tl.bfloat16))\n    tl.store(lse_ptr, lse_val)\n\n\n# ------------------------------------------------------------------\n# 3. Host Runner\n# ------------------------------------------------------------------\ndef run(\n    q_nope: torch.Tensor,\n    q_pe: torch.Tensor,\n    ckv_cache: torch.Tensor,\n    kpe_cache: torch.Tensor,\n    kv_indptr: torch.Tensor,\n    kv_indices: torch.Tensor,\n    sm_scale: float,\n):\n    batch_size, num_qo_heads, head_dim_ckv = q_nope.shape\n    head_dim_kpe = q_pe.shape[-1]\n    device = q_nope.device\n    \n    kc_d = ckv_cache.view(-1, head_dim_ckv)\n    kp_d = kpe_cache.view(-1, head_dim_kpe)\n\n    # Calculate max sequence length to determine global grid tile size\n    if batch_size > 0:\n        max_kv_len = int((kv_indptr[1:] - kv_indptr[:-1]).max().item())\n    else:\n        max_kv_len = 0\n\n    TILE_TOK = 256\n    num_tiles = max(1, (max_kv_len + TILE_TOK - 1) // TILE_TOK)\n\n    # Allocate intermediate workspace for tiling\n    p_m = torch.empty((batch_size, num_tiles, num_qo_heads), dtype=torch.float32, device=device)\n    p_l = torch.empty((batch_size, num_tiles, num_qo_heads), dtype=torch.float32, device=device)\n    p_acc = torch.empty((batch_size, num_tiles, num_qo_heads, head_dim_ckv), dtype=torch.bfloat16, device=device)\n\n    # Final outputs\n    output = torch.empty((batch_size, num_qo_heads, head_dim_ckv), dtype=torch.bfloat16, device=device)\n    lse = torch.empty((batch_size, num_qo_heads), dtype=torch.float32, device=device)\n\n    if batch_size > 0:\n        # Pass 1: Tile-wise computation\n        grid1 = (batch_size, num_tiles)\n        _mla_tile_kernel[grid1](\n            QN=q_nope, QP=q_pe, KC=kc_d, KP=kp_d,\n            KV_INDICES=kv_indices, KV_INDPTR=kv_indptr,\n            SM_SCALE=float(sm_scale),\n            P_M=p_m, P_L=p_l, P_ACC=p_acc,\n            max_kv_len=max_kv_len,\n            num_tiles=num_tiles,\n            H=num_qo_heads, D_CKV=head_dim_ckv, D_KPE=head_dim_kpe,\n            TILE_TOK=TILE_TOK,\n        )\n\n        # Pass 2: Reduction\n        grid2 = (batch_size, num_qo_heads)\n        _mla_reduce_kernel[grid2](\n            P_M=p_m, P_L=p_l, P_ACC=p_acc,\n            OUT=output, LSE=lse,\n            KV_INDPTR=kv_indptr,\n            num_tiles=num_tiles,\n            H=num_qo_heads, D_CKV=head_dim_ckv,\n            TILE_TOK=TILE_TOK,\n            num_warps=4,\n        )\n\n    return {\"output\": output, \"lse\": lse}"
    }
  ],
  "description": "c788a3eb3e304b64ab88c736488167b7_plan_2_1"
}