{
  "name": "1d322c9223554279b58e6f6fe8a61f03",
  "definition": "gqa_paged_prefill_causal_h32_kv4_d128_ps1",
  "author": "AccelOpt",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "H100"
    ],
    "entry_point": "main.py::run"
  },
  "sources": [
    {
      "path": "main.py",
      "content": "import torch\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_KV': 32,  'KV_STAGES': 2}, num_warps=4, num_stages=2),\n        triton.Config({'BLOCK_KV': 32,  'KV_STAGES': 3}, num_warps=4, num_stages=2),\n        triton.Config({'BLOCK_KV': 64,  'KV_STAGES': 2}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_KV': 64,  'KV_STAGES': 3}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_KV': 128, 'KV_STAGES': 2}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_KV': 128, 'KV_STAGES': 3}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_KV': 256, 'KV_STAGES': 3}, num_warps=8, num_stages=4),\n        triton.Config({'BLOCK_KV': 256, 'KV_STAGES': 4}, num_warps=8, num_stages=4),\n    ],\n    key=['HEAD_DIM'],\n)\n@triton.jit\ndef gqa_paged_prefill_causal_kernel_optimized_per_head(\n    q_ptr, k_cache_ptr, v_cache_ptr,\n    qo_indptr_ptr, kv_indptr_ptr, kv_indices_ptr,\n    output_ptr, lse_ptr,\n    sm_scale,\n    total_q, len_indptr,\n    HEAD_DIM: tl.constexpr,\n    NUM_QO_HEADS: tl.constexpr,\n    NUM_KV_HEADS: tl.constexpr,\n    BLOCK_KV: tl.constexpr,\n    KV_STAGES: tl.constexpr,\n):\n    # Each program handles one query token and one query head\n    q_idx = tl.program_id(0)\n    head_idx = tl.program_id(1)\n\n    # 1. Binary search to find the sequence (batch) index for the current query token\n    low = 0\n    high = len_indptr - 2\n    while low <= high:\n        mid = (low + high) // 2\n        if tl.load(qo_indptr_ptr + mid + 1) <= q_idx:\n            low = mid + 1\n        else:\n            high = mid - 1\n    batch_idx = low\n\n    q_start = tl.load(qo_indptr_ptr + batch_idx)\n    q_end = tl.load(qo_indptr_ptr + batch_idx + 1)\n    kv_start = tl.load(kv_indptr_ptr + batch_idx)\n    kv_end = tl.load(kv_indptr_ptr + batch_idx + 1)\n    \n    # Calculate causal masking limits\n    num_q_tokens = q_end - q_start\n    num_kv_tokens = kv_end - kv_start\n    q_tok_off = q_idx - q_start\n\n    # A query token at index 'q_tok_off' can see KV tokens up to 'q_tok_off + (num_kv - num_q)'\n    max_kv_idx = q_tok_off + 1 + (num_kv_tokens - num_q_tokens)\n    \n    if max_kv_idx <= 0:\n        lse_off = q_idx * NUM_QO_HEADS + head_idx\n        tl.store(lse_ptr + lse_off, -float('inf'))\n        return\n\n    # 2. Load Query head vector [HEAD_DIM]\n    d_range = tl.arange(0, HEAD_DIM)\n    q_offset = q_idx * (NUM_QO_HEADS * HEAD_DIM) + head_idx * HEAD_DIM + d_range\n    q_vec = tl.load(q_ptr + q_offset)\n    \n    # Reshape for tl.dot: [1, HEAD_DIM]\n    q_vec_2d = tl.reshape(q_vec, [1, HEAD_DIM])\n\n    # 3. GQA Logic\n    gqa_ratio = NUM_QO_HEADS // NUM_KV_HEADS\n    kv_head_idx = head_idx // gqa_ratio\n\n    # 4. Online Softmax Accumulators (FP32)\n    max_logit = tl.full([1], -float('inf'), dtype=tl.float32)\n    denom = tl.zeros([1], dtype=tl.float32)\n    acc = tl.zeros([1, HEAD_DIM], dtype=tl.float32)\n\n    # 5. KV Loop (Page Size fixed to 1 for this implementation)\n    page_stride = NUM_KV_HEADS * HEAD_DIM \n\n    # Optimization: Pipeline the KV loop using KV_STAGES\n    for kv_blk_start in tl.range(0, max_kv_idx, BLOCK_KV, num_stages=KV_STAGES):\n        blk_range = kv_blk_start + tl.arange(0, BLOCK_KV)\n        mask = blk_range < max_kv_idx\n\n        # Load physical page IDs from the page table\n        page_ids = tl.load(kv_indices_ptr + kv_start + blk_range, mask=mask, other=0)\n\n        # Load K and V: [BLOCK_KV, HEAD_DIM]\n        # Offset: page_id * (page_size * num_heads * head_dim) + head_off + dim_off\n        kv_common_off = page_ids[:, None] * page_stride + kv_head_idx * HEAD_DIM + d_range[None, :]\n        k_blk = tl.load(k_cache_ptr + kv_common_off, mask=mask[:, None], other=0.0)\n        v_blk = tl.load(v_cache_ptr + kv_common_off, mask=mask[:, None], other=0.0)\n\n        # Compute Attention Logits: [1, HEAD_DIM] @ [HEAD_DIM, BLOCK_KV] -> [1, BLOCK_KV]\n        logits = tl.dot(q_vec_2d, tl.trans(k_blk)) * sm_scale\n        # Apply mask for causal/padding\n        logits = tl.where(mask[None, :], logits, -float('inf'))\n\n        # Update Softmax state\n        block_max = tl.max(logits, axis=1)\n        new_max = tl.maximum(max_logit, block_max)\n        \n        alpha = tl.exp(max_logit - new_max)\n        exp_logits = tl.exp(logits - new_max)\n        \n        # Update Accumulators\n        p_v = exp_logits.to(v_blk.dtype)\n        acc = acc * alpha + tl.dot(p_v, v_blk)\n        denom = denom * alpha + tl.sum(exp_logits, axis=1)\n        max_logit = new_max\n\n    # 6. Finalize and Store\n    out_vec_2d = acc / denom\n    out_vec_1d = tl.reshape(out_vec_2d, [HEAD_DIM])\n    tl.store(output_ptr + q_offset, out_vec_1d.to(q_ptr.dtype.element_ty))\n\n    # Log-Sum-Exp Calculation (Base 2)\n    lse_val = (max_logit + tl.log(denom)) * 1.44269504 \n    lse_off = q_idx * NUM_QO_HEADS + head_idx\n    tl.store(lse_ptr + lse_off, tl.sum(lse_val))\n\ndef run(q, k_cache, v_cache, qo_indptr, kv_indptr, kv_indices, sm_scale=None):\n    total_q, num_qo_heads, head_dim = q.shape\n    # k_cache: [num_pages, page_size, num_kv_heads, head_dim]\n    num_pages, page_size, num_kv_heads, _ = k_cache.shape\n    len_indptr = qo_indptr.shape[0]\n\n    assert page_size == 1, \"This kernel currently expects page_size=1\"\n    \n    if sm_scale is None:\n        sm_scale = 1.0 / math.sqrt(head_dim)\n\n    output = torch.empty((total_q, num_qo_heads, head_dim), dtype=q.dtype, device=q.device)\n    lse = torch.empty((total_q, num_qo_heads), dtype=torch.float32, device=q.device)\n\n    # Grid: Every query token and every head gets its own program\n    grid = (total_q, num_qo_heads)\n\n    gqa_paged_prefill_causal_kernel_optimized_per_head[grid](\n        q_ptr=q, \n        k_cache_ptr=k_cache, \n        v_cache_ptr=v_cache,\n        qo_indptr_ptr=qo_indptr, \n        kv_indptr_ptr=kv_indptr, \n        kv_indices_ptr=kv_indices,\n        output_ptr=output, \n        lse_ptr=lse,\n        sm_scale=sm_scale,\n        total_q=total_q, \n        len_indptr=len_indptr,\n        HEAD_DIM=head_dim,\n        NUM_QO_HEADS=num_qo_heads,\n        NUM_KV_HEADS=num_kv_heads,\n    )\n\n    return output, lse"
    }
  ],
  "description": "689fc4073fb04c1aa729a9163567c38f_plan_2_0"
}