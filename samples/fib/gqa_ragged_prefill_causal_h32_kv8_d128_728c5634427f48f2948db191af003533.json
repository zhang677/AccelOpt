{
  "name": "728c5634427f48f2948db191af003533",
  "definition": "gqa_ragged_prefill_causal_h32_kv8_d128",
  "author": "AccelOpt",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "H100"
    ],
    "entry_point": "main.py::run"
  },
  "sources": [
    {
      "path": "main.py",
      "content": "import torch\nimport triton\nimport triton.language as tl\nimport math\n\n# Optimized GQA Ragged Prefill Kernel with 2D grid launch (Token-Tiles x KV-Heads).\n# This version removes the internal KV-head loop, reducing register pressure and improving occupancy.\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_N': 32,  'Q_BLOCK': 16, 'NUM_STAGES_LOOP': 2, 'UNROLL': 1}, num_warps=4, num_stages=2),\n        triton.Config({'BLOCK_N': 64,  'Q_BLOCK': 16, 'NUM_STAGES_LOOP': 2, 'UNROLL': 1}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_N': 128, 'Q_BLOCK': 16, 'NUM_STAGES_LOOP': 3, 'UNROLL': 2}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_N': 128, 'Q_BLOCK': 32, 'NUM_STAGES_LOOP': 4, 'UNROLL': 2}, num_warps=8, num_stages=4),\n        triton.Config({'BLOCK_N': 256, 'Q_BLOCK': 16, 'NUM_STAGES_LOOP': 4, 'UNROLL': 2}, num_warps=8, num_stages=4),\n        triton.Config({'BLOCK_N': 256, 'Q_BLOCK': 32, 'NUM_STAGES_LOOP': 5, 'UNROLL': 2}, num_warps=8, num_stages=5),\n    ],\n    key=['total_q']\n)\n@triton.jit\ndef gqa_ragged_prefill_causal_h32_kv8_d128_kernel(\n    q_ptr, k_ptr, v_ptr,\n    stride_q_q, stride_q_h, stride_q_d,\n    stride_k_k, stride_k_h, stride_k_d,\n    stride_v_k, stride_v_h, stride_v_d,\n    out_ptr, stride_out_q, stride_out_h, stride_out_d,\n    lse_ptr, stride_lse_q, stride_lse_h,\n    qo_indptr_ptr, kv_indptr_ptr,\n    total_q, len_indptr,\n    sm_scale, ln2,\n    RATIO: tl.constexpr,          \n    HEAD_DIM: tl.constexpr,       \n    BLOCK_N: tl.constexpr,        \n    Q_BLOCK: tl.constexpr,        \n    NUM_STAGES_LOOP: tl.constexpr,\n    UNROLL: tl.constexpr,\n):\n    # 2D Launch Grid: pid_q covers total query tokens, kvh covers KV heads\n    pid_q = tl.program_id(0)\n    kvh = tl.program_id(1)\n\n    # Step 1: Binary search to find sequence index 'b' for the current query token.\n    low = 0\n    high = len_indptr - 2\n    while low <= high:\n        mid = (low + high) // 2\n        if tl.load(qo_indptr_ptr + mid + 1) <= pid_q:\n            low = mid + 1\n        else:\n            high = mid - 1\n    b = low\n\n    q_start = tl.load(qo_indptr_ptr + b)\n    \n    # Each program handles a block of Q_BLOCK queries. \n    # To cover every token without overlap in a ragged structure, \n    # we filter so only the start of a Q_BLOCK tile proceeds.\n    if (pid_q - q_start) % Q_BLOCK != 0:\n        return\n\n    q_end = tl.load(qo_indptr_ptr + b + 1)\n    kv_start = tl.load(kv_indptr_ptr + b)\n    kv_end = tl.load(kv_indptr_ptr + b + 1)\n    \n    q_len = q_end - q_start\n    kv_len = kv_end - kv_start\n    delta = kv_len - q_len\n\n    # Query offsets and dimensions\n    q_offsets = pid_q + tl.arange(0, Q_BLOCK)\n    valid_q_mask = q_offsets < q_end\n    d_range = tl.arange(0, HEAD_DIM)\n\n    # Causal logic setup\n    q_pos_in_seq = q_offsets - q_start\n    kv_max_token = q_pos_in_seq + 1 + delta\n    \n    # Upper bound for the KV loop to avoid unnecessary iterations\n    kv_max_tile_scalar = tl.max(tl.where(valid_q_mask, kv_max_token, 0))\n    kv_max_tile_scalar = tl.minimum(kv_len, kv_max_tile_scalar)\n\n    # Query head offsets for GQA (Ratio of Q heads to 1 KV head)\n    h_offsets = kvh * RATIO + tl.arange(0, RATIO)\n    \n    # Broadcast causal constraints for the flattened GQA tile (Q_BLOCK * RATIO)\n    kv_max_per_flat_q = tl.reshape(tl.broadcast_to(kv_max_token[:, None], [Q_BLOCK, RATIO]), [Q_BLOCK * RATIO])\n    neg_inf = -float(\"inf\")\n\n    # Load Query block: [Q_BLOCK, RATIO, HEAD_DIM]\n    q_ptrs = q_ptr + (q_offsets[:, None, None] * stride_q_q + \n                      h_offsets[None, :, None] * stride_q_h + \n                      d_range[None, None, :] * stride_q_d)\n    q_block = tl.load(q_ptrs, mask=valid_q_mask[:, None, None], other=0.0)\n    q_flat = tl.reshape(q_block, [Q_BLOCK * RATIO, HEAD_DIM])\n\n    # Online softmax accumulators\n    m = tl.full([Q_BLOCK * RATIO], neg_inf, dtype=tl.float32)\n    l = tl.zeros([Q_BLOCK * RATIO], dtype=tl.float32)\n    acc = tl.zeros([Q_BLOCK * RATIO, HEAD_DIM], dtype=tl.float32)\n\n    # Pointers for K and V belonging to this specific KV head\n    k_base = k_ptr + kv_start * stride_k_k + kvh * stride_k_h\n    v_base = v_ptr + kv_start * stride_v_k + kvh * stride_v_h\n\n    # Step 2: Main loop over Key-Value blocks\n    for start_n in tl.range(0, kv_max_tile_scalar, BLOCK_N, \n                            num_stages=NUM_STAGES_LOOP, \n                            loop_unroll_factor=UNROLL):\n        kv_off = start_n + tl.arange(0, BLOCK_N)\n        kv_mask = kv_off < kv_len\n        \n        # Load K: [HEAD_DIM, BLOCK_N]\n        k_ptrs = k_base + kv_off[None, :] * stride_k_k + d_range[:, None] * stride_k_d\n        k_block = tl.load(k_ptrs, mask=kv_mask[None, :], other=0.0)\n        \n        # Dot product: (Q * sm_scale) @ K^T\n        logits = tl.dot(q_flat, k_block) * sm_scale\n        \n        # Apply causal mask and sequence padding\n        tile_causal_mask = kv_off[None, :] < kv_max_per_flat_q[:, None]\n        logits = tl.where(tile_causal_mask & kv_mask[None, :], logits, neg_inf)\n        \n        # Online softmax update\n        m_new = tl.maximum(m, tl.max(logits, axis=1))\n        alpha = tl.exp(m - m_new)\n        p = tl.exp(logits - m_new[:, None])\n        l = l * alpha + tl.sum(p, axis=1)\n        \n        # Load V: [BLOCK_N, HEAD_DIM]\n        v_ptrs = v_base + kv_off[:, None] * stride_v_k + d_range[None, :] * stride_v_d\n        v_block = tl.load(v_ptrs, mask=kv_mask[:, None], other=0.0)\n        \n        # Accumulate Output\n        acc = acc * alpha[:, None]\n        acc = tl.dot(p.to(tl.bfloat16), v_block, acc=acc)\n        m = m_new\n\n    # Step 3: Normalize and Store\n    lse_res = (m + tl.log(l)) / ln2\n    out_res = acc / l[:, None]\n\n    # Map results back to head offsets\n    lse_ptrs = lse_ptr + q_offsets[:, None] * stride_lse_q + h_offsets[None, :] * stride_lse_h\n    tl.store(lse_ptrs, tl.reshape(lse_res, [Q_BLOCK, RATIO]), mask=valid_q_mask[:, None])\n\n    out_ptrs = out_ptr + (q_offsets[:, None, None] * stride_out_q + \n                          h_offsets[None, :, None] * stride_out_h + \n                          d_range[None, None, :] * stride_out_d)\n    tl.store(out_ptrs, tl.reshape(out_res.to(tl.bfloat16), [Q_BLOCK, RATIO, HEAD_DIM]), mask=valid_q_mask[:, None, None])\n\n\ndef run(q, k, v, qo_indptr, kv_indptr, sm_scale=None):\n    \"\"\"\n    Optimized GQA Ragged Prefill Attention with parallelized KV-heads.\n    \"\"\"\n    total_q, num_qo_heads, head_dim = q.shape\n    num_kv_heads = k.shape[1]\n    len_indptr = qo_indptr.shape[0]\n    \n    if sm_scale is None:\n        sm_scale = 1.0 / (head_dim ** 0.5)\n    \n    ln2 = math.log(2.0)\n\n    output = torch.empty((total_q, num_qo_heads, head_dim), dtype=torch.bfloat16, device=q.device)\n    lse = torch.empty((total_q, num_qo_heads), dtype=torch.float32, device=q.device)\n\n    # Grid is 2D: (total_tokens, num_kv_heads). \n    # Kernel handles token-tile filtering and head parallelism.\n    grid = (total_q, num_kv_heads)\n\n    gqa_ragged_prefill_causal_h32_kv8_d128_kernel[grid](\n        q, k, v,\n        q.stride(0), q.stride(1), q.stride(2),\n        k.stride(0), k.stride(1), k.stride(2),\n        v.stride(0), v.stride(1), v.stride(2),\n        output, output.stride(0), output.stride(1), output.stride(2),\n        lse, lse.stride(0), lse.stride(1),\n        qo_indptr, kv_indptr,\n        total_q, len_indptr,\n        sm_scale, ln2,\n        RATIO=num_qo_heads // num_kv_heads, \n        HEAD_DIM=head_dim\n    )\n\n    return output, lse"
    }
  ],
  "description": "6d811665f6a14e14a746a87ef2b70ed5_plan_1_1"
}