{
  "name": "179eaadd8f774d70a52e6932d7f5f0f8",
  "definition": "gqa_paged_decode_h32_kv4_d128_ps1",
  "author": "AccelOpt",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "H100"
    ],
    "entry_point": "main.py::run"
  },
  "sources": [
    {
      "path": "main.py",
      "content": "import math\nimport torch\nimport triton\nimport triton.language as tl\n\n# TMA descriptors require a global memory allocation function\ndef alloc_fn(size: int, alignment: int, stream: int):\n    return torch.empty(size, device=\"cuda\", dtype=torch.int8)\n\ntriton.set_allocator(alloc_fn)\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 32,  'PIPE_STAGES': 2}, num_warps=2, num_stages=2),\n        triton.Config({'BLOCK_M': 64,  'PIPE_STAGES': 2}, num_warps=4, num_stages=2),\n        triton.Config({'BLOCK_M': 64,  'PIPE_STAGES': 3}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_M': 128, 'PIPE_STAGES': 4}, num_warps=8, num_stages=4),\n        triton.Config({'BLOCK_M': 256, 'PIPE_STAGES': 4}, num_warps=8, num_stages=4),\n    ],\n    key=['batch_size']\n)\n@triton.jit\ndef gqa_paged_decode_kernel_optimized(\n    q_ptr,                         # *bf16 [B, Hq, D]\n    k_cache_ptr,                   # *bf16 [P, S=1, Hk, D]\n    v_cache_ptr,                   # *bf16 [P, S=1, Hk, D]\n    kv_indptr_ptr,                 # *i32  [B+1]\n    kv_indices_ptr,                # *i32  [N]\n    sm_scale,                      # f32 scalar\n    output_ptr,                    # *bf16 [B, Hq, D]\n    lse_ptr,                       # *f32  [B, Hq]\n    batch_size,                    # i32\n    num_pages,                     # i32\n    num_kv_heads,                  # i32\n    stride_q_b, stride_q_h, stride_q_d,\n    stride_k_p, stride_k_s, stride_k_h, stride_k_d,\n    stride_v_p, stride_v_s, stride_v_h, stride_v_d,\n    stride_o_b, stride_o_h, stride_o_d,\n    stride_lse_b, stride_lse_h,\n    BLOCK_M: tl.constexpr,         \n    D_HEAD: tl.constexpr,          \n    GQA_RATIO: tl.constexpr,       \n    PIPE_STAGES: tl.constexpr      \n):\n    # Map each program to one KV head group for a specific batch index\n    pid = tl.program_id(0)\n    kv_h = pid % num_kv_heads\n    b = pid // num_kv_heads\n\n    if b >= batch_size:\n        return\n\n    # Offsets\n    h_offsets = tl.arange(0, GQA_RATIO)\n    d_offsets = tl.arange(0, D_HEAD)\n    t_offsets = tl.arange(0, BLOCK_M)\n    INV_LN2: tl.constexpr = 1.4426950408889634\n    \n    # Retrieve page range (sequence length) for this sequence\n    page_start = tl.load(kv_indptr_ptr + b).to(tl.int32)\n    page_end = tl.load(kv_indptr_ptr + (b + 1)).to(tl.int32)\n    seq_len = page_end - page_start\n\n    # Base pointers for the specific KV head\n    k_ptr_base = k_cache_ptr + kv_h * stride_k_h\n    v_ptr_base = v_cache_ptr + kv_h * stride_v_h\n\n    # Load Query block: [GQA_RATIO, D_HEAD]\n    q_base = q_ptr + b * stride_q_b + (kv_h * GQA_RATIO) * stride_q_h\n    q_ptrs = q_base + h_offsets[:, None] * stride_q_h + d_offsets[None, :] * stride_q_d\n    \n    # Keep Q in BF16 for the dot product, but apply scaling.\n    # We cast to FP32 for scaling and back to BF16 to satisfy tl.dot constraints.\n    q_block = tl.load(q_ptrs)\n    q_block = (q_block.to(tl.float32) * sm_scale).to(tl.bfloat16)\n    \n    # Online softmax accumulators\n    m_i = tl.full([GQA_RATIO], -float(\"inf\"), dtype=tl.float32)\n    l_i = tl.zeros([GQA_RATIO], dtype=tl.float32)\n    acc = tl.zeros([GQA_RATIO, D_HEAD], dtype=tl.float32)\n\n    # Main KV loop\n    for pos in tl.range(0, seq_len, step=BLOCK_M, num_stages=PIPE_STAGES):\n        mask_t = (pos + t_offsets) < seq_len\n        \n        # Gather load physical page IDs\n        page_indices = page_start + pos + t_offsets\n        page_ids = tl.load(kv_indices_ptr + page_indices, mask=mask_t, other=0)\n        \n        # Load K-block: [BLOCK_M, D_HEAD]\n        k_ptrs = k_ptr_base + page_ids[:, None] * stride_k_p + d_offsets[None, :] * stride_k_d\n        k_block = tl.load(k_ptrs, mask=mask_t[:, None], other=0.0)\n        \n        # Compute Logits: [GQA_RATIO, BLOCK_M]\n        # Both operands are BF16, result is FP32\n        logits = tl.dot(q_block, tl.trans(k_block))\n        logits = tl.where(mask_t[None, :], logits, -float(\"inf\"))\n\n        # Online Softmax update\n        m_curr = tl.max(logits, axis=1)\n        m_new = tl.maximum(m_i, m_curr)\n        \n        alpha = tl.exp(m_i - m_new)\n        p = tl.exp(logits - m_new[:, None])\n        \n        l_i = l_i * alpha + tl.sum(p, axis=1)\n        \n        # Load V-block: [BLOCK_M, D_HEAD]\n        v_ptrs = v_ptr_base + page_ids[:, None] * stride_v_p + d_offsets[None, :] * stride_v_d\n        v_block = tl.load(v_ptrs, mask=mask_t[:, None], other=0.0)\n        \n        # Update numerator: acc = acc * alpha + p @ v\n        acc = acc * alpha[:, None]\n        # Cast p back to BF16 for the dot product with v_block (BF16)\n        acc = tl.dot(p.to(tl.bfloat16), v_block, acc=acc)\n        \n        m_i = m_new\n\n    # Final normalization\n    acc = acc / l_i[:, None]\n    \n    # Store Output\n    o_base = output_ptr + b * stride_o_b + (kv_h * GQA_RATIO) * stride_o_h\n    o_ptrs = o_base + h_offsets[:, None] * stride_o_h + d_offsets[None, :] * stride_o_d\n    tl.store(o_ptrs, acc.to(tl.bfloat16))\n\n    # Store Log-Sum-Exp (LSE)\n    lse_val = (m_i + tl.log(l_i)) * INV_LN2\n    lse_base = lse_ptr + b * stride_lse_b + (kv_h * GQA_RATIO + h_offsets) * stride_lse_h\n    tl.store(lse_base, lse_val)\n\ndef run(q, k_cache, v_cache, kv_indptr, kv_indices, sm_scale=None):\n    batch_size, num_qo_heads, head_dim = q.shape\n    num_kv_heads = k_cache.shape[2]\n    gqa_ratio = num_qo_heads // num_kv_heads\n    \n    if sm_scale is None:\n        sm_scale = 1.0 / math.sqrt(head_dim)\n    \n    output = torch.empty_like(q)\n    lse = torch.empty((batch_size, num_qo_heads), dtype=torch.float32, device=q.device)\n\n    # Grid: One program per KV head per batch item\n    grid = lambda META: (batch_size * num_kv_heads,)\n\n    gqa_paged_decode_kernel_optimized[grid](\n        q_ptr=q, \n        k_cache_ptr=k_cache, \n        v_cache_ptr=v_cache, \n        kv_indptr_ptr=kv_indptr, \n        kv_indices_ptr=kv_indices,\n        sm_scale=float(sm_scale), \n        output_ptr=output, \n        lse_ptr=lse,\n        batch_size=batch_size, \n        num_pages=k_cache.shape[0],\n        num_kv_heads=num_kv_heads,\n        stride_q_b=q.stride(0), stride_q_h=q.stride(1), stride_q_d=q.stride(2),\n        stride_k_p=k_cache.stride(0), stride_k_s=k_cache.stride(1), stride_k_h=k_cache.stride(2), stride_k_d=k_cache.stride(3),\n        stride_v_p=v_cache.stride(0), stride_v_s=v_cache.stride(1), stride_v_h=v_cache.stride(2), stride_v_d=v_cache.stride(3),\n        stride_o_b=output.stride(0), stride_o_h=output.stride(1), stride_o_d=output.stride(2),\n        stride_lse_b=lse.stride(0), stride_lse_h=lse.stride(1),\n        D_HEAD=head_dim,\n        GQA_RATIO=gqa_ratio,\n    )\n\n    return output, lse"
    }
  ],
  "description": "95f879ea57644991bd8b4efcc033b196_plan_2_1"
}