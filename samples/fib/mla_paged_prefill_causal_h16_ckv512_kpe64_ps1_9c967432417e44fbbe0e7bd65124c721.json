{
  "name": "9c967432417e44fbbe0e7bd65124c721",
  "definition": "mla_paged_prefill_causal_h16_ckv512_kpe64_ps1",
  "author": "AccelOpt",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "H100"
    ],
    "entry_point": "main.py::run"
  },
  "sources": [
    {
      "path": "main.py",
      "content": "import torch\nimport triton\nimport triton.language as tl\nfrom typing import Optional\n\n# TMA descriptors require a global memory allocator\ndef alloc_fn(size: int, alignment: int, stream: Optional[int]):\n    return torch.empty(size, device=\"cuda\", dtype=torch.int8)\n\ntriton.set_allocator(alloc_fn)\n\n@triton.autotune(\n    configs=[\n        # Optimized for D_CKV=512. Reducing BLOCK_N to stay within shared memory limits.\n        triton.Config({'BLOCK_M': 16, 'BLOCK_N': 32, 'UNROLL': 1}, num_warps=4, num_stages=2),\n        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 32, 'UNROLL': 1}, num_warps=4, num_stages=2),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32, 'UNROLL': 1}, num_warps=4, num_stages=2),\n        triton.Config({'BLOCK_M': 16, 'BLOCK_N': 64, 'UNROLL': 1}, num_warps=4, num_stages=2),\n        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 64, 'UNROLL': 1}, num_warps=8, num_stages=2),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'UNROLL': 1}, num_warps=8, num_stages=2),\n        # A few slightly more aggressive configs\n        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 64, 'UNROLL': 2}, num_warps=8, num_stages=3),\n    ],\n    key=['D_CKV', 'D_KPE'],\n)\n@triton.jit\ndef mla_paged_prefill_kernel_fused(\n    q_nope_ptr,      # [total_q, H, D_CKV]\n    q_pe_ptr,        # [total_q, H, D_KPE]\n    ckv_ptr,         # [num_kv_indices, D_CKV]\n    kpe_ptr,         # [num_kv_indices, D_KPE]\n    qo_indptr_ptr,   # [NUM_SEQS + 1]\n    kv_indptr_ptr,   # [NUM_SEQS + 1]\n    sm_scale,        \n    output_ptr,      # [total_q, H, D_CKV]\n    lse_ptr,         # [total_q, H]\n    num_kv_indices,\n    H: tl.constexpr,\n    D_CKV: tl.constexpr,\n    D_KPE: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    UNROLL: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_bh = tl.program_id(1)\n    \n    seq_id = pid_bh // H\n    head_id = pid_bh % H\n\n    # Sequence offsets\n    q_start = tl.load(qo_indptr_ptr + seq_id)\n    q_end = tl.load(qo_indptr_ptr + seq_id + 1)\n    cur_q_len = q_end - q_start\n    \n    start_q_idx = pid_m * BLOCK_M\n    if start_q_idx >= cur_q_len:\n        return\n\n    kv_start = tl.load(kv_indptr_ptr + seq_id)\n    kv_end = tl.load(kv_indptr_ptr + seq_id + 1)\n    cur_kv_len = kv_end - kv_start\n    prefix_len = cur_kv_len - cur_q_len\n    \n    # Q offsets and masking\n    offs_m = start_q_idx + tl.arange(0, BLOCK_M)\n    q_mask = (offs_m < cur_q_len)\n    q_abs = q_start + offs_m\n    \n    offs_d_ckv = tl.arange(0, D_CKV)\n    offs_d_kpe = tl.arange(0, D_KPE)\n\n    # Load query components\n    qn_ptr = q_nope_ptr + (q_abs[:, None].to(tl.int64) * H * D_CKV + head_id * D_CKV + offs_d_ckv[None, :])\n    qp_ptr = q_pe_ptr + (q_abs[:, None].to(tl.int64) * H * D_KPE + head_id * D_KPE + offs_d_kpe[None, :])\n    \n    qn = tl.load(qn_ptr, mask=q_mask[:, None], other=0.0)\n    qp = tl.load(qp_ptr, mask=q_mask[:, None], other=0.0)\n    \n    # Softmax accumulators\n    m_i = tl.full([BLOCK_M], -float(\"inf\"), dtype=tl.float32)\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc_o = tl.zeros([BLOCK_M, D_CKV], dtype=tl.float32)\n\n    # TMA descriptors\n    ckv_desc = tl.make_tensor_descriptor(ckv_ptr, [num_kv_indices, D_CKV], [D_CKV, 1], [BLOCK_N, D_CKV])\n    kpe_desc = tl.make_tensor_descriptor(kpe_ptr, [num_kv_indices, D_KPE], [D_KPE, 1], [BLOCK_N, D_KPE])\n\n    scaled_sm = sm_scale * 1.44269504089  # log2 scale\n    \n    # Main loop over KV blocks (causal-aware)\n    # The max KV index we need to look at for this M-block\n    max_kv_idx = prefix_len + start_q_idx + BLOCK_M\n    lo = 0\n    hi = tl.minimum(max_kv_idx, cur_kv_len)\n\n    for start_n in tl.range(lo, hi, BLOCK_N, loop_unroll_factor=UNROLL):\n        # TMA Load\n        kc = ckv_desc.load([kv_start + start_n, 0])\n        kp = kpe_desc.load([kv_start + start_n, 0])\n        \n        # Logits computation\n        logits = tl.dot(qn.to(kc.dtype), tl.trans(kc))\n        logits = tl.dot(qp.to(kp.dtype), tl.trans(kp), acc=logits)\n        logits *= scaled_sm\n        \n        # Causal mask\n        offs_n = start_n + tl.arange(0, BLOCK_N)\n        mask = (prefix_len + offs_m[:, None]) >= offs_n[None, :]\n        logits = tl.where(mask & q_mask[:, None], logits, -float(\"inf\"))\n\n        # Online softmax\n        m_ij = tl.max(logits, axis=1)\n        m_next = tl.maximum(m_i, m_ij)\n        \n        alpha = tl.exp2(m_i - m_next)\n        p = tl.exp2(logits - m_next[:, None])\n        p = tl.where(mask & q_mask[:, None], p, 0.0)\n        \n        # Update\n        alpha_o = tl.where(m_i == -float(\"inf\"), 0.0, alpha)\n        acc_o = acc_o * alpha_o[:, None]\n        acc_o = tl.dot(p.to(kc.dtype), kc, acc=acc_o)\n        \n        l_i = l_i * alpha_o + tl.sum(p, axis=1)\n        m_i = m_next\n\n    # Final normalization\n    acc_o = acc_o / (l_i[:, None] + 1e-10)\n    lse_val = m_i + tl.log2(l_i)\n\n    # Store\n    out_ptr = output_ptr + (q_abs[:, None].to(tl.int64) * H * D_CKV + head_id * D_CKV + offs_d_ckv[None, :])\n    tl.store(out_ptr, acc_o.to(output_ptr.dtype.element_ty), mask=q_mask[:, None])\n    \n    lse_ptr_final = lse_ptr + (q_abs.to(tl.int64) * H + head_id)\n    tl.store(lse_ptr_final, lse_val, mask=q_mask)\n\ndef run(q_nope, q_pe, ckv_cache, kpe_cache, qo_indptr, kv_indptr, kv_indices, sm_scale=None):\n    total_q, num_heads, head_dim_ckv = q_nope.shape\n    head_dim_kpe = q_pe.shape[-1]\n    batch_size = qo_indptr.shape[0] - 1\n    device = q_nope.device\n    \n    if sm_scale is None:\n        sm_scale = (head_dim_ckv + head_dim_kpe)**-0.5\n\n    # Pre-order KV cache\n    ckv_ordered = ckv_cache.view(-1, head_dim_ckv)[kv_indices.to(torch.long)]\n    kpe_ordered = kpe_cache.view(-1, head_dim_kpe)[kv_indices.to(torch.long)]\n    num_kv_indices = kv_indices.shape[0]\n\n    output = torch.empty((total_q, num_heads, head_dim_ckv), dtype=q_nope.dtype, device=device)\n    lse = torch.empty((total_q, num_heads), dtype=torch.float32, device=device)\n\n    q_lens = qo_indptr[1:] - qo_indptr[:-1]\n    max_q_len = int(torch.max(q_lens).item()) if q_lens.numel() > 0 else 0\n\n    grid = lambda META: (\n        triton.cdiv(max_q_len, META['BLOCK_M']),\n        batch_size * num_heads\n    )\n\n    mla_paged_prefill_kernel_fused[grid](\n        q_nope, q_pe, ckv_ordered, kpe_ordered,\n        qo_indptr, kv_indptr,\n        sm_scale, output, lse,\n        num_kv_indices,\n        H=num_heads, D_CKV=head_dim_ckv, D_KPE=head_dim_kpe\n    )\n\n    return output, lse"
    }
  ],
  "description": "06a4f4af963a414d808556b6f77e3502_plan_0_0"
}