{
  "name": "0cb2d36f3f91486f95befba29bfa03bf",
  "definition": "gqa_paged_decode_h32_kv8_d128_ps1",
  "author": "AccelOpt",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "H100"
    ],
    "entry_point": "main.py::run"
  },
  "sources": [
    {
      "path": "main.py",
      "content": "import torch\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        # Optimized configurations following the plan\n        # Small-block config for short sequences\n        triton.Config({'BLOCK_M': 16, 'UNROLL_FACTOR': 1, 'STAGES_L': 2}, num_warps=2, num_stages=2),\n        # Existing and refined configurations\n        triton.Config({'BLOCK_M': 32, 'UNROLL_FACTOR': 1, 'STAGES_L': 2}, num_warps=4, num_stages=2),\n        triton.Config({'BLOCK_M': 64, 'UNROLL_FACTOR': 1, 'STAGES_L': 4}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_M': 128, 'UNROLL_FACTOR': 1, 'STAGES_L': 8}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_M': 256, 'UNROLL_FACTOR': 1, 'STAGES_L': 8}, num_warps=8, num_stages=4),\n        triton.Config({'BLOCK_M': 512, 'UNROLL_FACTOR': 1, 'STAGES_L': 4}, num_warps=8, num_stages=4),\n        triton.Config({'BLOCK_M': 1024, 'UNROLL_FACTOR': 1, 'STAGES_L': 4}, num_warps=8, num_stages=4),\n        # Larger blocks for longer sequences, more warps, and deeper pipelines\n        triton.Config({'BLOCK_M': 2048, 'UNROLL_FACTOR': 2, 'STAGES_L': 8}, num_warps=16, num_stages=8),\n        triton.Config({'BLOCK_M': 4096, 'UNROLL_FACTOR': 2, 'STAGES_L': 8}, num_warps=16, num_stages=8),\n    ],\n    key=['batch_size'],\n)\n@triton.jit\ndef gqa_paged_decode_kernel(\n    q_ptr, k_cache_ptr, v_cache_ptr,\n    kv_indptr_ptr, kv_indices_ptr,\n    output_ptr, lse_ptr,\n    sm_scale,\n    batch_size,\n    NUM_QO_HEADS: tl.constexpr,\n    NUM_KV_HEADS: tl.constexpr,\n    HEAD_DIM: tl.constexpr,\n    GQA_RATIO: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_G: tl.constexpr,\n    UNROLL_FACTOR: tl.constexpr,\n    STAGES_L: tl.constexpr,\n):\n    # Program IDs: Each block handles one sequence and one group of GQA query heads\n    batch_idx = tl.program_id(0)\n    kv_head_idx = tl.program_id(1)\n\n    if batch_idx >= batch_size:\n        return\n\n    # Constants for indexing\n    d_idx = tl.arange(0, HEAD_DIM)\n    g_idx = tl.arange(0, BLOCK_G)\n\n    # Sequence bounds for paged KV cache\n    seq_start = tl.load(kv_indptr_ptr + batch_idx)\n    seq_end = tl.load(kv_indptr_ptr + batch_idx + 1)\n    seq_len = seq_end - seq_start\n\n    # Load group of queries: [BLOCK_G, HEAD_DIM]\n    # Each group shares the same KV head in GQA\n    q_group_base_ptr = (\n        q_ptr + \n        batch_idx * NUM_QO_HEADS * HEAD_DIM + \n        (kv_head_idx * GQA_RATIO) * HEAD_DIM\n    )\n    q_ptrs = q_group_base_ptr + g_idx[:, None] * HEAD_DIM + d_idx[None, :]\n    q_group = tl.load(q_ptrs)\n\n    # Online softmax accumulators\n    m_i = tl.full((BLOCK_G,), -float(\"inf\"), dtype=tl.float32)\n    l_i = tl.zeros((BLOCK_G,), dtype=tl.float32)\n    acc = tl.zeros((BLOCK_G, HEAD_DIM), dtype=tl.float32)\n\n    # Paged cache strides\n    stride_cache_p = NUM_KV_HEADS * HEAD_DIM\n    stride_cache_h = HEAD_DIM\n\n    if seq_len > 0:\n        token_offsets_base = tl.arange(0, BLOCK_M)\n        \n        # Iterate over tokens in chunks of BLOCK_M\n        # STAGES_L and UNROLL_FACTOR are used to deepen the pipeline and reduce branch overhead\n        for token_start in tl.range(0, seq_len, BLOCK_M, num_stages=STAGES_L, loop_unroll_factor=UNROLL_FACTOR, flatten=True):\n            token_offsets = token_start + token_offsets_base\n            token_mask = token_offsets < seq_len\n            \n            # Load page indices for the current token block\n            page_indices = tl.load(\n                kv_indices_ptr + seq_start + token_offsets,\n                mask=token_mask,\n                other=0\n            )\n\n            # Load K and V from the paged cache: [BLOCK_M, HEAD_DIM]\n            # Offsets are calculated relative to the shared KV head in the group\n            offs_kv_base = kv_head_idx * stride_cache_h + d_idx[None, :]\n            kv_ptrs = k_cache_ptr + page_indices[:, None] * stride_cache_p + offs_kv_base\n            \n            k_vals = tl.load(kv_ptrs, mask=token_mask[:, None], other=0.0)\n            \n            # Attention Logits: [BLOCK_G, HEAD_DIM] @ [HEAD_DIM, BLOCK_M] -> [BLOCK_G, BLOCK_M]\n            logits = tl.dot(q_group.to(tl.bfloat16), tl.trans(k_vals).to(tl.bfloat16))\n            logits *= sm_scale\n            \n            # Mask padding within the block\n            logits = tl.where(token_mask[None, :], logits, -float(\"inf\"))\n\n            # Update Online Softmax state\n            m_i_new = tl.maximum(m_i, tl.max(logits, axis=1))\n            alpha = tl.exp(m_i - m_i_new)\n            p = tl.exp(logits - m_i_new[:, None])\n            \n            l_i = l_i * alpha + tl.sum(p, axis=1)\n            acc = acc * alpha[:, None]\n\n            # Weighted sum for Output (V): [BLOCK_G, BLOCK_M] @ [BLOCK_M, HEAD_DIM] -> [BLOCK_G, HEAD_DIM]\n            # Reuse kv_ptrs base for V-cache access\n            v_ptrs = v_cache_ptr + page_indices[:, None] * stride_cache_p + offs_kv_base\n            v_vals = tl.load(v_ptrs, mask=token_mask[:, None], other=0.0)\n            acc = tl.dot(p.to(tl.bfloat16), v_vals.to(tl.bfloat16), acc=acc)\n\n            m_i = m_i_new\n\n        # Final normalization of output\n        acc = acc / l_i[:, None]\n        # Compute base-2 Log-Sum-Exp: log2(e) * (m_i + ln(l_i))\n        lse_val = (m_i + tl.log(l_i)) * 1.44269504\n    else:\n        lse_val = tl.full((BLOCK_G,), -float(\"inf\"), dtype=tl.float32)\n\n    # Store final attention results: [BLOCK_G, HEAD_DIM]\n    out_base_ptr = (\n        output_ptr + \n        batch_idx * NUM_QO_HEADS * HEAD_DIM + \n        (kv_head_idx * GQA_RATIO) * HEAD_DIM\n    )\n    out_ptrs = out_base_ptr + g_idx[:, None] * HEAD_DIM + d_idx[None, :]\n    tl.store(out_ptrs, acc.to(tl.bfloat16))\n\n    # Store LSE: [BLOCK_G]\n    lse_ptrs = lse_ptr + (batch_idx * NUM_QO_HEADS + kv_head_idx * GQA_RATIO) + g_idx\n    tl.store(lse_ptrs, lse_val)\n\ndef run(q, k_cache, v_cache, kv_indptr, kv_indices, sm_scale):\n    # Dimensions\n    batch_size, num_qo_heads, head_dim = q.shape\n    num_pages, page_size, num_kv_heads, _ = k_cache.shape\n    \n    # Assertions for kernel support\n    assert page_size == 1, \"Currently only page_size=1 is supported.\"\n    assert head_dim == 128\n    assert num_qo_heads == 32\n    assert num_kv_heads == 8\n    \n    gqa_ratio = num_qo_heads // num_kv_heads\n    \n    # Allocate outputs\n    output = torch.empty_like(q)\n    lse = torch.empty((batch_size, num_qo_heads), dtype=torch.float32, device=q.device)\n\n    # Grid dimension: Sequence Batch x KV Head Groups\n    grid = (batch_size, num_kv_heads)\n    \n    # Flatten cache for easier kernel indexing\n    k_cache_flat = k_cache.view(num_pages, num_kv_heads, head_dim)\n    v_cache_flat = v_cache.view(num_pages, num_kv_heads, head_dim)\n\n    # Launch optimized GQA Paged Decode Kernel\n    gqa_paged_decode_kernel[grid](\n        q_ptr=q,\n        k_cache_ptr=k_cache_flat,\n        v_cache_ptr=v_cache_flat,\n        kv_indptr_ptr=kv_indptr,\n        kv_indices_ptr=kv_indices,\n        output_ptr=output,\n        lse_ptr=lse,\n        sm_scale=sm_scale,\n        batch_size=batch_size,\n        NUM_QO_HEADS=num_qo_heads,\n        NUM_KV_HEADS=num_kv_heads,\n        HEAD_DIM=head_dim,\n        GQA_RATIO=gqa_ratio,\n        BLOCK_G=gqa_ratio,\n    )\n    \n    return output, lse"
    }
  ],
  "description": "f264794f57eb4bb1bfd56b70db54f0f5_plan_0_0"
}