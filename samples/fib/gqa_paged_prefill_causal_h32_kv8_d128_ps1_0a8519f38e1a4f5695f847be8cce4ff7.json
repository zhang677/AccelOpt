{
  "name": "0a8519f38e1a4f5695f847be8cce4ff7",
  "definition": "gqa_paged_prefill_causal_h32_kv8_d128_ps1",
  "author": "AccelOpt",
  "spec": {
    "language": "triton",
    "target_hardware": [
      "H100"
    ],
    "entry_point": "main.py::run"
  },
  "sources": [
    {
      "path": "main.py",
      "content": "import torch\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 32}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 64}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64}, num_warps=8, num_stages=5),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64}, num_warps=8, num_stages=5),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128}, num_warps=8, num_stages=6),\n        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 64}, num_warps=8, num_stages=6),\n        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 128}, num_warps=8, num_stages=6),\n    ],\n    key=['total_q', 'max_q_len'],\n)\n@triton.jit\ndef _gqa_paged_prefill_causal_kernel(\n    Q, K_cache, V_cache,\n    QO_indptr, KV_indptr, KV_indices,\n    sm_scale,\n    Output, LSE,\n    stride_q_total, stride_q_head, stride_q_dim,\n    stride_k_page, stride_k_head, stride_k_dim,\n    stride_v_page, stride_v_head, stride_v_dim,\n    total_q, max_q_len,\n    NUM_QO_HEADS: tl.constexpr, \n    NUM_KV_HEADS: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    HEAD_DIM: tl.constexpr,\n):\n    # Map program IDs to batch index, query tile index, and KV head index.\n    batch_idx = tl.program_id(0)\n    tile_idx = tl.program_id(1)\n    kv_head_idx = tl.program_id(2)\n\n    # GQA ratio: each KV head is shared by multiple Query heads.\n    GQA_RATIO: tl.constexpr = NUM_QO_HEADS // NUM_KV_HEADS\n    qo_head_base = kv_head_idx * GQA_RATIO\n    qo_head_range = tl.arange(0, GQA_RATIO)\n    \n    # Identify sequence-specific boundaries for both Query and KV indices.\n    q_start_ptr = tl.load(QO_indptr + batch_idx)\n    q_end_ptr = tl.load(QO_indptr + batch_idx + 1)\n    q_len = q_end_ptr - q_start_ptr\n\n    # Tile-level boundary check.\n    tile_start_in_seq = tile_idx * BLOCK_M\n    if tile_start_in_seq >= q_len:\n        return\n\n    kv_start = tl.load(KV_indptr + batch_idx)\n    kv_end = tl.load(KV_indptr + batch_idx + 1)\n    num_kv_tokens = kv_end - kv_start\n    \n    # Delta used for causal masking.\n    delta = num_kv_tokens - q_len\n\n    # Coordinate ranges for the query and dimensions.\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, HEAD_DIM)\n    \n    q_idx_in_seq = tile_start_in_seq + offs_m\n    mask_q = q_idx_in_seq < q_len\n    q_global_idx = q_start_ptr + q_idx_in_seq\n\n    # Load Query block without pre-scaling (Optimization Plan)\n    q_ptr = Q + (q_global_idx[None, :, None] * stride_q_total + \n                 (qo_head_base + qo_head_range)[:, None, None] * stride_q_head + \n                 offs_d[None, None, :] * stride_q_dim)\n    q_block = tl.load(q_ptr, mask=mask_q[None, :, None], other=0.0)\n\n    # Online softmax accumulators.\n    m_i = tl.full([GQA_RATIO, BLOCK_M], -float(\"inf\"), dtype=tl.float32)\n    l_i = tl.zeros([GQA_RATIO, BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([GQA_RATIO, BLOCK_M, HEAD_DIM], dtype=tl.float32)\n\n    # Main loop over the KV cache. \n    for n_offset in tl.range(0, num_kv_tokens, BLOCK_N):\n        offs_n = n_offset + tl.arange(0, BLOCK_N)\n        mask_n = offs_n < num_kv_tokens\n        \n        # Paged KV access: load physical page indices.\n        page_ids = tl.load(KV_indices + kv_start + offs_n, mask=mask_n, other=0)\n\n        # Construct pointers for the gathered K and V tiles.\n        k_ptr = K_cache + (page_ids[:, None] * stride_k_page + \n                           kv_head_idx * stride_k_head + \n                           offs_d[None, :] * stride_k_dim)\n        v_ptr = V_cache + (page_ids[:, None] * stride_v_page + \n                           kv_head_idx * stride_v_head + \n                           offs_d[None, :] * stride_v_dim)\n        \n        k_block = tl.load(k_ptr, mask=mask_n[:, None], other=0.0)\n        v_block = tl.load(v_ptr, mask=mask_n[:, None], other=0.0)\n\n        # Compute Attention Scores (Logits) and fuse sm_scale.\n        k_block_3d = tl.broadcast_to(tl.trans(k_block)[None, :, :], [GQA_RATIO, HEAD_DIM, BLOCK_N])\n        # dot produces [GQA_RATIO, BLOCK_M, BLOCK_N] in FP32 accumulation.\n        logits = tl.dot(q_block, k_block_3d) * sm_scale\n\n        # Apply causal mask.\n        causal_mask = offs_n[None, :] < (q_idx_in_seq[:, None] + delta + 1)\n        total_mask = mask_q[:, None] & mask_n[None, :] & causal_mask\n        logits = tl.where(total_mask[None, :, :], logits, -float(\"inf\"))\n\n        # Online Softmax update.\n        m_i_next = tl.maximum(m_i, tl.max(logits, axis=2))\n        p = tl.exp(logits - m_i_next[:, :, None])\n        alpha = tl.exp(m_i - m_i_next)\n        l_i_next = alpha * l_i + tl.sum(p, axis=2)\n\n        # Update Attention output accumulator.\n        acc = acc * alpha[:, :, None]\n        v_block_3d = tl.broadcast_to(v_block[None, :, :], [GQA_RATIO, BLOCK_N, HEAD_DIM])\n        acc = tl.dot(p.to(Q.dtype.element_ty), v_block_3d, acc=acc)\n\n        m_i = m_i_next\n        l_i = l_i_next\n\n    # Final normalization of the output values.\n    o = acc / l_i[:, :, None]\n    \n    # Store finalized Attention Output.\n    out_ptr = Output + (q_global_idx[None, :, None] * stride_q_total + \n                        (qo_head_base + qo_head_range)[:, None, None] * stride_q_head + \n                        offs_d[None, None, :] * stride_q_dim)\n    tl.store(out_ptr, o.to(Q.dtype.element_ty), mask=mask_q[None, :, None])\n\n    # Store Log-Sum-Exp (LSE) in base-2.\n    lse_val = (m_i + tl.log(l_i)) * 1.44269504 # log2(e)\n    lse_ptr = LSE + q_global_idx[None, :] * NUM_QO_HEADS + (qo_head_base + qo_head_range)[:, None]\n    tl.store(lse_ptr, lse_val, mask=mask_q[None, :])\n\n\ndef run(q, k_cache, v_cache, qo_indptr, kv_indptr, kv_indices, sm_scale=None):\n    total_q, num_qo_heads, head_dim = q.shape\n    num_kv_heads = k_cache.shape[2]\n    batch_size = qo_indptr.shape[0] - 1\n    \n    if sm_scale is None:\n        sm_scale = 1.0 / math.sqrt(head_dim)\n\n    # Pre-allocate output buffers.\n    output = torch.empty_like(q)\n    lse = torch.empty((total_q, num_qo_heads), device=q.device, dtype=torch.float32)\n\n    if batch_size > 0:\n        q_lens = qo_indptr[1:] - qo_indptr[:-1]\n        max_q_len = int(torch.max(q_lens).item())\n    else:\n        max_q_len = 0\n    \n    if total_q == 0 or max_q_len == 0:\n        return output, lse\n\n    # Grid: Each sequence in batch, Query tiles, and shared KV heads.\n    grid = lambda META: (\n        batch_size,\n        (max_q_len + META['BLOCK_M'] - 1) // META['BLOCK_M'],\n        num_kv_heads,\n    )\n\n    _gqa_paged_prefill_causal_kernel[grid](\n        q, k_cache, v_cache,\n        qo_indptr, kv_indptr, kv_indices,\n        sm_scale,\n        output, lse,\n        q.stride(0), q.stride(1), q.stride(2),\n        k_cache.stride(0), k_cache.stride(2), k_cache.stride(3),\n        v_cache.stride(0), v_cache.stride(2), v_cache.stride(3),\n        total_q, max_q_len,\n        NUM_QO_HEADS=num_qo_heads,\n        NUM_KV_HEADS=num_kv_heads,\n        HEAD_DIM=head_dim,\n    )\n\n    return output, lse"
    }
  ],
  "description": "b8a4ae63e63441c094d128455eab5fab_plan_0_1"
}